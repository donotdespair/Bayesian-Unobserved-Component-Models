---
title: "Bayesian Unobserved Component Models"
author:
  - name: Tomasz WoÅºniak
    url: https://github.com/donotdespair
    orcid: 0000-0003-2212-2378
  - name: Zheyuan Li
    url: https://github.com/lzyzero3
    execute:
  #echo: false
citation: 
  issued: 2024-05-01
  url: https://donotdespair.github.io/Bayesian-Unobserved-Component-Models/
  doi: 10.26188/25814617
bibliography: references.bib
---

> **Abstract.** We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. 
> The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. 
> We scrutinise Bayesian forecasting and sampling from the predictive density.
>
> **Keywords.** Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling

# Unobserved component models

Unobserved Component (UC) models are a popular class of models in macroeconometrics that use the state-space representation for unit-root nonstationary time series. 
The simple formulation of the model equations decomposing the series into a non-stationary and stationary component facilitates economic interpretations and good forecasting performance.

# A simple local-level model

The model is set for a univariate time series whose observation at time
$t$ is denoted by $y_t$. 
It decomposes the variable into a stochastic trend component, $\tau_t$, and a stationary error component, $\epsilon_t$. 
The former follows a Gaussian random walk process with the conditional variance $\sigma_\eta^2$, and the latter is zero-mean normally distributed with the variance $\sigma^2$.
These are expressed as the model equations:
\begin{align}
y_t &= \tau_t + \epsilon_t,\\
\tau_t &= \tau_{t-1} + \eta_t,\\
\epsilon_t &\sim\mathcal{N}\left(0, \sigma^2\right),\\
\eta_t &\sim\mathcal{N}\left(0, \sigma_\eta^2\right),
\end{align}
where the initial condition $\tau_0$ is a parameter of the model.

## Matrix notation for the model

To simplify the notation and the derivations introduce matrix notation for the model. Let $T$ be the available sample size for the variable $y$. 
Define a $T$-vector of zeros, $\mathbf{0}_T$, and of ones, $\boldsymbol\imath_T$, the identity matrix of order $T$, $\mathbf{I}_T$, as well as $T\times1$ vectors:
\begin{align}
\mathbf{y} = \begin{bmatrix} y_1\\ \vdots\\ y_T \end{bmatrix},\quad
\boldsymbol\tau = \begin{bmatrix} \tau_1\\ \vdots\\ \tau_T \end{bmatrix},\quad
\boldsymbol\epsilon = \begin{bmatrix} \epsilon_1\\ \vdots\\ \epsilon_T \end{bmatrix},\quad
\boldsymbol\eta = \begin{bmatrix} \eta_1\\ \vdots\\ \eta_T \end{bmatrix},\qquad
\mathbf{i} = \begin{bmatrix} 1\\0\\ \vdots\\ 0 \end{bmatrix},
\end{align}
and a $T\times T$ matrix $\mathbf{H}$ with the elements:
\begin{align}
\mathbf{H} = \begin{bmatrix}
1 & 0 & \cdots & 0 & 0\\
-1 & 1 & \cdots & 0 & 0\\
0 & -1 & \cdots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & \cdots & 1 & 0\\
0 & 0 & \cdots & -1 & 1
\end{bmatrix}.
\end{align}

Then the model can be written in a concise notation as:
\begin{align}
\mathbf{y} &= \mathbf{\tau} + \boldsymbol\epsilon,\\
\mathbf{H}\boldsymbol\tau &= \mathbf{i} \tau_0 + \boldsymbol\eta,\\
\boldsymbol\epsilon &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma^2\mathbf{I}_T\right),\\
\boldsymbol\eta &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma_\eta^2\mathbf{I}_T\right).
\end{align}

## Likelihood function

The model equations imply the predictive density of the data vector
$\mathbf{y}$. To see this, consider the model equation as a linear
transformation of a normal vector $\boldsymbol\epsilon$. Therefore, the data vector follows a multivariate normal distribution given by:
\begin{align}
\mathbf{y}\mid \boldsymbol\tau, \sigma^2 &\sim\mathcal{N}_T\left(\boldsymbol\tau, \sigma^2\mathbf{I}_T\right).
\end{align}

This distribution determines the shape of the likelihood function that
is defined as the sampling data density: 
\begin{align}
L(\boldsymbol\tau,\sigma^2|\mathbf{y})\equiv p\left(\mathbf{y}\mid\boldsymbol\tau, \sigma^2 \right).
\end{align}

The likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of $\mathbf{y}$, is considered a function of parameters $\boldsymbol\tau$ and $\sigma^2$ is given by: 
\begin{align}
L(\boldsymbol\tau,\sigma^2|\mathbf{y}) = 
(2\pi)^{-\frac{T}{2}}\left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}(\mathbf{y} - \boldsymbol\tau)'(\mathbf{y} - \boldsymbol\tau)\right\}.
\end{align}

## Prior distributions

# Bayesian estimation

## Gibbs sampler

## Simulation smoother and precision sampler

## Analytical solution for a joint posterior

# Hierarchical modeling

## Estimating gamma error term variance prior scale
To estimate the gamma error term variance, we need to firstly put a prior on the prior scale of and present its full conditional posterior distribution.

The prior structure is below:

```{=tex}
\begin{align*}
\sigma^2 \mid s &\sim \text{IG2}(s, \nu) \\
s &\sim \mathcal{G}(s, a)
\end{align*}
```


Then we use Gibbs sampling to draw samples from the full conditional distributions.


The code define the prior parameters for $s$ and $\sigma^2$ as 's_prior', 'nu_prior', 'a_prior' and 'b_prior'. Then we initialize samples for $s$ and $\sigma^2$. In each iteration, first update $\sigma^2$ then update $s$. Also, we run 1000 iterations of sampling and plot the posterior distributions. 


```{r}
#| echo: true
#| message: false
#| warning: false
library(MCMCpack)

# Define prior parameters
s_prior <- 2
nu_prior <- 2
a_prior <- 2
b_prior <- 1  

# Define the function for the conditional posterior distribution
posterior_sampler <- function(y, n_iter = 1000, s_prior, nu_prior, a_prior, b_prior) {
  # Initialize and store samples
  s <- rgamma(1, shape = a_prior, rate = b_prior)
  sigma2 <- rinvgamma(1, shape = s_prior, scale = nu_prior)  

  s_samples <- numeric(n_iter)
  sigma2_samples <- numeric(n_iter)

  for (i in 1:n_iter) {
    # Update sigma2
    shape_sigma2 <- (length(y) / 2) + s
    rate_sigma2 <- (sum((y - mean(y))^2) / 2) + nu_prior
    sigma2 <- rinvgamma(1, shape = shape_sigma2, scale = rate_sigma2)  

    # Update s
    shape_s <- a_prior + 1
    rate_s <- b_prior + sigma2
    s <- rgamma(1, shape = shape_s, rate = rate_s)

    # Store the samples
    sigma2_samples[i] <- sigma2
    s_samples[i] <- s
  }

  return(list(sigma2_samples = sigma2_samples, s_samples = s_samples))
}

set.seed(123)
y <- rnorm(100, mean = 0, sd = 1)

# Run the sampler
posterior_samples <- posterior_sampler(y, n_iter = 1000, s_prior, nu_prior, a_prior, b_prior)

# Plot posterior distributions
par(mfrow = c(2, 1))
hist(posterior_samples$sigma2_samples, main = expression(paste("Posterior of ", sigma^2)), xlab = expression(sigma^2))
hist(posterior_samples$s_samples, main = "Posterior of s", xlab = "s")

```

The results show that the posterior distribution of $\sigma^2$ lies between 0.6 and 1.2, indicating a high probability density within this interval.From the peak we can see the $\sigma^2$ are concentrated around 0.8.

The posterior distribution of $s$ lies between 0 and 4, indicating a high probability density within this interval.And the histogram shows that the values of $s$ are concentrated between 1 and 2.


For the full conditional posterior distribution of $\sigma^2$, we can drive it based on the prior information.

The observed data is normal distribution:

```{=tex}
\begin{align}
\mathbf{y} \mid \boldsymbol{\tau}, \sigma^2 \sim \mathcal{N}_T(\boldsymbol{\tau}, \sigma^2 I_T)
\end{align}
```

Then combine the observed data and $s$, we can get the likelihood function:

```{=tex}
\begin{align}
p(\mathbf{y} \mid \boldsymbol{\tau}, \sigma^2) = (2\pi\sigma^2)^{-T/2} \exp \left( -\frac{1}{2\sigma^2} (\mathbf{y} - \boldsymbol{\tau})' (\mathbf{y} - \boldsymbol{\tau}) \right)
\end{align}
```

The prior distribution is:


```{=tex}
\begin{align}
p(\sigma^2 \mid s) \propto (\sigma^2)^{-s-1} \exp \left( -\frac{\nu}{\sigma^2} \right)
\end{align}
```

Combine the likelihood function and prior distribution, we can know the posterior distribution:

```{=tex}
\begin{align}
p(\sigma^2 \mid \mathbf{y}, \boldsymbol{\tau}, s) \propto (\sigma^2)^{-s-1-T/2} \exp \left( -\frac{(\mathbf{y} - \boldsymbol{\tau})' (\mathbf{y} - \boldsymbol{\tau})/2 + \nu}{\sigma^2} \right)
\end{align}
```





## Estimating inverted-gamma 2 error term variance prior scale

## Estimating the initial condition prior scale

## Student-t prior for the trend component

## Estimating Student-t degrees of freedom parameter

## Laplace prior for the trend component

# Model extensions

## Estimation of autoregressive parameters for cycle component

## Autoregressive cycle component

## Random walk with  time-varying drift parameter

## Student-t error terms

## Conditional heteroskedasticity

# Bayesian forecasting

## Predictive density

## Sampling from the predictive density




## References {.unnumbered}
