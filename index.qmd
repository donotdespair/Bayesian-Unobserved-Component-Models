---
title: "Bayesian Unobserved Component Models"
author:
  - name: Tomasz WoÅºniak
    url: https://github.com/donotdespair
    orcid: 0000-0003-2212-2378
  - name: Rui Liu
    url: https://github.com/rruiliu1005
    orcid: 0009-0008-9348-8581

    execute:
  #echo: false
citation: 
  issued: 2024-05-01
  url: https://donotdespair.github.io/Bayesian-Unobserved-Component-Models/
  doi: 10.26188/25814617
bibliography: references.bib
---

> **Abstract.** We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. 
> The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. 
> We scrutinise Bayesian forecasting and sampling from the predictive density.
>
> **Keywords.** Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling

# Unobserved component models

Unobserved Component (UC) models are a popular class of models in macroeconometrics that use the state-space representation for unit-root nonstationary time series. 
The simple formulation of the model equations decomposing the series into a non-stationary and stationary component facilitates economic interpretations and good forecasting performance.

# A simple local-level model

The model is set for a univariate time series whose observation at time
$t$ is denoted by $y_t$. 
It decomposes the variable into a stochastic trend component, $\tau_t$, and a stationary error component, $\epsilon_t$. 
The former follows a Gaussian random walk process with the conditional variance $\sigma_\eta^2$, and the latter is zero-mean normally distributed with the variance $\sigma^2$.
These are expressed as the model equations:
\begin{align}
y_t &= \tau_t + \epsilon_t,\\
\tau_t &= \tau_{t-1} + \eta_t,\\
\epsilon_t &\sim\mathcal{N}\left(0, \sigma^2\right),\\
\eta_t &\sim\mathcal{N}\left(0, \sigma_\eta^2\right),
\end{align}
where the initial condition $\tau_0$ is a parameter of the model.

## Matrix notation for the model

To simplify the notation and the derivations introduce matrix notation for the model. Let $T$ be the available sample size for the variable $y$. 
Define a $T$-vector of zeros, $\mathbf{0}_T$, and of ones, $\boldsymbol\imath_T$, the identity matrix of order $T$, $\mathbf{I}_T$, as well as $T\times1$ vectors:
\begin{align}
\mathbf{y} = \begin{bmatrix} y_1\\ \vdots\\ y_T \end{bmatrix},\quad
\boldsymbol\tau = \begin{bmatrix} \tau_1\\ \vdots\\ \tau_T \end{bmatrix},\quad
\boldsymbol\epsilon = \begin{bmatrix} \epsilon_1\\ \vdots\\ \epsilon_T \end{bmatrix},\quad
\boldsymbol\eta = \begin{bmatrix} \eta_1\\ \vdots\\ \eta_T \end{bmatrix},\qquad
\mathbf{i} = \begin{bmatrix} 1\\0\\ \vdots\\ 0 \end{bmatrix},
\end{align}
and a $T\times T$ matrix $\mathbf{H}$ with the elements:
\begin{align}
\mathbf{H} = \begin{bmatrix}
1 & 0 & \cdots & 0 & 0\\
-1 & 1 & \cdots & 0 & 0\\
0 & -1 & \cdots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & \cdots & 1 & 0\\
0 & 0 & \cdots & -1 & 1
\end{bmatrix}.
\end{align}

Then the model can be written in a concise notation as:
\begin{align}
\mathbf{y} &= \mathbf{\tau} + \boldsymbol\epsilon,\\
\mathbf{H}\boldsymbol\tau &= \mathbf{i} \tau_0 + \boldsymbol\eta,\\
\boldsymbol\epsilon &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma^2\mathbf{I}_T\right),\\
\boldsymbol\eta &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma_\eta^2\mathbf{I}_T\right).
\end{align}

## Likelihood function

The model equations imply the predictive density of the data vector
$\mathbf{y}$. To see this, consider the model equation as a linear
transformation of a normal vector $\boldsymbol\epsilon$. Therefore, the data vector follows a multivariate normal distribution given by:
\begin{align}
\mathbf{y}\mid \boldsymbol\tau, \sigma^2 &\sim\mathcal{N}_T\left(\boldsymbol\tau, \sigma^2\mathbf{I}_T\right).
\end{align}

This distribution determines the shape of the likelihood function that
is defined as the sampling data density: 
\begin{align}
L(\boldsymbol\tau,\sigma^2|\mathbf{y})\equiv p\left(\mathbf{y}\mid\boldsymbol\tau, \sigma^2 \right).
\end{align}

The likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of $\mathbf{y}$, is considered a function of parameters $\boldsymbol\tau$ and $\sigma^2$ is given by: 
\begin{align}
L(\boldsymbol\tau,\sigma^2|\mathbf{y}) = 
(2\pi)^{-\frac{T}{2}}\left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}(\mathbf{y} - \boldsymbol\tau)'(\mathbf{y} - \boldsymbol\tau)\right\}.
\end{align}

## Prior distributions

# Bayesian estimation

## Gibbs sampler

## Simulation smoother and precision sampler

## Analytical solution for a joint posterior

Consider a simplified model with a fixed signal-to-noise ratio $c$ given by

\begin{align}
\text{measurement equation} && y &= \tau + \epsilon\\
\text{state equation} && \mathbf{H}\tau &= \eta\\
\text{error term} && \epsilon\mid\tau &\sim \mathcal{N}_T(\mathbf{0}_T, \sigma^2\mathbf{I}_T)\\
\text{innovation} && \eta &\sim \mathcal{N}_T(\mathbf{0}_T, c\sigma^2\mathbf{I}_T)
\end{align}
where $c$ is a constant, and $\tau_0 = 0$.


In this model the state equation for $\tau$ is given by a Gaussian random ralk presented as:

$$\begin{pmatrix} 
1 & 0 & \cdots & 0 \\
-1 & 1 & \cdots & 0 \\ 
\vdots & \ddots & \ddots & \vdots \\ 
0 & 0 & -1 & 1
\end{pmatrix} \begin{pmatrix} \tau_1 \\ \tau_2 \\ \vdots \\ \tau_T \end{pmatrix}  = \begin{pmatrix} \tau_1 \\ \tau_2-\tau_1 \\ \vdots \\ \tau_T - \tau_{T-1} \end{pmatrix} = \eta$$

The parameters to be estimated are: $\mathbf{\color{purple}{\tau, \sigma^2}}$

### Prior distributions 

#### Prior distribution for $\tau$ 

Rewrite the state equation as
$$\tau =  H^{-1}\eta$$
Then the prior distribution of $\tau$ is formed using
$$\mathbf{\color{purple}{\eta \sim \mathcal{N}(0_T, c \sigma^2 I_T) \implies H^{-1}\eta \sim \mathcal{N}(0_T, c\sigma^2(H^T H)^{-1})}}$$
since $Var(H^{-1}\eta) = H^{-1}Var(\eta)(H^{-1})^T =c\sigma^2(H^TH)^{-1}$

Then, the prior distribution of $\tau|\sigma^2$ is 
$$\tau|\sigma^2 \sim \mathcal{N}(0_T, c\sigma^2(H^TH)^{-1}) \\ \propto (c\sigma^2)^{-\frac{T}{2}}exp\left(-\frac{\tau^TH^T H\tau}{2c\sigma^2}\right)
$$

#### Prior assumptions for $\sigma^2$

$$\sigma^2 \sim \mathcal{IG2}(s_{prior}, \nu_{prior}) \propto (\sigma^2)^\frac{-\nu_{prior}+2}{2}exp\left(-\frac{s_{prior}}{2\sigma^2}\right)$$

### The Joint Posterior Distribution

The likelihood function is constructed using the the measurement equation: 
$$y = \tau + \epsilon$$
and the distributional assumption on its error term
$$\epsilon \sim \mathcal{N}(0_T, \sigma^2I_T)$$
which results in the likelihood function:
$$y|\tau, \sigma^2 \sim \mathcal{N}(\tau, \sigma^2I_T)\propto (\sigma^2)^{-\frac{T}{2}}exp\left(-\frac{1}{2\sigma^2}(y-\tau)^T(y-\tau)\right)$$

The likelihood is combined with the joint prior distribution of $\tau$ and $\sigma^2$ to derive the joint posterior distribution of $\tau$ and $\sigma^2$. It is derived as follows: 

$$
p(\tau, \sigma^2 |y) = \frac{p(\tau, \sigma^2, y)}{p(y)} \propto p(\tau, \sigma^2, y) = p(y | \tau, \sigma^2)p(\tau, \sigma^2) =p(y | \tau, \sigma^2)p(\tau|\sigma^2)p(\sigma^2)
$$

This expression is proportional to
\begin{align}
&\propto (\sigma^2)^{-\frac{T}{2}}exp\left(-\frac{(y-\tau)^T(y-\tau)}{2\sigma^2}\right) \times 
(\sigma^2)^{-\frac{T}{2}}\exp\left(-\frac{\tau^TH^T H\tau}{2c\sigma^2}\right)\\
&\qquad\times(\sigma^2)^{-\frac{\nu_{prior}+2}{2}} \exp(-\frac{s_{prior}}{2\sigma^2})\\
&\propto \exp(-\frac{y^Ty - 2\tau^T y + \tau^T\tau + c^{-1}\tau^TH^T H\tau}{2\sigma^2})\\ &\qquad\times \exp(-\frac{s_{prior}}{2\sigma^2})\times(\sigma^2)^{-\frac{2T+\nu_{prior}+2}{2}}\\
&= \exp(-\frac{\tau^T(c^{-1}H^T H + I_T)\tau - 2\tau^Ty}{2\sigma^2})\\ 
&\qquad\times\exp(-\frac{y^Ty + s_{prior}}{2\sigma^2})\times(\sigma^2)^{-\frac{2T+\nu_{prior}+2}{2}}
\end{align}

Let $\bar{\Sigma} = (c^{-1}H^T H + I_T)^{-1}$, then
\begin{align}
p(\tau, \sigma^2 |y) &\propto \exp(-\frac{\tau^T\bar{\Sigma}^{-1}\tau - 2\tau^Ty + y^T\bar{\Sigma}y}{2\sigma^2})\exp(-\frac{y^Ty + s_{prior}-y^T\bar{\Sigma}y}{2\sigma^2})\\
&\qquad\times(\sigma^2)^{-\frac{2T+\nu_{prior}+2}{2}}
\end{align}

In the expression above, we recognise the kernel of the $T$-variate normal inverted gamma 2 distribution. Therefore, the joint posterior of the parameters of the model is given by:
\begin{align}
\tau, \Sigma |y &\sim \mathcal{NIG2}(\bar{\tau}, \bar{\Sigma}, \bar{\nu}, \bar{s})\\[2ex]
\bar{\Sigma} &= (c^{-1}H^T H + I_T)^{-1}\\
\bar{\tau} &= \bar{\Sigma}y\\
\bar{\nu} &= 2T+\nu_{prior}\\
\bar{s} &= s_{prior}+y^Ty-y^T\bar{\Sigma}y
\end{align}

The function below implements the sampler from this joint distribution.

```{r joint_sampler}

UC.local.tau.sigma.Gibbs.sampler    = function(S, starting.values, priors){
  
  aux     = starting.values
  T       = nrow(aux$Y)

  posteriors    = list(
    tau     = matrix(NA,T,S),
    sigma   = rep(NA,S)
  )

  for (s in 1:S){
    
    V.tau.bar.inv = priors$c^(-1)*t(aux$H)%*%aux$H + diag(T)
    V.tau.bar.inv = 0.5*(V.tau.bar.inv + t(V.tau.bar.inv))
    
    # Sampling sigma
    ###########################
    sigma.s   = as.numeric(priors$sigma.s + t(aux$Y)%*%aux$Y - t(aux$tau)%*%V.tau.bar.inv%*%aux$tau)
    sigma.nu  = priors$sigma.nu + 2*T
    sigma.draw= sigma.s/rchisq(1,sigma.nu)
    aux$sigma     = sigma.draw

    # Sampling tau
    ###########################
    b.tau         = aux$Y
    precision.L   = t(bandchol(V.tau.bar.inv))
    epsilon       = rnorm(T)
    b.tau.tmp     = forwardsolve(precision.L, b.tau)
    tau.draw      = backsolve(t(precision.L), b.tau.tmp + epsilon)
    aux$tau       = tau.draw

    posteriors$tau[,s]     = aux$tau
    posteriors$sigma[,s]   = aux$sigma
  }

  output      = list(
    posterior = posteriors,
    last.draw = aux
  )
  return(output)
}


```
# Hierarchical modeling

## Estimating gamma error term variance prior scale


## Estimating inverted-gamma 2 error term variance prior scale

## Estimating the initial condition prior scale

## Student-t prior for the trend component

## Estimating Student-t degrees of freedom parameter

## Laplace prior for the trend component

# Model extensions

## Autoregressive cycle component

## Random walk with  time-varying drift parameter

## Student-t error terms

## Conditional heteroskedasticity

# Bayesian forecasting

## Predictive density

## Sampling from the predictive density

## Missing observations


## References {.unnumbered}
