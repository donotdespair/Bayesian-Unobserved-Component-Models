[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Unobserved Component Models",
    "section": "",
    "text": "Abstract. We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. We scrutinise Bayesian forecasting and sampling from the predictive density.\nKeywords. Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling"
  },
  {
    "objectID": "index.html#matrix-notation-for-the-model",
    "href": "index.html#matrix-notation-for-the-model",
    "title": "Bayesian Unobserved Component Models",
    "section": "Matrix notation for the model",
    "text": "Matrix notation for the model\nTo simplify the notation and the derivations introduce matrix notation for the model. Let \\(T\\) be the available sample size for the variable \\(y\\). Define a \\(T\\)-vector of zeros, \\(\\mathbf{0}_T\\), and of ones, \\(\\boldsymbol\\imath_T\\), the identity matrix of order \\(T\\), \\(\\mathbf{I}_T\\), as well as \\(T\\times1\\) vectors:\n\\[\n\\begin{align}\n\\mathbf{y} = \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_T \\end{bmatrix},\\quad\n\\boldsymbol\\tau = \\begin{bmatrix} \\tau_1\\\\ \\vdots\\\\ \\tau_T \\end{bmatrix},\\quad\n\\boldsymbol\\epsilon = \\begin{bmatrix} \\epsilon_1\\\\ \\vdots\\\\ \\epsilon_T \\end{bmatrix},\\quad\n\\boldsymbol\\eta = \\begin{bmatrix} \\eta_1\\\\ \\vdots\\\\ \\eta_T \\end{bmatrix},\\qquad\n\\mathbf{i} = \\begin{bmatrix} 1\\\\0\\\\ \\vdots\\\\ 0 \\end{bmatrix},\n\\end{align}\n\\] and a \\(T\\times T\\) matrix \\(\\mathbf{H}\\) with the elements:\n\\[\n\\begin{align}\n\\mathbf{H} = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 & 0\\\\\n-1 & 1 & \\cdots & 0 & 0\\\\\n0 & -1 & \\cdots & 0 & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n0 & 0 & \\cdots & 1 & 0\\\\\n0 & 0 & \\cdots & -1 & 1\n\\end{bmatrix}.\n\\end{align}\n\\]\nThen the model can be written in a concise notation as: \\[\n\\begin{align}\n\\mathbf{y} &= \\mathbf{\\tau} + \\boldsymbol\\epsilon,\\\\\n\\mathbf{H}\\boldsymbol\\tau &= \\mathbf{i} \\tau_0 + \\boldsymbol\\eta,\\\\\n\\boldsymbol\\epsilon &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma^2\\mathbf{I}_T\\right),\\\\\n\\boldsymbol\\eta &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma_\\eta^2\\mathbf{I}_T\\right).\n\\end{align}\n\\]"
  },
  {
    "objectID": "index.html#likelihood-function",
    "href": "index.html#likelihood-function",
    "title": "Bayesian Unobserved Component Models",
    "section": "Likelihood function",
    "text": "Likelihood function\nThe model equations imply the predictive density of the data vector \\(\\mathbf{y}\\). To see this, consider the model equation as a linear transformation of a normal vector \\(\\boldsymbol\\epsilon\\). Therefore, the data vector follows a multivariate normal distribution given by: \\[\n\\begin{align}\n\\mathbf{y}\\mid \\boldsymbol\\tau, \\sigma^2 &\\sim\\mathcal{N}_T\\left(\\boldsymbol\\tau, \\sigma^2\\mathbf{I}_T\\right).\n\\end{align}\n\\]\nThis distribution determines the shape of the likelihood function that is defined as the sampling data density: \\[\n\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y})\\equiv p\\left(\\mathbf{y}\\mid\\boldsymbol\\tau, \\sigma^2 \\right).\n\\end{align}\n\\]\nThe likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of \\(\\mathbf{y}\\), is considered a function of parameters \\(\\boldsymbol\\tau\\) and \\(\\sigma^2\\) is given by: \\[\n\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y}) =\n(2\\pi)^{-\\frac{T}{2}}\\left(\\sigma^2\\right)^{-\\frac{T}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y} - \\boldsymbol\\tau)'(\\mathbf{y} - \\boldsymbol\\tau)\\right\\}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "index.html#prior-distributions",
    "href": "index.html#prior-distributions",
    "title": "Bayesian Unobserved Component Models",
    "section": "Prior distributions",
    "text": "Prior distributions\nThe state equation for \\(\\boldsymbol\\tau\\) can be rewritten as follows: \\[\n\\begin{gather}\n\\boldsymbol\\tau = \\mathbf{H}^{-1} \\mathbf{i} \\tau_0+\\mathbf{H}^{-1}\\boldsymbol\\eta \\\\\n\\boldsymbol\\eta \\sim \\mathcal{N}(\\mathbf{0}_T, \\sigma_\\eta^2 \\mathbf{I}_T) \\\\\n\\mathbf{H}^{-1} \\boldsymbol\\eta \\sim \\mathcal{N}(\\mathbf{0}_T, \\sigma_\\eta^2\\left(\\mathbf{H}^{\\prime} \\mathbf{H}\\right)^{-1})\n\\end{gather}\n\\] Using the state equation for \\(\\boldsymbol\\tau\\) above, we can derive the prior distribution of \\(\\boldsymbol\\tau\\) as: \\[\n\\begin{align}\n\\boldsymbol\\tau | \\tau_0, \\sigma_\\eta^2 &\\sim \\mathcal{N}_T(\\mathbf{H}^{-1} \\mathbf{i} \\tau_0, \\sigma_\\eta^2(\\mathbf{H}^{\\prime} \\mathbf{H})^{-1})\n\\\\ &\\propto \\exp \\left\\{-\\frac{1}{2} \\frac{1}{\\sigma_\\eta^2}\\left(\\boldsymbol\\tau-\\mathbf{H}^{-1} \\mathbf{i} \\tau_0\\right)^{\\prime}\\left(\\mathbf{H}^{\\prime} \\mathbf{H}\\right)\\left(\\boldsymbol\\tau-\\mathbf{H}^{-1} \\mathbf{i} \\tau_0\\right)\\right\\}\n\\end{align}\n\\] Next, the prior distribution of \\(\\tau_0\\) is defined as uni-variate normal: \\[\n\\begin{align}\n\\tau_0 &\\sim \\mathcal{N}(\\underline{\\tau_0}, \\underline{V_{\\tau_{0}}})\n\\\\ &\\propto \\exp \\left\\{-\\frac{1}{2}(\\tau_0-\\underline{\\tau_0})^{\\prime} \\underline{V_{\\tau_{0}}}^{-1}(\\tau_0-\\underline{\\tau_0})\\right\\}\n\\end{align}\n\\] and the prior distribution of \\(\\sigma_\\eta^2\\) is inverted gamma 2: \\[\n\\begin{align}\n\\sigma_\\eta^2 &\\sim \\mathcal{IG}2(\\underline{s_\\eta}, \\underline{v_\\eta}) \\\\\n&\\propto (\\sigma_\\eta^2)^{-\\frac{\\underline{v}+2}{2}} \\exp \\left\\{-\\frac{1}{2} \\frac{s}{\\sigma_\\eta^2}\\right\\}\n\\end{align}\n\\]\nThe prior distribution of \\(\\sigma^2\\) is the following: \\[\n\\begin{align}\n\\sigma^2 &\\sim \\mathcal{IG}2(\\underline{s}, \\underline{v}) \\\\\n&\\propto (\\sigma^2)^{-\\frac{\\underline{v}+2}{2}} \\exp \\left\\{-\\frac{1}{2} \\frac{s}{\\sigma^2}\\right\\}\n\\end{align}\n\\]\nThe Joint prior distribution of the parameters of the model \\(\\boldsymbol\\tau\\), \\(\\tau_0\\), \\(\\sigma^2_{\\eta}\\), and \\(\\sigma^2\\) is then given by: \\[\n\\begin{aligned}\np(\\boldsymbol\\tau,\\tau_{0},\\sigma_{\\eta}^{2},\\sigma^{2}) = p(\\boldsymbol\\tau|\\tau_{0},\\sigma_{\\eta}^{2}) \\space p(\\tau_{0}) \\space p(\\sigma_{\\eta}^{2}) \\space p(\\sigma^{2})\n\\end{aligned}\n\\] where the individual distributions on the RHS are as specified above.\nFinally, given that \\(\\boldsymbol\\epsilon\\) follows normal distribution, \\[\n\\begin{align}\n\\boldsymbol\\epsilon\\mid\\sigma^2 &\\sim \\mathcal{N}(\\mathbf{0}_T, \\sigma^2 \\mathbf{I}_T)\n\\end{align}\n\\] the prior distribution of \\(\\boldsymbol\\epsilon\\) given \\(\\sigma^2\\) is proportional to: \\[\n\\begin{align}\n\\boldsymbol\\epsilon | \\sigma^2 &\\propto \\exp \\left\\{-\\frac{1}{2} \\frac{1}{\\sigma^2} \\boldsymbol\\epsilon^{\\prime}\\boldsymbol\\epsilon\\right\\}\n\\end{align}\n\\]"
  },
  {
    "objectID": "index.html#derivation-of-full-conditional-posterior-distributions",
    "href": "index.html#derivation-of-full-conditional-posterior-distributions",
    "title": "Bayesian Unobserved Component Models",
    "section": "Derivation of full conditional posterior distributions",
    "text": "Derivation of full conditional posterior distributions\nFull conditional posterior distribution could be estimated basing on Bayesâ€™ theorem and priors shown above. Therefore, the full conditional posterior distribution is proportional to the product of likelihood functions and priors. \\[\n\\begin{aligned}\np(\\tau,\\tau_{0},\\sigma^{2},\\sigma_{\\eta}^{2}|\\mathbf{y}) & \\propto p(\\mathbf{y}|\\tau,\\tau_{0},\\sigma^{2},\\sigma_{\\eta}^{2}) \\space p(\\tau,\\tau_{0},\\sigma^{2},\\sigma_{\\eta}^{2})\n\\end{aligned}\n\\] The joint prior is as specified in the section above.\nTwo Conditional likelihood functions to derive the full Conditional posterior distribution of \\(\\tau,\\tau_{0},\\sigma^{2},\\sigma_{\\eta}^{2}\\) is as follows: \\[\n\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^{2}|\\mathbf{y},\\tau_{0},\\sigma_{\\eta}^{2}) \\propto \\left( \\sigma^{2} \\right)^{-\\frac{T}{2}} \\times\\exp\\left\\{ -\\frac{1}{2}\\sigma^{-2}\\left( \\mathbf{y}-\\boldsymbol\\tau \\right)'\\left( \\mathbf{y}-\\boldsymbol\\tau \\right) \\right\\}\n\\end{align}\n\\] and \\[\n\\begin{align}\nL(\\tau_{0},\\sigma_{\\eta}^{2}|\\mathbf{y},\\boldsymbol\\tau,\\sigma^{2}) \\propto \\left( \\sigma_{\\eta}^{2} \\right)^{-\\frac{T}{2}} \\times\\exp\\left\\{-\\frac{1}{2}\\sigma_{\\eta}^{-2}\\left(\\mathbf{H}\\boldsymbol\\tau -\\mathbf{i}\\tau_{0}\\right)'\\left(\\mathbf{H}\\boldsymbol\\tau-\\mathbf{i}\\tau_{0}\\right)\\right\\}\n\\end{align}\n\\]\nThe full Conditional posterior distribution of \\(\\boldsymbol\\tau\\) is derived as follows: \\[\n\\begin{align}\nP(\\boldsymbol\\tau|\\mathbf{y},\\tau_{0},\\sigma^{2},\\sigma_{\\eta}^{2})&\\propto L(\\boldsymbol\\tau,\\sigma^{2}|\\mathbf{y},\\tau_{0},\\sigma_{\\eta}^{2}) \\space P(\\boldsymbol\\tau|\\tau_{0},\\sigma_{\\eta}^{2})\\\\\n\\\\&\\propto \\exp\\left\\{ -\\frac{1}{2}\\sigma^{-2}\\left( \\mathbf{y}-\\boldsymbol\\tau \\right)'\\left( \\mathbf{y}-\\boldsymbol\\tau \\right) \\right\\}\\\\\n&\\qquad\\times \\exp\\left\\{ -\\frac{1}{2}\\sigma_{\\eta}^{-2}\\left(\\boldsymbol\\tau-\\mathbf{H}^{-1} \\mathbf{i}\\tau_{0}\\right)'\\mathbf{H}'\\mathbf{H}\\left( \\boldsymbol\\tau-\\mathbf{H}^{-1} \\mathbf{i}\\tau_{0} \\right) \\right\\}\n\\end{align}\n\\] that after completing th squares results in a multivariate normal full conditional posterior distribution: \\[\n\\begin{align}\nP(\\boldsymbol\\tau &|\\mathbf{y},\\tau_{0},\\sigma^{2},\\sigma_{\\eta}^{2}) \\sim \\mathcal{N}(\\overline{\\boldsymbol\\tau},\\overline{V_{\\boldsymbol\\tau}}) \\\\\n\\\\ \\overline{V_{\\boldsymbol\\tau}} &= \\left[ \\sigma^{-2}\\mathbf{I}_T +\\sigma_{\\eta}^{-2}\\mathbf{H}'\\mathbf{H}\\right]^{-1}\\\\\n\\overline{\\boldsymbol\\tau} &= \\overline{V_{\\tau}}\\left[\\sigma^{-2}\\mathbf{y} + \\sigma_{\\eta}^{-2}\\mathbf{H}'\\mathbf{i}\\tau_{0} \\right]\n\\end{align}\n\\] The full conditional posterior distribution of \\(\\tau_{0}\\) is the following: \\[\n\\begin{align}\nP(\\tau_{0}|\\mathbf{y},\\boldsymbol\\tau,\\sigma^{2},\\sigma_{\\eta}^{2}) &\\propto L(\\tau_{0}|\\mathbf{y},\\boldsymbol\\tau,\\sigma^{2},\\sigma_{\\eta}^{2}) \\space P(\\tau_{0}|\\sigma_{\\eta}^{2}) \\\\\n\\\\ &\\propto \\exp\\left\\{ -\\frac{1}{2}\\sigma_{\\eta}^{-2}\\left(\\mathbf{H}\\boldsymbol\\tau -\\mathbf{i}\\tau_{0}\\right)'\\left(\\mathbf{H}\\boldsymbol\\tau-\\mathbf{i}\\tau_{0}\\right)\\right\\}\\\\\n&\\times \\exp\\left\\{ -\\frac{1}{2}\\left(\\tau_{0}-  \\underline{\\tau_0}\\right)'\\underline{V_{\\tau_{0}}}^{-1}\\left(\\tau_{0}-  \\underline{\\tau_0}\\right) \\right\\}\n\\end{align}\n\\] and is given by a uni-variate normal distribution: \\[\n\\begin{align}\nP(\\tau_{0} &|\\mathbf{y},\\boldsymbol\\tau,\\sigma^{2},\\sigma_{\\eta}^{2}) \\sim N(\\overline{\\tau_{0}},\\overline{V_{\\tau_{0}}}) \\\\\n\\\\ \\overline{V_{\\tau_{0}}}&=\\left[ \\sigma_{\\eta}^{-2}\\mathbf{i}'\\mathbf{i} +\\underline{V_{\\tau_{0}}}^{-1}\\right]^{-1} =\\left[ \\sigma_{\\eta}^{-2} +\\underline{V_{\\tau_{0}}}^{-1}\\right]^{-1} \\\\\n\\overline{\\tau_{0}}&=\\overline{V_{\\tau_{0}}}\\left[ \\sigma_{\\eta}^{-2} \\mathbf{i}'\\mathbf{H}\\boldsymbol\\tau +\\underline{V_{\\tau_{0}}}^{-1}\\underline{\\tau_{0}}\\right]\\\\\n&= \\overline{V_{\\tau_{0}}}\\left[ \\sigma_{\\eta}^{-2} \\tau_1 +\\underline{V_{\\tau_{0}}}^{-1}\\underline{\\tau_{0}}\\right]\n\\end{align}\n\\] The full conditional posterior distribution of \\(\\sigma_{\\eta}^{2}\\) is derived as: \\[\n\\begin{align}\nP(\\sigma_{\\eta}^{2}|\\mathbf{y},\\boldsymbol\\tau,\\tau_{0},\\sigma^{2}) &\\propto L(\\sigma_{\\eta}^{2}|\\mathbf{y},\\boldsymbol\\tau,\\tau_{0},\\sigma^{2}) \\space P(\\sigma_{\\eta}^{2}) \\\\\n\\\\ &\\propto \\left( \\sigma_{\\eta}^{2} \\right)^{-\\frac{T}{2}} \\exp\\left\\{ -\\frac{1}{2}\\sigma_{\\eta}^{-2}\\left(\\mathbf{H}\\boldsymbol\\tau -\\mathbf{i}\\tau_{0}\\right)'\\left(\\mathbf{H}\\boldsymbol\\tau -\\mathbf{i}\\tau_{0}\\right)\\right\\}\n\\\\ &\\times \\left( \\sigma_{\\eta}^{2} \\right)^{-\\frac{\\underline{\\upsilon}+2}{2}} \\exp\\left\\{ -\\frac{1}{2}\\frac{\\underline{s}}{\\sigma_{\\eta}^{2}}\\right\\}\n\\end{align}\n\\] where \\[\n\\begin{align}\nP(\\sigma_{\\eta}^{2}&|\\mathbf{y},\\boldsymbol\\tau,\\tau_{0},\\sigma^{2}) \\sim \\mathcal{IG}2(\\overline{s_{\\eta}},\\overline{\\upsilon_{\\eta}}) \\\\\n\\\\ \\overline{s_{\\eta}}&=\\underline{s} + \\left(\\mathbf{H}\\boldsymbol\\tau -\\mathbf{i}\\tau_{0}\\right)'\\left(\\mathbf{H}\\boldsymbol\\tau -\\mathbf{i}\\tau_{0}\\right) \\\\\n\\overline{\\upsilon_{\\eta}}&= \\underline{\\upsilon}+T\n\\end{align}\n\\] Finally, the full conditional posterior distribution of \\(\\sigma^{2}\\) is the following: \\[\n\\begin{align}\nP(\\sigma^{2}|\\mathbf{y},\\boldsymbol\\tau,\\tau_{0},\\sigma_{\\eta}^{2})&=L(\\sigma^{2}|\\mathbf{y},\\boldsymbol\\tau,\\tau_{0},\\sigma_{\\eta}^{2}) \\space P(\\sigma^{2}) \\\\\n\\\\ &\\propto \\left( \\sigma^{2} \\right)^{-\\frac{T}{2}} \\exp\\left\\{ -\\frac{1}{2}\\sigma^{-2}\\left(\\mathbf{y}-\\boldsymbol\\tau\\right)'\\left(\\mathbf{y}-\\boldsymbol\\tau\\right)\\right\\}\n\\\\ &\\times \\left( \\sigma^{2} \\right)^{-\\frac{\\underline{\\upsilon}+2}{2}} \\exp\\left\\{ -\\frac{1}{2}\\frac{\\underline{s}}{\\sigma^{2}}\\right\\}\n\\end{align}\n\\] where \\[\n\\begin{align}\nP(\\sigma^{2}|&\\mathbf{y},\\boldsymbol\\tau,\\tau_{0},\\sigma_{\\eta}^{2}) \\sim \\mathcal{IG}2(\\overline{s},\\overline{\\upsilon}) \\\\\n\\\\ \\overline{s}&=\\underline{s} + \\left(\\mathbf{y}-\\boldsymbol\\tau\\right)'\\left(\\mathbf{y}-\\boldsymbol\\tau\\right) \\\\\n\\overline{\\upsilon}&= \\underline{\\upsilon}+T\n\\end{align}\n\\]"
  },
  {
    "objectID": "index.html#gibbs-sampler",
    "href": "index.html#gibbs-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nA Gibbs Sampler can be applied using the following steps:\nAt each iteration \\(s\\) where \\(s\\) goes from 1 to \\(S\\),\nÂ Â Â Step 1. Draw \\(\\tau_0^{(s)}\\) from the \\(N({\\tau_{0}}, {V_{\\tau_{0}}})\\) distribution and collect \\(\\tau_0\\).\nÂ Â Â Step 2. Draw \\({\\sigma^2_{\\eta}}^{(s)}\\) from the \\(\\mathcal{IG}2({S_\\eta}, {v_\\eta})\\) distribution and collect \\(\\sigma_\\eta^2\\).\nÂ Â Â Step 3. Draw \\({\\sigma^2}^{(s)}\\) from the \\(\\mathcal{IG}2(S, v)\\) distribution and collect \\(\\sigma^2\\).\nÂ Â Â Step 4. Draw \\(\\boldsymbol\\tau^{(s)}\\) from the \\(\\mathcal{N}({\\boldsymbol\\tau},{V_{\\tau}})\\) distribution and collect \\(\\boldsymbol\\tau\\).\n\n# Gibbs sampler for a simple UC model using simulation smoother\n############################################################\n\nUC.Gibbs.sampler    = function(S, starting.values, priors){\n  # Initialize the data \n  aux     = starting.values\n  T       = nrow(aux$Y)\n  i_matrix &lt;- diag(T)\n  i &lt;- matrix(0, T, 1)  \n  i[1, 1] &lt;- 1 \n  # Posteriors list\n  posteriors    = list(\n    tau     = matrix(NA,T,S),\n    tau_0   = matrix(NA,1,S),\n    sigma   = matrix(NA,2,S)\n  )\n  HH = crossprod(priors$H)\n  \n  for (s in 1:S){\n    \n    # Sampling tau_0\n    ###########################\n    tau_0.v.inv    = 1/priors$tau_0.v\n    V.tau_0.bar    = 1/((1/aux$sigma[1]) + tau_0.v.inv )\n    tau_0.bar      = V.tau_0.bar %*% ( (1/aux$sigma[1])*aux$tau[1] + tau_0.v.inv*priors$tau_0 )\n    tau_0.draw     = rnorm(1,as.vector(tau_0.bar),sqrt(V.tau_0.bar))\n    aux$tau_0      = tau_0.draw\n    \n    # Sampling sigma\n    ###########################\n    # sigma of tau (sigma_eta)\n    sigma.eta.s   = as.numeric(priors$sigma.s + crossprod(priors$H%*%aux$tau - i%*%priors$tau_0))\n    sigma.eta.nu  = priors$sigma.nu + T\n    sigma.eta.draw = sigma.eta.s/rchisq(1,sigma.eta.nu)\n    # sigma of errors (sigma)\n    sigma.e.s     = as.numeric(priors$sigma.s + crossprod(aux$Y - aux$tau))\n    sigma.e.nu    = priors$sigma.nu + T\n    sigma.e.draw  = sigma.e.s/rchisq(1,sigma.e.nu)\n    aux$sigma     = c(sigma.eta.draw,sigma.e.draw)\n    \n    # Sampling tau\n    ###########################\n    V.tau.inv     = (1/aux$sigma[2])*i_matrix + (1/aux$sigma[1])*HH\n    V.tau.inv     = 0.5*(V.tau.inv + t(V.tau.inv))\n    b.tau         = (1/aux$sigma[2])*aux$Y + (1/aux$sigma[1])*t(priors$H)%*%i*priors$tau_0\n    precision.L   = t(bandchol(V.tau.inv))\n    epsilon       = rnorm(T)\n    b.tau.tmp     = forwardsolve(precision.L, b.tau)\n    tau.draw      = backsolve(t(precision.L), b.tau.tmp + epsilon)\n    aux$tau       = tau.draw\n    \n    posteriors$tau[,s]     = aux$tau\n    posteriors$tau_0[,s]   = aux$tau_0\n    posteriors$sigma[,s]   = aux$sigma\n    \n    if (s%%1000==0){cat(\" \",s)}\n  }\n  \n  output      = list(\n    posterior = posteriors,\n    last.draw = aux\n  )\n  return(output)\n}"
  },
  {
    "objectID": "index.html#simulation-smoother-and-precision-sampler",
    "href": "index.html#simulation-smoother-and-precision-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Simulation smoother and precision sampler",
    "text": "Simulation smoother and precision sampler\nSimulation Smoother is used when drawing state variables in discrete time state-space models from their conditional distribution. This tool greatly improved Bayesian estimation process in conditional linear Gaussian state-space models. The benefit of using this method comes from the fact that one does not rely on an iterative procedure of forward filtering and backward smoothing. Instead, this step is performed all without a loop.\nConsider sampling random draws from a multivariate normal distribution presented as:\n\\[\nN_{T}(D^{-1}b,D^{-1})\n\\]\nwhere: \\(D\\) is a \\(T \\times T\\) precision matrix of covariance matrix \\(\\Sigma\\), such that \\(D = \\Sigma^{-1}\\), and \\(b\\) is an \\(T\\times1\\) restriction vector to specify the Mean of multivariate normal distribution, where \\(M = \\Sigma b\\).\nThis parameterisation is particularly useful as matrices \\(D\\) and \\(b\\) can be computed easily. Additionally, matrix \\(D\\) that in the simulation smoother for state-space models is a band matrix and often a tridiagonal matrix. The latter case arises for a one-lag autoregressive state-space process. Therefore, the computation time is much shorter if dedicated numerical algorithms are employed. For tridiagnal matrix \\(D\\) in state space model, let \\(L\\) be a lower-triangular matrix obtained by Cholesky decomposition, where \\(D = LL'\\) (Assume that \\(L^{-1}\\) can be compute efficiently).\nTherefore, the mean of the normal distribution above can be presented as:\n\\[\nM = D^{-1}b = (LL')^{-1}b = L'^{-1}L^{-1}b\n\\]\nLet a \\(T \\times 1\\) vector x contain \\(T\\) independent random draws from a standard normal distribution, \\(X\\sim N_{T} ( 0_{T}, I_{T})\\). Therefore, the target distribution, \\(Y\\sim N_{T}(D^{-1}b,D^{-1})\\), can be treated as:\n\\[\nY = D^{-1}b + \\sqrt{D^{-1}} X = L'^{-1}L^{-1}b + L'^{-1} X\n\\]\nTherefor a draw form the target distribution, \\(N_{T}(D^{-1}b,D^{-1})\\), itâ€™s equivalent to:\n\\[\nL'^{-1}L^{-1}b + L'^{-1}x = L'^{-1}(L^{-1}b + x)\n\\]\nFollowing algorithm base on solving a linear equation by back-substitution rather than computation of the inverse matrix:\n\nCompute \\(L = chol(D)\\) such that \\(D = LL'\\)\nSample \\(x\\sim N_{T} ( 0_{T}, I_{T})\\)\nCompute a draw from the distribution via the affine transformation: \\(L '\\smallsetminus (L \\smallsetminus b + x)\\)\n\nNote: Let \\(L \\smallsetminus b\\) denote the unique solution to the triangular system Lx = b obtained by forward (backward) substitution, that is, \\(L \\smallsetminus b = L^{âˆ’1}b\\).\n\nSimulation smoother algorithm in R\nCompare the computational speed of generating random numbers from a multivariate normal distribution using dedicated numerical algorithms in function rmvnorm.tridiag.precision and function solve for the matrix inversion ignoring the tridiagonality of the precision matrix in function rmvnorm.usual:\n\nlibrary(mgcv)\nrmvnorm.tridiag.precision = function(n, D, b){\n  N           = dim(D)[1]\n  lead.diag   = diag(D)\n  sub.diag    = sdiag(D, -1)\n  D.chol      = trichol(ld = lead.diag, sd=sub.diag)\n  D.L         = diag(D.chol$ld)\n  sdiag(D.L,-1) = D.chol$sd\n  x           = matrix(rnorm(n*N), ncol=n)\n  a           = forwardsolve(D.L, b)\n  draw        = backsolve(t(D.L), matrix(rep(a,n), ncol=n) + x)\n  return(draw)\n}\n\n# Function of normal method\nrmvnorm.usual = function(n, D, b){\n  N           = dim(D)[1]\n  D.chol      = t(chol(D))\n  variance.chol = solve(D.chol)\n  x           = matrix(rnorm(n*N), ncol=n)\n  draw        = t(variance.chol) %*%\n           (matrix(rep(variance.chol%*%b,n), ncol=n) + x)\n  return(draw)\n}\n\n# Comparison\nset.seed(12345)\nT = 300\nmd = rgamma(T, shape=10, scale=10) \nod = rgamma(T-1, shape=10, scale=1) \nD = 2*diag(md)\nsdiag(D, 1) = -od\nsdiag(D, -1) = -od\nb = as.matrix(rnorm(T))\n\nmicrobenchmark(\n  trid  = rmvnorm.tridiag.precision(n=100, D=D, b=b),\n  usual = rmvnorm.usual(n=100, D=D, b=b),\n  check = \"equal\", setup=set.seed(123456)\n)\n\nWarning in microbenchmark(trid = rmvnorm.tridiag.precision(n = 100, D = D, :\nless accurate nanosecond times to avoid potential integer overflows\n\n\nUnit: milliseconds\n  expr       min        lq      mean    median        uq      max neval\n  trid  3.099969  3.207758  4.792219  3.252038  3.286868 53.63788   100\n usual 22.485384 22.684173 23.063725 22.761212 22.950755 25.30709   100\n\n\nTherefore, we could clearly see that the computational time drops substatially due to simulation smoothing method."
  },
  {
    "objectID": "index.html#analytical-solution-for-a-joint-posterior-distribution",
    "href": "index.html#analytical-solution-for-a-joint-posterior-distribution",
    "title": "Bayesian Unobserved Component Models",
    "section": "Analytical solution for a joint posterior distribution",
    "text": "Analytical solution for a joint posterior distribution\nConsider a simplified model with a fixed signal-to-noise ratio \\(c\\) given by\n\\[\\begin{align}\n\\text{measurement equation} && y &= \\tau + \\epsilon\\\\\n\\text{state equation} && \\mathbf{H}\\tau &= \\eta\\\\\n\\text{error term} && \\epsilon\\mid\\tau &\\sim \\mathcal{N}_T(\\mathbf{0}_T, \\sigma^2\\mathbf{I}_T)\\\\\n\\text{innovation} && \\eta &\\sim \\mathcal{N}_T(\\mathbf{0}_T, c\\sigma^2\\mathbf{I}_T)\n\\end{align}\\] where \\(c\\) is a constant, and \\(\\tau_0 = 0\\).\nIn this model the state equation for \\(\\tau\\) is given by a Gaussian random ralk presented as:\n\\[\\begin{pmatrix}\n1 & 0 & \\cdots & 0 \\\\\n-1 & 1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & -1 & 1\n\\end{pmatrix} \\begin{pmatrix} \\tau_1 \\\\ \\tau_2 \\\\ \\vdots \\\\ \\tau_T \\end{pmatrix}  = \\begin{pmatrix} \\tau_1 \\\\ \\tau_2-\\tau_1 \\\\ \\vdots \\\\ \\tau_T - \\tau_{T-1} \\end{pmatrix} = \\eta\\]\nThe parameters to be estimated are: \\(\\mathbf{\\color{purple}{\\tau, \\sigma^2}}\\)\n\nPrior distributions\n\nPrior distribution for \\(\\tau\\)\nRewrite the state equation as \\[\\tau =  H^{-1}\\eta\\] Then the prior distribution of \\(\\tau\\) is formed using \\[\\mathbf{\\color{purple}{\\eta \\sim \\mathcal{N}(0_T, c \\sigma^2 I_T) \\implies H^{-1}\\eta \\sim \\mathcal{N}(0_T, c\\sigma^2(H^T H)^{-1})}}\\] since \\(Var(H^{-1}\\eta) = H^{-1}Var(\\eta)(H^{-1})^T =c\\sigma^2(H^TH)^{-1}\\)\nThen, the prior distribution of \\(\\tau|\\sigma^2\\) is \\[\\tau|\\sigma^2 \\sim \\mathcal{N}(0_T, c\\sigma^2(H^TH)^{-1}) \\\\ \\propto (c\\sigma^2)^{-\\frac{T}{2}}exp\\left(-\\frac{\\tau^TH^T H\\tau}{2c\\sigma^2}\\right)\n\\]\n\n\nPrior assumptions for \\(\\sigma^2\\)\n\\[\\sigma^2 \\sim \\mathcal{IG2}(s_{prior}, \\nu_{prior}) \\propto (\\sigma^2)^\\frac{-\\nu_{prior}+2}{2}exp\\left(-\\frac{s_{prior}}{2\\sigma^2}\\right)\\]\n\n\n\nThe Joint Posterior Distribution\nThe likelihood function is constructed using the the measurement equation: \\[y = \\tau + \\epsilon\\] and the distributional assumption on its error term \\[\\epsilon \\sim \\mathcal{N}(0_T, \\sigma^2I_T)\\] which results in the likelihood function: \\[y|\\tau, \\sigma^2 \\sim \\mathcal{N}(\\tau, \\sigma^2I_T)\\propto (\\sigma^2)^{-\\frac{T}{2}}exp\\left(-\\frac{1}{2\\sigma^2}(y-\\tau)^T(y-\\tau)\\right)\\]\nThe likelihood is combined with the joint prior distribution of \\(\\tau\\) and \\(\\sigma^2\\) to derive the joint posterior distribution of \\(\\tau\\) and \\(\\sigma^2\\). It is derived as follows:\n\\[\np(\\tau, \\sigma^2 |y) = \\frac{p(\\tau, \\sigma^2, y)}{p(y)} \\propto p(\\tau, \\sigma^2, y) = p(y | \\tau, \\sigma^2)p(\\tau, \\sigma^2) =p(y | \\tau, \\sigma^2)p(\\tau|\\sigma^2)p(\\sigma^2)\n\\]\nThis expression is proportional to \\[\\begin{align}\n&\\propto (\\sigma^2)^{-\\frac{T}{2}}exp\\left(-\\frac{(y-\\tau)^T(y-\\tau)}{2\\sigma^2}\\right) \\times\n(\\sigma^2)^{-\\frac{T}{2}}\\exp\\left(-\\frac{\\tau^TH^T H\\tau}{2c\\sigma^2}\\right)\\\\\n&\\qquad\\times(\\sigma^2)^{-\\frac{\\nu_{prior}+2}{2}} \\exp(-\\frac{s_{prior}}{2\\sigma^2})\\\\\n&\\propto \\exp(-\\frac{y^Ty - 2\\tau^T y + \\tau^T\\tau + c^{-1}\\tau^TH^T H\\tau}{2\\sigma^2})\\\\ &\\qquad\\times \\exp(-\\frac{s_{prior}}{2\\sigma^2})\\times(\\sigma^2)^{-\\frac{2T+\\nu_{prior}+2}{2}}\\\\\n&= \\exp(-\\frac{\\tau^T(c^{-1}H^T H + I_T)\\tau - 2\\tau^Ty}{2\\sigma^2})\\\\\n&\\qquad\\times\\exp(-\\frac{y^Ty + s_{prior}}{2\\sigma^2})\\times(\\sigma^2)^{-\\frac{2T+\\nu_{prior}+2}{2}}\n\\end{align}\\]\nLet \\(\\bar{\\Sigma} = (c^{-1}H^T H + I_T)^{-1}\\), then \\[\\begin{align}\np(\\tau, \\sigma^2 |y) &\\propto \\exp(-\\frac{\\tau^T\\bar{\\Sigma}^{-1}\\tau - 2\\tau^Ty + y^T\\bar{\\Sigma}y}{2\\sigma^2})\\exp(-\\frac{y^Ty + s_{prior}-y^T\\bar{\\Sigma}y}{2\\sigma^2})\\\\\n&\\qquad\\times(\\sigma^2)^{-\\frac{2T+\\nu_{prior}+2}{2}}\n\\end{align}\\]\nIn the expression above, we recognise the kernel of the \\(T\\)-variate normal inverted gamma 2 distribution. Therefore, the joint posterior of the parameters of the model is given by: \\[\\begin{align}\n\\tau, \\Sigma |y &\\sim \\mathcal{NIG2}(\\bar{\\tau}, \\bar{\\Sigma}, \\bar{\\nu}, \\bar{s})\\\\[2ex]\n\\bar{\\Sigma} &= (c^{-1}H^T H + I_T)^{-1}\\\\\n\\bar{\\tau} &= \\bar{\\Sigma}y\\\\\n\\bar{\\nu} &= 2T+\\nu_{prior}\\\\\n\\bar{s} &= s_{prior}+y^Ty-y^T\\bar{\\Sigma}y\n\\end{align}\\]\nThe function below implements the sampler from this joint distribution.\n\nUC.local.tau.sigma.Gibbs.sampler    = function(S, starting.values, priors){\n  \n  aux     = starting.values\n  T       = nrow(aux$Y)\n\n  posteriors    = list(\n    tau     = matrix(NA,T,S),\n    sigma   = rep(NA,S)\n  )\n\n  for (s in 1:S){\n    \n    V.tau.bar.inv = priors$c^(-1)*t(aux$H)%*%aux$H + diag(T)\n    V.tau.bar.inv = 0.5*(V.tau.bar.inv + t(V.tau.bar.inv))\n    \n    # Sampling sigma\n    ###########################\n    sigma.s   = as.numeric(priors$sigma.s + t(aux$Y)%*%aux$Y - t(aux$tau)%*%V.tau.bar.inv%*%aux$tau)\n    sigma.nu  = priors$sigma.nu + 2*T\n    sigma.draw= sigma.s/rchisq(1,sigma.nu)\n    aux$sigma     = sigma.draw\n\n    # Sampling tau\n    ###########################\n    b.tau         = aux$Y\n    precision.L   = t(bandchol(V.tau.bar.inv))\n    epsilon       = rnorm(T)\n    b.tau.tmp     = forwardsolve(precision.L, b.tau)\n    tau.draw      = backsolve(t(precision.L), b.tau.tmp + epsilon)\n    aux$tau       = tau.draw\n\n    posteriors$tau[,s]     = aux$tau\n    posteriors$sigma[,s]   = aux$sigma\n  }\n\n  output      = list(\n    posterior = posteriors,\n    last.draw = aux\n  )\n  return(output)\n}"
  },
  {
    "objectID": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "href": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating gamma error term variance prior scale",
    "text": "Estimating gamma error term variance prior scale\nTo estimate the scale of inverted gamma 2 error term variance, we need to firstly put a prior on the prior scale of and present its full conditional posterior distribution.\nThe hirercical prior structure is below:\n\\[\\begin{align*}\n\\sigma^2 \\mid s &\\sim \\text{IG2}(s, \\nu) \\\\\ns &\\sim \\mathcal{G}(s_s, a)\n\\end{align*}\\]\nThe full conditional posterior distribution of the prior scale \\(s\\) is based on the prior \\(p(\\sigma^2 \\mid s)\\): \\[\\begin{align}\np(\\sigma^2 \\mid s) \\propto s^{\\frac{\\nu}{2}}(\\sigma^2)^{-\\frac{\\nu+2}{2}} \\exp \\left( -\\frac{1}{2}\\frac{s}{\\sigma^2} \\right)\n\\end{align}\\] and that for \\(s\\): \\[\\begin{align}\ns \\sim \\mathcal{G}(s_s, a) \\propto s^{a-1} \\exp(-\\frac{s}{s_s})\n\\end{align}\\] which results in the following full conditional posterior distribution: \\[\\begin{align}\np(s \\mid \\mathbf{y}, \\mathbf{\\tau}, \\sigma^2) &= \\mathcal{G}(\\bar{s}_s,\\bar{a})\\\\\n\\bar{s}_s &= \\left(s_s^{-1} + (2\\sigma^2)^{-1}\\right)^{-1}\\\\\n\\bar{a} &= a + \\frac{\\nu}{2}\n\\end{align}\\]\nThe code below implements the sampler from the full conditional posterior for \\(s\\)\n\nsample_s = function(sigma2, s_s, a, nu) {\n  shape_s = a + nu/2\n  scale_s = 1/((1/s_s) + 1/(2*sigma2))\n  s = rgamma(1, shape = shape_s, scale = scale_s)\n  return(s)\n}"
  },
  {
    "objectID": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "href": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating inverted-gamma 2 error term variance prior scale",
    "text": "Estimating inverted-gamma 2 error term variance prior scale\nIn this section, we estimate the error term variance prior scale \\(s\\) that follows a Inverted-gamma 2 distribution with scale \\(\\underline{s}\\) and shape \\(\\underline{\\mu}\\): \\(s\\sim\\mathcal{IG2}(\\underline{s},\\underline{\\mu})\\). The probability density function equal to:\n\\[\np\\left(s\\right) = \\Gamma(\\frac{\\underline{\\mu}}{2})^{-1} (\\frac{\\underline{s}}{2})^{\\frac{\\underline{\\mu}}{2}} s^{-\\frac{\\underline{\\mu}+2}{2}}exp\\{ -\\frac{1}{2}\\frac{\\underline{s}}{s}\\}\n\\]\nAs we assume \\(\\sigma^2|s \\sim \\mathcal{IG2} (s,\\underline{\\nu})\\)\n\\[\np\\left(\\sigma^2|s\\right) = \\Gamma(\\frac{\\underline{\\nu}}{2})^{-1} (\\frac{s}{2})^{\\frac{\\underline{\\nu}}{2}} \\sigma^2{^{-\\frac{\\underline{\\nu}+2}{2}}}exp\\{-\\frac{1}{2}\\frac{s}{\\sigma^2}\\}\n\\]\nIn order to find full conditional posterior of \\(s\\) write out its kernel as:\n\\[\n\\begin{aligned}\np(s|\\tau, \\sigma^2) &\\propto L(\\tau, \\sigma^2|y) \\cdot p(\\epsilon|\\sigma^2) \\cdot p(\\sigma^2|s) \\cdot p(s) \\\\\n&\\propto p(\\sigma^2|s) \\cdot p(s) \\\\\n&\\propto (\\frac{s}{2})^{\\frac{\\underline{\\nu}}{2}} exp\\{ -\\frac{1}{2}\\frac{s}{\\sigma^2}\\} \\cdot s^{-\\frac{\\underline{\\mu}+2}{2}}exp\\{-\\frac{1}{2}\\frac{\\underline{s}}{s}\\} \\\\\n&\\propto (\\frac{s}{2})^{\\frac{\\underline{\\nu}}{2}} \\cdot s^{-\\frac{\\underline{\\mu}+2}{2}} \\cdot exp \\{-\\frac{1}{2}(\\frac{\\underline{s}}{s} + \\frac{s}{\\sigma^2} )\\}  \\\\\n&\\propto (\\frac{1}{2}^{\\frac{\\underline{\\nu}}{2}}) \\cdot s^{\\frac{\\underline{\\nu}}{2}} \\cdot s^{-\\frac{\\underline{\\mu}+2}{2}} \\cdot exp \\{-\\frac{1}{2}(\\frac{\\underline{s}}{s} + \\frac{s}{\\sigma^2} )\\}  \\\\\n&\\propto s^{\\frac{\\underline{\\nu}-\\underline{\\mu}-2}{2}} exp\\{ -\\frac{\\frac{1}{\\sigma^2}s +\\frac{s}{\\underline{s}}}{2}\\}\n\\end{aligned}\n\\]\nfrom which we recognize a Generalized inverse Gaussian distribution: \\(\\mathcal{GIG}(\\overline{p}, \\overline{a}, \\overline{b})\\) with parameters:\n\\[\\begin{align}\n\\overline{p} &= \\frac{\\underline{\\nu} - \\underline{\\mu}}{2}\\\\\n\\overline{a} &= \\frac{1}{\\sigma^2 }\\\\\n\\overline{b} &= \\underline{s }\n\\end{align}\\]\nThe following script illustrates sampling from the full conditional posterior distribution of \\(s\\) and the function s.sampler:\n\ns.sampler = function(sigma2, prior){ \n  # sigma2 - the current draw\n  # prior is a list containing:\n  #   s.prior - a positive scalar\n  #   mu.prior - a scalar\n  #   nu.prior - a scalar\n  \n  a.bar.s      = 1/sigma2\n  b.bar.s      = (prior$nu.prior - prior$mu.prior)/2\n  p.bar.s      = prior$s.prior\n\n  s    = GIGrvg::rgig(n = 1, lambda = p.bar.s, chi = b.bar.s, psi = a.bar.s)\n\n    return(s)\n}"
  },
  {
    "objectID": "index.html#estimating-the-initial-condition-prior-scale",
    "href": "index.html#estimating-the-initial-condition-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating the initial condition prior scale",
    "text": "Estimating the initial condition prior scale\nIn the univariate local-level model, the initial state \\(\\tau_0\\) and the variance \\(s^2\\) are key parameters determining the behavior of the stochastic process. For the initial state \\(\\tau_0\\), a prior is specified as a univariate normal distribution, denoted by\n\\[\\tau_0\\mid s^2 \\sim \\text{N}(\\underline\\tau_0, s^2)\\]\nreflecting our belief about the distribution of \\(\\tau_0\\) before observing the data. While, for the variance \\(s^2\\), which controls the fluctuation of the process, we use inverse gamma 2 distribution, \\[s^2 \\sim \\text{IG2}(\\underline{s}, \\underline{\\nu})\\]\nThe full posterior of \\(s^2\\) is derived below:\n\\[\\begin{align}\np(s^2|y,\\sigma,\\tau,\\tau_0,\\sigma_{\\eta}, \\underline{s}, \\underline{\\nu}) &\\propto\np(y|\\tau,\\sigma)p(\\tau|\\tau_0,\\sigma_{\\eta}^2)p(\\tau_0|s^2)p(s^2) \\\\\n&\\propto p(\\tau_0|s^2)p(s^2|\\underline{s}, \\underline{\\nu}) \\\\\n&= (s^2)^{-\\frac{1}{2}}\\exp\\left\\{-\\frac{1}{2}(\\tau_0-\\underline{\\tau_0})'(s^2)^{-1}(\\tau_0-\\underline{\\tau_0})\\right\\}\\\\\n&\\qquad\\times (s^2)^{-\\frac{\\underline{\\nu}+2}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{\\underline{s}}{s^2}\\right\\} \\\\\n&= (s^2)^{-\\frac{1+\\underline{\\nu}+2}{2}}\n\\exp \\left\\{-\\frac{1}{2}\\frac{(\\tau_0-\\underline{\\tau_0})^2+\\underline{s}}{s^2}\\right\\}\n\\end{align}\\]\nHence, full conditional posterior of \\(s^2\\) in a form of inverse gamma 2 distribution, where\n\\[\\begin{align}\ns^2\\mid \\mathbf{y}, \\mathbf{\\tau}, \\tau_0 &\\sim\\mathcal{IG}2(\\overline{s}, \\overline{\\nu})\\\\\n\\overline{\\nu} &= 1+\\underline{\\nu} \\\\\n\\overline{s} &= (\\tau_0-\\underline{\\tau_0})^2+\\underline{s}\n\\end{align}\\]\nThis R code below implements a sampler from the full-conditional posterior distribution of \\(s^2\\):\n\ns2_sampler &lt;- function(tau_0, tau_0_prior, s_prior, nu_prior) {\n    \n    nu_posterior &lt;- nu_prior + 1\n    s_posterior &lt;- (tau_0 - tau_0_prior)^2 + s_prior\n      \n    s2 &lt;- s_posterior / rchisq(1, nu_posterior)\n    \n    return(s2)\n}"
  },
  {
    "objectID": "index.html#student-t-prior-for-the-trend-component",
    "href": "index.html#student-t-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t prior for the trend component",
    "text": "Student-t prior for the trend component"
  },
  {
    "objectID": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "href": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating Student-t degrees of freedom parameter",
    "text": "Estimating Student-t degrees of freedom parameter\nThe Student-t distribution is commonly used in statistical modeling to handle data with heavier tails than the normal distribution. An essential parameter of the Student-t distribution is the degrees of freedom \\(\\nu\\), which controls the tail heaviness. In this note, we present the Bayesian estimation of the degrees of freedom parameter for a \\(T\\)-variate Student-t distribution represented as an inverted gamma 2 scale mixture of normals:\n\\[\\begin{align}\n\\mathbf{y} \\mid \\boldsymbol\\tau, \\boldsymbol\\lambda, \\sigma^2 &\\sim \\mathcal{N}_T(\\boldsymbol\\tau, \\sigma^2diag(\\boldsymbol\\lambda))\\\\\n\\boldsymbol\\lambda &= (\\lambda_1, \\dots, \\lambda_T)'\\\\\n\\lambda_t \\mid \\nu &\\sim \\mathcal{IG2}(\\nu - 2, \\nu)\n\\end{align}\\]\nTherefore the joint prior for vector \\(\\boldsymbol\\lambda\\) is proportional to: \\[\n\\Gamma\\left(\\frac{\\nu}{2}\\right)^{-T}\\left(\\frac{\\nu-2}{2}\\right)^{\\frac{T\\nu}{2}}\\left(\\prod_{t=1}^T\\lambda_t\\right)^{-\\frac{\\nu+2}{2}}\\exp\\left\\{-\\frac{\\nu-2}{2}\\sum_{t=1^T}\\lambda_t^{-1}\\right\\}\n\\] Assume the following Gamma prior for \\(\\nu\\): \\[\np(\\nu) \\sim \\text{Gamma}(a, b) \\propto \\nu^{a-1}\\exp\\{-b\\nu\\}\n\\]\nTherefore the kernel of the full conditional posterior of \\(\\nu\\) is defined by: \\[\np(\\nu\\mid\\mathbf{y},\\boldsymbol\\tau,\\boldsymbol\\lambda,\\sigma^2, \\nu) \\propto p(,\\boldsymbol\\lambda\\mid\\nu)p(\\nu)\n\\]\nThis expression does not lead to a full conditional posterior distribution of a known typ, so we use the Metropolis-Hastings algorithm to sample from this posterior. We employ a random walk proposal distribution with a truncated normal distribution centred at the current value of \\(\\nu\\) with a fixed standard deviation.\n\nlog_kernel_nu &lt;- function(nu, lambda, a, b) {\n  \n  T = length(lambda)  \n  log_c = -T * lgamma(nu / 2) + (T * nu / 2) * log((nu - 2) / 2) \n  log_k = -((nu + 2) / 2) * sum(log(lambda)) - ((nu - 2) / 2) * sum(1 / lambda)\n  log_nu = (a - 1) * log(nu) - b * nu\n  return(log_c + log_k + log_nu)\n}\n\nsample_nu &lt;- function(nu, lambda, a, b, proposal_sd) {\n  \n  T               = length(lambda)  \n  nu_proposal     = RcppTN::rtn(nu, proposal_sd, 0, Inf)\n  kernel_ratio    = exp(log_kernel_nu(nu_proposal, lambda, a, b) - log_kernel_nu(nu, lambda, a, b))\n  candidate_ratio = RcppTN::dtn(nu_proposal, nu, proposal_sd, 0, Inf) / RcppTN::dtn(nu, nu_proposal, proposal_sd, 0, Inf)\n  accept_ratio    = kernel_ratio * candidate_ratio\n  \n  if (runif(1) &lt; accept_ratio) {\n    nu &lt;- nu_proposal\n  }\n\n  return(nu)\n}"
  },
  {
    "objectID": "index.html#laplace-prior-for-the-trend-component",
    "href": "index.html#laplace-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Laplace prior for the trend component",
    "text": "Laplace prior for the trend component\nTo apply the Laplace prior for our trend component, we have \\(\\lambda_t\\) following an exponential distribution with mean alpha, where \\(\\lambda_t \\sim exp({\\frac{1}{\\alpha}})\\), with the prior of the trend component is \\(\\tau|\\eta,\\lambda \\sim \\mathcal{N}(H^{-1}i\\tau_0, \\sigma_\\eta^2 H^{-1}\\Omega(H^{-1})')\\), then we have the marginal distribution of \\(\\tau\\) is Laplace distribution.\nThe model is where \\(\\Omega=diag(\\lambda_1,\\lambda_2,...,\\lambda_t)\\), each lambda independently drawn from an exponential distribution.\nDue to the change prior distribution of the trend component, now we have different full posterior distributions for parameters and in this note we only focus on the derivation for \\(\\tau\\) and \\(\\lambda_t\\). The change of the prior distribution for \\(\\tau\\) gives the followings:\n\\[\\tau =y - \\epsilon\\]\n\\[\\tau = H^{-1}i\\tau_0 + H^{-1} \\eta \\]\n\\[\\tau|\\eta,\\lambda \\sim \\mathcal{N}(H^{-1}i\\tau_0, \\sigma_\\eta^2 H^{-1}\\Omega(H^{-1})') \\]\n\\[\\lambda_t \\sim exp({\\frac{1}{\\alpha}})\\]\n\nFull-conditional posterior distribution for \\(\\tau|\\tau_0,\\sigma^2,\\Omega\\)\nConditional Likelihood: \\[\\tau =y - \\epsilon\\]\n\\[\\epsilon \\sim \\mathcal{N}(0_T, \\sigma^2I_T)\\]\n\\[L(\\tau|y,\\sigma^2) = exp({-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y} - \\boldsymbol\\tau)'(\\mathbf{y} - \\boldsymbol\\tau)})\\]\nThe prior distribution of \\(\\tau\\)is:\n\\[p(\\tau|\\tau_0,\\sigma_\\eta^2,\\Omega) \\propto exp(-\\frac{1}{2}\\frac{1}{\\sigma_\\eta^2}(\\tau - H^{-1}i\\tau_0)'H'\\Omega^{-1}H(\\tau - H^{-1}i\\tau_0))\\]\nFull-conditional posterior distribution of \\({\\tau}|y,\\sigma_\\eta^2,\\Omega\\):\n\\[\\begin{align}\nP(\\tau|y,\\tau_0,\\sigma^2,\\Omega) &\\propto L(\\tau|y,\\sigma^2)\\times p(\\tau|\\tau_0,\\sigma_\\eta^2,\\Omega)\\\\\n\n&\\propto exp({-\\frac{1}{2}}\\sigma^{-2}\\ \\times (\\tau'\\tau -2y'\\tau+y'y))\\\\\n&\\times exp({-\\frac{1}{2}\\sigma_\\eta^{-2}\\ (\\tau'H'\\Omega^{-1}H\\tau - 2\\tau'H'\\Omega^{-1}H(H^{-1}i\\tau_0) +(H^{-1}i\\tau_0)'H\\Omega^{-1}H(H^{-1}i\\tau_0)}\\\\\n\n&\\propto exp({-\\frac{1}{2}}\\sigma^{-2}\\sigma_\\eta^{-2}[\\tau'(\\sigma_\\eta^2+\\sigma^2H'\\Omega^{-1}H)\\tau-2\\tau'(\\sigma_\\eta^2y+\\sigma^2H'\\Omega^{-1}H(H^{-1}i\\tau_0))])\\\\\n\n&= exp({-\\frac{1}{2}}(\\tau'(\\sigma^{-2}+\\sigma_\\eta^{-2}H'\\Omega^{-1}H\\tau-2\\tau'(\\sigma^{-2}y+\\sigma_\\eta^{-2}H'\\Omega^{-1}H(H^{-1}i\\tau_0))\n\\end{align}\\]\nwhich results in\n\\[\\begin{align}\n\\tau|y,\\tau_0,\\sigma^2,\\Omega&\\sim N(\\bar{V_\\tau},\\bar{\\tau})\\\\[1ex]\n\\bar{V_\\tau} &= (\\sigma^{-2}+\\sigma_\\eta^{-2}H'\\Omega^{-1}H)^{-1}\\\\\n\\bar{\\tau} &=\\bar{V_\\tau} (\\sigma^{-2}y+\\sigma_\\eta^{-2}H'\\Omega^{-1}i\\tau_0)\n\\end{align}\\]\n\n\nFull-conditional posterior distribution for \\(\\lambda_t|y,\\tau,\\sigma_\\eta^2\\)\nConditional Likelihood:\nDefine: \\(H\\tau - i\\tau_o=\\eta\\)\n\\[\\begin{align}\nL(\\lambda_t|y,\\tau_0,\\tau,\\sigma_\\eta^2)&\\propto  (\\prod^{T}_{i = 1} \\lambda_t^{-\\frac{1}{2}}) exp({-\\frac{1}{2}\\frac{1}{\\sigma^2_\\eta}(i\\tau_0-H\\tau)'\\Omega^{-1}(i\\tau_0-H\\tau)})\\\\\n&= (\\prod^{T}_{i = 1} \\lambda_t^{-\\frac{1}{2}}exp({-\\frac{1}{2}\\frac{1}{\\sigma^2_\\eta}\\frac{1}{\\lambda_t}\\eta_t'\\eta_t}))                \n\n\\end{align}\\]\nThe prior distribution of \\(\\lambda_t\\) is:\n\\[\nP(\\lambda_t) ={\\frac{1}{\\alpha}}exp({{-\\frac{1}{\\alpha}}\\lambda_t)})\n\\]\nFull-conditional posterior distribution of \\(\\lambda_t\\)is:\n\\[\\begin{align}\nP(\\lambda_t|y,\\tau,\\sigma_\\eta^2) &\\propto \\lambda_t^{-\\frac{1}{2}+1-1}exp(-\\frac{1}{2}({\\frac{\\eta_t'\\sigma_\\eta^{-2}\\eta_t}{\\lambda_t}+{\\frac{2}{\\alpha}}\\lambda_t)}) \\\\\n\\end{align}\\]\nThe above expression can be rearranged in the form of a Generalized inverse Gaussian distribution kernel as follows:\n\\[\\begin{align}\n\\lambda_t|Y,A,\\Sigma &\\sim GIG(a,b,p) \\\\\n\\\\\na &=\\frac{2}{\\alpha} \\\\\nb &= \\eta_t'\\sigma_\\eta^{-2}\\eta_t \\\\\np &= -\\frac{1}{2}+1\n\\end{align}\\]\n\n\nR Function for Gibbs Sampler\n\nUC.AR.Gibbs.sampler    = function(S, starting.values, priors){\n  aux     = starting.values\n  p       = length(aux$alpha)\n  T       = nrow(aux$Y)\n  i &lt;- matrix(0, T, 1)  \n  i[1, 1] &lt;- 1 \n  posteriors    = list(\n    tau     = matrix(NA,T,S),\n    epsilon = matrix(NA,T,S),\n    tau_0   = matrix(NA,1,S),\n    sigma   = matrix(NA,2,S)\n  )\n  \n  alpha &lt;- 2\n  lambda.0 &lt;- rexp(T, rate = 1/alpha)\n  lambda.priors = list(alpha = 2)\n  lambda.posterior.draws = array(NA,c(T,S+1))\n  \n  for (s in 1:S){\n    \n    if (s == 1) {\n      lambda.s = lambda.0\n    } else {\n      lambda.s    = lambda.posterior.draws[,s]\n    }\n    \n    Omega = (diag(lambda.s))\n    Omega.inv = diag(1/lambda.s)\n    \n    # Sampling tau\n    ###########################\n    V.tau.inv     = (1/aux$sigma[2]) + (1/aux$sigma[1])*t(H) %*% Omega.inv %*% H\n    V.tau.inv     = 0.5*(V.tau.inv + t(V.tau.inv))\n    b.tau         = (1/aux$sigma[2])%*%aux$Y + (1/aux$sigma[1])*crossprod(H, Omega.inv%*%i%*%priors$tau_0)\n    precision.L   = t(bandchol(V.tau.inv))\n    epsilon       = rnorm(T)\n    b.tau.tmp     = forwardsolve(precision.L, b.tau)\n    tau.draw      = backsolve(t(precision.L), b.tau.tmp + epsilon)\n    aux$tau       = tau.draw\n    \n    # Sampling tau_0\n    ###########################\n    tau_0.v.inv    = diag(1/priors$tau_0.v)\n    V.tau_0.bar    = 1/((1/aux$sigma[1])*crossprod(i,Omega.inv%*%i) + tau_0.v.inv)\n    tau_0.bar      = V.tau_0.bar %*% ( (1/aux$sigma[1])%*%t(i)%*%Omega.inv%*%H%*%aux$tau + tau_0.v.inv * priors$tau_0.m )\n    tau_0.draw     = rnorm(1,as.numeric(tau_0.bar), sqrt(V.tau_0.bar))\n    aux$tau_0      = as.vector(tau_0.draw)\n    \n    # Sampling sigma_epsilon\n    ###########################\n    sigma.eta.s   = as.numeric(priors$sigma.s + crossprod((c(aux$tau_0[1],diff(aux$tau_0)) - i%*%aux$tau_0),(priors$H%*%aux$tau - i%*%aux$tau_0)))\n    sigma.eta.nu  = priors$sigma.nu + T\n    sigma.eta.draw= sigma.eta.s/rchisq(1,sigma.eta.nu)\n    sigma.eta.draw.inv=1/(sigma.eta.draw)\n    \n    sigma.e.s     = as.numeric(priors$sigma.s + crossprod(aux$tau-aux$Y))\n    sigma.e.nu    = priors$sigma.nu + T\n    sigma.e.draw  = sigma.e.s/rchisq(1,sigma.e.nu)\n    aux$sigma     = c(sigma.eta.draw,sigma.e.draw)\n\n    # Sampling lambda\n    ###########################\n    u.t = H%*%aux$tau[,,s]-i%*%aux$tau_0[,,s]\n    #    ---- loop lambda posterior ----   #\n    c                      = -1/2 + 1         \n    a                      = 2 / lambda.priors$alpha\n    for (x in 1:T){\n      b                  = t((u.t)[x,])%*%(1/aux$sigma[1][,,s])%*%(u.t)[x,]\n      lambda.posterior.draws[x,s+1] = GIGrvg::rgig(1, lambda = c, chi = b, psi = a)\n    }\n\n    aux$lambda = lambda.posterior.draws\n  \n    \n    posteriors$tau[,s]     = aux$tau\n    posteriors$tau_0[,s]   = aux$tau_0\n    posteriors$lambda[,s]  = aux$lambda\n    posteriors$sigma[,s]   = aux$sigma\n  }\n  \n  output      = list(\n    posterior = posteriors,\n    last.draw = aux\n  )\n  return(output)\n}"
  },
  {
    "objectID": "index.html#estimation-of-autoregressive-parameters-for-cycle-component",
    "href": "index.html#estimation-of-autoregressive-parameters-for-cycle-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimation of autoregressive parameters for cycle component",
    "text": "Estimation of autoregressive parameters for cycle component\nWe consider an extension of the model by a zero-mean autoregressive equation for \\(\\epsilon_{t}\\):\n\\[\\begin{align}\n\\epsilon_{t}=\\alpha_{1}\\epsilon_{t-1}+...+\\alpha_{p}\\epsilon_{t-p}+e_{t}\n\\end{align}\\]\nWhere:\n\\[\\begin{align}\ne_{t}\\sim\\mathcal{N}\\left(0,\\sigma_e^{2}\\right)\n\\end{align}\\]\nDefine the autoregressive parameter vector \\(\\alpha=\\left(\\alpha_{1},...,\\alpha_{p}\\right)'\\). The prior distribution for \\(\\alpha\\) is then:\n\\[\\begin{align}\n\\alpha\\sim\\mathcal{N}_{p}\\left(\\underline{\\alpha},\\sigma_{\\alpha}^{2}I_{p}\\right)\\mathcal{I}\\left(\\alpha\\in A\\right)\\propto\\exp\\left(-\\frac{1}{2}\\frac{1}{\\sigma_{\\alpha}^{2}}\\left(\\alpha-\\underline{\\alpha}\\right)'\\left(\\alpha-\\underline{\\alpha}\\right)\\right)\\mathcal{I}\\left(\\alpha\\in A\\right)\n\\end{align}\\]\nWhere \\(\\alpha\\in A\\) denotes the set of parameters for \\(\\alpha\\) in which stationarity holds:\n\\[\\begin{align}\n\\mathcal{I}\\left(\\alpha\\in A\\right)=\\begin{cases}\n1 & \\text{if }\\alpha\\in A\\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\\] This condition is not implemented in the code though.\nUsing from the simple unobserved component model:\n\\[\\begin{align}\ne=\\epsilon-X_{\\epsilon}\\alpha\n\\end{align}\\] where \\(\\epsilon\\) and \\(X_{\\epsilon}\\) are \\(T\\times 1\\) and \\(T\\times p\\) matrices respectively collecting the terms from the autoregressive equation for all periods.\nThen we have the likelihood: \\[\\begin{align}\nL\\left(\\alpha|\\mathbf{y},\\boldsymbol\\epsilon,\\sigma_{\\alpha}^{2}\\right)\\propto\\exp\\left(-\\frac{1}{2\\sigma_e^{2}}\\left(X_{\\epsilon}\\alpha-\\epsilon\\right)'\\left(X_{\\epsilon}\\alpha-\\epsilon\\right)\\right)\n\\end{align}\\]\nSo the full-conditional posterior distribution for \\(\\alpha\\) is then given by: \\[\\begin{align}\np\\left(\\alpha|y,\\epsilon,\\sigma_{\\alpha}^{2}\\right)&\\propto L\\left(\\alpha|y,\\epsilon,\\sigma_{\\alpha}^{2}\\right)p\\left(\\alpha\\right)\\mathcal{I}\\left(\\alpha\\in A\\right)\\\\\n&=\\exp\\left(-\\frac{1}{2\\sigma_e^{2}}\\left(X_{\\epsilon}\\alpha-\\epsilon\\right)'\\left(X_{\\epsilon}\\alpha-\\epsilon\\right)\\right)\\exp\\left(-\\frac{1}{2}\\frac{1}{\\sigma_{\\alpha}^{2}}\\left(\\alpha-\\underline{\\alpha}\\right)'\\left(\\alpha-\\underline{\\alpha}\\right)\\right)\\mathcal{I}\\left(\\alpha\\in A\\right)\\\\\n&=\\exp\\left(-\\frac{1}{2\\sigma_{\\alpha}^{2}}\\left(X_{\\epsilon}\\alpha-\\epsilon\\right)'\\left(X_{\\epsilon}\\alpha-\\epsilon\\right)+\\frac{1}{\\sigma_{\\alpha}^{2}}\\left(\\alpha-\\underline{\\alpha}\\right)'\\left(\\alpha-\\underline{\\alpha}\\right)\\right)\\mathcal{I}\\left(\\alpha\\in A\\right)\\\\\n&=\\exp\\left(-\\frac{1}{2}\\left(\\alpha'\\overline{V}_{\\alpha}\\alpha-2\\alpha'\\overline{V}_{\\alpha}^{-1}\\overline{\\alpha}+...\\right)\\right)\\mathcal{I}\\left(\\alpha\\in A\\right)\n\\end{align}\\]\nHence we have the full-conditional posterior as:\n\\[\\begin{align}\np\\left(\\alpha|y,\\epsilon,\\sigma_{\\alpha}^{2}\\right)\n&=\\mathcal{N}_{p}\\left(\\overline{\\alpha},\\overline{V}_{\\alpha}\\right)\\mathcal{I}\\left(\\alpha\\in A\\right)\\\\\n\\overline{V}_{\\alpha}&=\\left[\\sigma_{e}^{-2}X_{\\epsilon}'X_{\\epsilon}+\\sigma_{\\alpha}^{-2}\\mathbf{I}_{p}^{-1}\\right]^{-1}\\\\\n\\overline{\\alpha}&=\\overline{V}_{\\alpha}\\left[\\sigma_{e}^{-2}X_{\\epsilon}'\\epsilon+\\sigma_{\\alpha}^{-2}\\underline{\\alpha}\\right]\n\\end{align}\\]\n\nsample_alpha = function(epsilon, X_epsilon, priors, sigma2_e){\n  \n  p = ncol(X_epsilon) \n  \n  # full conditional posterior distribution alpha\n  V.alpha.bar = solve((1/priors[2]) * diag(p) + (1/sigma2_e)* t(X_epsilon)%*%X_epsilon)\n  alpha.bar        = V.alpha.bar*( priors[1]/priors[2] + (1/sigma2_e)*as.numeric(t(X_epsilon)%*%epsilon) )\n  draw         = mvtnorm::rmvnorm(1, alpha.bar, V.alpha.bar)\n  \n  return(draw)\n}"
  },
  {
    "objectID": "index.html#autoregressive-cycle-component",
    "href": "index.html#autoregressive-cycle-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Autoregressive cycle component",
    "text": "Autoregressive cycle component\nIn this section, we will modify the error component \\(\\epsilon_{t}\\) from the simple local-level model to take the form of an autoregressive (AR) expression as follows \\[\\begin{align}\n\\epsilon_{t} &= \\alpha_{1}\\epsilon_{t-1} + \\alpha_{2}\\epsilon_{t-2} + \\cdots  + \\alpha_{p}\\epsilon_{t-p} + e_{t},\\\\\ne_{t}&\\sim \\mathcal{N}(0,\\sigma_{e}^{2}).\n\\end{align}\\]\nCollect vector of autoregressive parametersin a \\(p\\)-vector \\(\\boldsymbol\\alpha = (\\alpha_{1},\\alpha_{2}, \\cdots, \\alpha_{p})'\\), where \\(\\boldsymbol\\alpha \\in A\\), with the stationarity restriction being required to hold.\nBy modelling the error component in an autoregressive form, we enable our model to capture the cyclical component, capturing its negative autocorrelations, and allowing the series to deviate from the long-term stochastic trend (\\(\\tau_{t}\\)) in the short run.\nThe matrix notation can be represented as:\n\\[\\begin{align}\n\\mathbf{H}_{\\alpha} \\boldsymbol\\epsilon &= \\mathbf{e} \\\\\n\\boldsymbol\\epsilon &= H_{\\alpha}^{-1}\\mathbf{e} \\\\\n\\mathbf{e} &\\sim \\mathcal{N}_{T}(\\mathbf{0}_{T},\\sigma_{e}^{2} \\mathbf{I}_{T})\n\\end{align}\\]\nWhere\n\\[\n\\mathbf{H}_{\\alpha} =\n\\begin{bmatrix}\n1  &0  &0  &0  &0  &\\cdots  &0  &0  &0  &0 \\\\\n-\\alpha_{1}  &1  &0  &0  &0  &\\cdots  &0  &0  &0  &0 \\\\\n-\\alpha_{2}  &-\\alpha_{1}  &1  &0  &0  &\\cdots  &\\vdots  &\\vdots  &\\vdots  &\\vdots \\\\\n-\\alpha_{3}  &-\\alpha_{2}  &-\\alpha_{1}  &1  &0  &\\ddots  &\\vdots  &\\vdots  &\\vdots  &\\vdots \\\\\n\\vdots   &\\vdots  &\\ddots  &\\ddots  & \\ddots &\\ddots  &\\vdots  &\\vdots  &\\vdots &\\vdots \\\\\n-\\alpha_{p}  &-\\alpha_{p-1}  &-\\alpha_{p-2} &\\ddots  &\\ddots  &\\ddots  &0  &0  &0  &0 \\\\\n\\vdots  &\\vdots  &\\vdots  &\\vdots  &\\ddots  &\\ddots  &\\ddots  &\\vdots  &\\vdots &\\vdots  \\\\\n\\vdots  &\\vdots  &\\vdots  &\\vdots  &\\cdots  &\\ddots  &-\\alpha_{1}  &1  &0  &0 \\\\\n0  &0  &0  &0  &\\cdots  &\\cdots   &-\\alpha_{2}   &-\\alpha_{1} &1 &0 \\\\\n0  &0  & 0 &0  &\\cdots  &\\cdots  &-\\alpha_{3}  &-\\alpha_{2}  &-\\alpha_{1} &1\n\\end{bmatrix}_{T\\times T}\n\\]\n\\[\\begin{align}\n\\boldsymbol\\epsilon =\n\\begin{bmatrix}\n\\epsilon_{1} \\\\\n\\epsilon_{2}\\\\\n\\vdots\\\\\n\\epsilon_{T}\\\\\n\\end{bmatrix}_{T\\times 1}\n\\qquad and \\qquad\n\\mathbf{e} =\n\\begin{bmatrix}\ne_{1} \\\\\ne_{2}\\\\\n\\vdots\\\\\ne_{T}\\\\\n\\end{bmatrix}_{T\\times 1}\n\n\\end{align}\\]\n\nTypically, in most cases, we set the initial value (\\(\\boldsymbol\\epsilon_{0}\\)) to zero since it aligns with the well-defined stationarity assumption. However, in this note, these values are estimated \\(\\boldsymbol\\epsilon_{0}\\) from an autoregressive model, which is facilitated by assuming a normal prior distribution: \\(\\boldsymbol\\epsilon_{0} \\sim \\mathcal{N}_{p}(\\mathbf{0}_{p},\\sigma_{0}^{2}\\mathbf{I}_{p})\\) and that \\(\\mathbf{e}_{0} \\sim \\mathcal{N}_{p}(\\mathbf{0}_{p},\\sigma_{e}^{2}\\mathbf{I}_{p})\\). Define:\n\\[\\begin{align}\n\\boldsymbol\\epsilon_{0} =\n\\begin{bmatrix}\n\\epsilon_{-p+1}   \\\\\n\\epsilon_{-p+2}   \\\\\n\\vdots\\\\\n\\epsilon_{0}\n\\end{bmatrix}_{p\\times 1},\n\\qquad and \\qquad\n\\mathbf{e}_{0} =\n\\begin{bmatrix}\ne_{-p+1}   \\\\\ne_{-p+2}   \\\\\n\\vdots\\\\\ne_{0}\n\\end{bmatrix}_{p\\times 1}\n\n\\end{align}\\]\nThen, the author will combine \\(\\boldsymbol\\epsilon\\) and \\(\\boldsymbol\\epsilon_{0}\\) together. This can be represented using the notations \\(\\mathbf{H}_{\\alpha}^{*}\\), \\(\\boldsymbol\\epsilon^{*}\\), and \\(\\mathbf{e}^{*}\\).\n\\(\\mathbf{H}_{\\alpha}^{*}\\) is constructed in the same way as \\(\\mathbf{H}_{\\alpha}\\), but the dimension increases to \\((T+p)\\times (T+p)\\).\n\\[\\begin{align}\n\\boldsymbol\\epsilon^{*} =\n\\begin{bmatrix}\n\\epsilon_{0}   \\\\\n\\epsilon\n\\end{bmatrix}_{(T+p)\\times 1}  =\n\\begin{bmatrix}\n\\epsilon_{-p+1}   \\\\\n\\epsilon_{-p+2}   \\\\\n\\vdots\\\\\n\\epsilon_{1} \\\\\n\\epsilon_{2} \\\\\n\\vdots\\\\\n\\epsilon_{T}\n\\end{bmatrix}_{(T+p)\\times 1}\n\\qquad and \\qquad\n\\mathbf{e}^{*} =\n\\begin{bmatrix}\ne_{0}   \\\\\ne\n\\end{bmatrix}_{(T+p)\\times 1}\n=\n\\begin{bmatrix}\ne_{-p+1}   \\\\\ne_{-p+2}   \\\\\n\\vdots\\\\\ne_{1} \\\\\ne_{2} \\\\\n\\vdots\\\\\ne_{T}\n\\end{bmatrix}_{(T+p)\\times 1}\n\\end{align}\\]\nThe matrix notation can be represented as:\n\\[\\begin{align}\n\\mathbf{H}_{\\alpha}^{*} \\boldsymbol\\epsilon^{*} &=\\mathbf{e}^{*} \\\\\n\\boldsymbol\\epsilon^{*} &= \\mathbf{H}^{*-1}_{\\alpha}\\mathbf{e}^{*} \\\\\n\n\\mathbf{e}^{*} &\\sim \\mathcal{N}_{T+p}(\\mathbf{0}_{T+p},\\sigma_{e*}^{2}\\mathbf{I}_{T+p})\n\\end{align}\\]\nThen\n\\[\\begin{align}\n\n\\boldsymbol\\epsilon^{*}|\\boldsymbol\\alpha,\\sigma_{e}^2 &\\sim \\mathcal{N}_{T+p}(\\mathbf{0}_{T+p},\\underbrace{\\sigma_{e*}^2(\\mathbf{H}_{\\alpha}^{*'}\\mathbf{H}_{\\alpha}^{*})^{-1}}_{{\\boldsymbol\\Omega}})\n\\end{align}\\]\n\\[\\begin{align}\n\\boldsymbol\\Omega = \\begin{bmatrix}\n\\boldsymbol\\Omega_{11}& \\boldsymbol\\Omega_{12}\\\\\n\\boldsymbol\\Omega_{21}& \\boldsymbol\\Omega_{21}\n\\end{bmatrix} \\\\\n\\end{align}\n\n\\text{ with dimensions}\n\n\\begin{bmatrix}\npXP&pXT\\\\\nTXp&TXT\n\\end{bmatrix}_{(T+p)X(T+p)} \\\\\n\\]\n\nThe prior distribution\nUsing knowledge from the conditional distributions of multivariate normal distribution, we can find the prior distribution of \\(\\boldsymbol\\epsilon \\text{ and } \\boldsymbol\\epsilon_{0}\\) as follows.\n\nThe prior distribution of \\(\\boldsymbol\\epsilon\\)\n\n\\[\\begin{align}\n\n\\boldsymbol\\epsilon|\\boldsymbol\\epsilon_{0},\\boldsymbol\\alpha,\\sigma_{e*}^2 &\\sim \\mathcal{N}_{T}(\\underbrace{\\boldsymbol\\Omega_{21}\\boldsymbol\\Omega_{11}^{-1}\\boldsymbol\\epsilon_{0}}_{\\underline{\\boldsymbol\\epsilon}},\\underbrace{\\boldsymbol\\Omega_{22}-\\boldsymbol\\Omega_{21}\\boldsymbol\\Omega_{11}^{-1}\\boldsymbol\\Omega_{12}}_{\\underline{\\mathbf{V}}_{\\epsilon}}) \\\\\n&\\propto exp\\left\\{-\\frac{1}{2} (\\boldsymbol\\epsilon-\\underline{\\boldsymbol\\epsilon})' \\underline{\\mathbf{V}}_{\\epsilon}^{-1} (\\boldsymbol\\epsilon-\\underline{\\boldsymbol\\epsilon})\\right\\}\n\n\n\\end{align}\\]\n\nThe prior distribution of \\(\\boldsymbol\\epsilon_{0}\\)\n\n\\[\\begin{align}\n\n\\boldsymbol\\epsilon_{0}|\\boldsymbol\\epsilon,\\boldsymbol\\alpha,\\sigma_{e*}^2 &\\sim \\mathcal{N}_{P}(\\underbrace{\\boldsymbol\\Omega_{12}\\boldsymbol\\Omega_{22}^{-1}\\boldsymbol\\epsilon}_{\\underline{\\boldsymbol\\epsilon}_{0}},\\underbrace{\\boldsymbol\\Omega_{11}-\\boldsymbol\\Omega_{12}\\boldsymbol\\Omega_{22}^{-1}\\boldsymbol\\Omega_{21}}_{\\underline{\\mathbf{V}}_{\\epsilon_{0}}}) \\\\\n\n&\\propto exp\\left\\{-\\frac{1}{2} (\\boldsymbol\\epsilon_{0}-\\underline{\\boldsymbol\\epsilon}_{0})' \\underline{\\mathbf{V}}_{\\epsilon_{0}}^{-1}  (\\boldsymbol\\epsilon_{0}-\\underline{\\boldsymbol\\epsilon}_{0})\\right\\}\n\n\n\\end{align}\\]\n\n\nLikelihood\nFrom the simple local-level model we can write\n\\[\\begin{align}\n\\mathbf{y} &= \\boldsymbol\\tau + \\boldsymbol\\epsilon,\\\\\n\\mathbf{H}\\boldsymbol\\tau &= \\mathbf{i} \\tau_0 + \\boldsymbol\\eta,\\\\\n\\boldsymbol\\tau &= \\mathbf{H}^{-1} \\mathbf{i} \\tau_0 + \\mathbf{H}^{-1} \\boldsymbol\\eta\n\\end{align}\\]\nThen,\n\\[\\begin{align}\n\\boldsymbol\\epsilon &= \\mathbf{y} - \\boldsymbol\\tau \\\\\n\\boldsymbol\\epsilon &\\sim N(\\mathbf{y}-\\mathbf{H}^{-1} \\mathbf{i} \\tau_0, \\sigma_{\\eta}^2 \\mathbf{I}_{T} (\\mathbf{H}'\\mathbf{H})^{-1})\\\\\nL(\\boldsymbol\\epsilon|\\mathbf{y},\\boldsymbol\\tau,\\tau_0, \\sigma_{\\eta}^2)  &\\propto exp\\{-\\frac{1}{2}\\frac{1}{\\sigma_{\\eta}^2}(\\boldsymbol\\epsilon-(\\mathbf{y}-\\mathbf{H}^{-1} \\mathbf{i} \\tau_0))' \\mathbf{H}'\\mathbf{H} (\\boldsymbol\\epsilon-(\\mathbf{y}-\\mathbf{H}^{-1} \\mathbf{i} \\tau_0))\n\\end{align}\\]\n\n\nThe full-conditional posterior distribution\n\nThe full-conditional posterior distribution of \\(\\boldsymbol\\epsilon\\)\n\n\\[\\begin{align}\np(\\boldsymbol\\epsilon|\\boldsymbol\\epsilon_{0},\\mathbf{y},\\boldsymbol\\tau,\\tau_0, \\sigma_{\\eta}^2, \\underline{\\mathbf{V}}_{\\epsilon}) &\\propto L(\\boldsymbol\\epsilon|\\mathbf{y},\\boldsymbol\\tau,\\tau_0,\\sigma_{\\eta}^2) \\; p(\\boldsymbol\\epsilon|\\boldsymbol\\epsilon_{0},\\boldsymbol\\alpha,\\sigma_{e*}^2) \\\\\n&= \\mathcal{N}_{T}(\\bar{\\boldsymbol\\epsilon},\\bar{\\mathbf{V}}_{\\epsilon}) \\\\\n\n\\\\\n\n\\bar{\\mathbf{V}}_{\\epsilon} &= \\left[\\underline{\\mathbf{V}}^{-1}_{\\epsilon} + \\sigma_{\\eta}^{-2}\\mathbf{H}'\\mathbf{H}\\right]^{-1} \\\\\n\\bar{\\boldsymbol\\epsilon} &= \\bar{\\mathbf{V}}_{\\epsilon} \\left[\\sigma_{\\eta}^{-2} \\mathbf{H}'\\mathbf{H} (\\mathbf{y}-\\mathbf{H}^{-1} \\mathbf{i} \\tau_0) + \\underline{\\mathbf{V}}^{-1}_{\\epsilon}\\; \\underline{\\boldsymbol\\epsilon}\\right]\n\n\\end{align}\\]\n\nThe full-conditional posterior distribution of \\(\\boldsymbol\\epsilon_{0}\\)\n\n\\[\\begin{align}\n\np(\\boldsymbol\\epsilon_{0}|\\epsilon,\\mathbf{y},\\boldsymbol\\tau,\\tau_0, \\sigma_{\\eta}^2,\\underline{\\mathbf{V}}_{\\epsilon_{0}}) &\\propto L(\\boldsymbol\\epsilon|\\mathbf{y},\\boldsymbol\\tau,\\tau_0,\\sigma_{\\eta}^2) \\; p(\\boldsymbol\\epsilon_{0}) \\; p(\\boldsymbol\\epsilon_{0}|\\boldsymbol\\epsilon,\\boldsymbol\\alpha,\\sigma_{e*}^2) \\\\\n\n\n&\\propto p(\\boldsymbol\\epsilon_{0}) \\; p(\\boldsymbol\\epsilon_{0}|\\boldsymbol\\epsilon,\\boldsymbol\\alpha,\\sigma_{e*}^2) \\\\\n\n&= \\mathcal{N}_{p}(\\bar{\\boldsymbol\\epsilon_{0}},\\bar{\\mathbf{V}}_{\\epsilon_{0}}) \\\\\n\n\\\\\n\n\\bar{\\mathbf{V}}_{\\epsilon_{0}} &= \\left[\\underline{\\mathbf{V}}^{-1}_{\\epsilon_{0}} + \\sigma_{0}^{-2} \\mathbf{I}_{p}\\right]^{-1} \\\\\n\\bar{\\boldsymbol\\epsilon_{0}} &= \\bar{\\mathbf{V}}_{\\epsilon_{0}} \\; \\underline{\\boldsymbol\\epsilon_{0}}\n\n\\end{align}\\]\n\n\nThe Sampler function\nGiven that we have priors and other sampling parameters from another function, and also some fixed matrices such as \\(\\mathbf{H}'\\mathbf{H}\\) and \\(\\mathbf{H^{*}}_{\\alpha}'\\mathbf{H^{*}}_{\\alpha}\\).\nWe can sampling each of \\(\\epsilon\\) and \\(\\epsilon_{0}\\) from the following function\n\nSampling.epsilon.epsilonzero    = function(starting.values, priors){\n\n  \n  aux            = starting.values\n  p              = length(aux$alpha)\n  T              = nrow(aux$Y)\n  Ha             = diag(T + p)\n  for (i in 1:p) {\n    mgcv::sdiag(Ha, -i) &lt;- - aux$alpha[i]\n  }\n  HaHa           = crossprod(Ha)\n    \n    # Start calculating (and updating parameters)\n    ###########################\n\n    epsilon.star        = rbind(aux$epsilon.zero, aux$epsilon)\n    omega               = sigma.e.star%*%solve(HaHa)\n    omega_11            = omega[1:p, 1:p]\n    omega_12            = omega[1:p, (p+1):(p+T)]\n    omega_21            = omega[(p+1):(p+T), 1:p]\n    omega_22            = omega[(p+1):(p+T), (p+1):(p+T)]\n    epsilon             = omega_21%*%solve(omega_11)%*%aux$epsilon.zero\n    epsilon.zero        = omega_12%*%solve(omega_22)%*%aux$epsilon\n    V.epsilon           = omega_22 - omega_21%*%solve(omega_11)%*%omega_12\n    V.epsilon.zero      = omega_11 - omega_12%*%solve(omega_22)%*%omega_21\n    \n    # Sampling epsilon\n    ###########################\n    V.epsilon.post.inv     = solve(V.epsilon) + (1/aux$sigma.eta)*HH\n    b.epsilon              = (1/aux$sigma.eta)*HH%*%(aux$Y- cumsum(i%*%priors$tau_0)) + \n                             solve(V.epsilon)%*%aux$epsilon\n    precision.L            = t(chol(V.epsilon.post.inv))\n    epsilon.n              = rnorm(T)\n    b.epsilon.tmp          = solve(precision.L, b.epsilon)\n    epsilon.draw           = solve(t(precision.L), b.epsilon.tmp + epsilon.n)\n    aux$epsilon            = epsilon.draw\n    \n    # Sampling epsilon.zero\n    ###########################\n    V.epsilon.zero.post.inv     = solve(V.epsilon.zero) + (1/sigma.zero)*diag(p)\n    b.epsilon.zero              = epsilon.zero\n    precision.L                 = t(chol(V.epsilon.zero.post.inv))\n    epsilon.n                   = rnorm(p)\n    b.epsilon.zero.tmp          = solve(precision.L, b.epsilon.zero)\n    epsilon.zero.draw           = solve(t(precision.L), b.epsilon.zero.tmp + epsilon.n)\n    aux$epsilon.zero            = epsilon.zero.draw\n    \n    # The draw\n  output      = list(\n    epsilon_draw = epsilon.draw, # Drawn epsilon values\n    epsilon_zero_draw = epsilon.zero.draw # Drawn epsilon.zero values\n  )\n  return(output)\n}"
  },
  {
    "objectID": "index.html#random-walk-with-time-varying-drift-parameter",
    "href": "index.html#random-walk-with-time-varying-drift-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Random walk with time-varying drift parameter",
    "text": "Random walk with time-varying drift parameter\nConsider a simple Unobserved Component model with time-varying drift parameter \\[\\begin{align}\n\\tau_{t} &= \\mu_{s_t} + \\tau_{t-1} + \\eta_t,\\\\\n\\eta_t &\\sim \\mathcal{N}(0, \\sigma_{\\eta}^2),\n\\end{align}\\] where \\(\\mu_{s_t}\\) is a regime-dependent mean term, and \\(s_t\\) is a regime state variable. Assume there are two regimes, for example, before and after the exchange rate crisis in 1985. Define:\n\\[s_t = \\left\\{\\begin{array}{ll}1 &\\text{ for }t &lt; Tb \\\\  2 &\\text{ for }t \\geq Tb\\end{array}\\right.\\]\nTo write a model in a matrix notation, define vector \\(\\boldsymbol\\beta = \\begin{bmatrix}\\tau_0 &\\mu_1 &\\mu_2\\end{bmatrix}'\\) and matrix: \\[\\begin{align}\n\\mathbf{X}_{\\tau} = \\begin{bmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\end{align}\\] Then the model is written as \\[\n\\mathbf{H}\\boldsymbol\\tau = \\mathbf{X}_{\\tau}\\boldsymbol\\beta+\\boldsymbol\\eta\\\\\n\\] Prior distribution \\[p(\\boldsymbol\\beta) = exp\\left(-\\frac{1}{2}(\\boldsymbol\\beta - \\underline{\\boldsymbol\\beta})'\\mathbf{\\underline{V}}_\\beta^{-1}(\\boldsymbol\\beta - \\underline{\\boldsymbol\\beta})\\right)\\]\nFull conditional posterior distribution \\[\\begin{align}\np(\\boldsymbol\\beta|y,\\boldsymbol\\tau,\\sigma_\\eta^2) &= \\mathcal{N}_3(\\overline{\\boldsymbol\\beta},\\mathbf{\\overline{V}}_\\beta)\\\\[1ex]\n\\mathbf{\\overline{V}}_{\\beta} &= [\\sigma_{\\eta}^{-2}\\mathbf{X}_\\tau'\\mathbf{X}_\\tau+\\mathbf{\\underline{V}}_\\beta^{-1}]^{-1}\\\\\n\\bar{\\boldsymbol\\beta} &= \\mathbf{\\overline{V}}_{\\beta}[\\sigma_{\\eta}^{-2}\\mathbf{X}_\\tau'\\mathbf{H}\\boldsymbol\\tau + \\mathbf{\\underline{V}}_\\beta^{-1}\\underline{\\boldsymbol\\beta}]\n\\end{align}\\]\n\n# function to draw beta\ndraw_beta &lt;- function(Tb, tau, sigma_eta_sq, beta_prior, V_prior) {\n  \n  T = length(tau)\n  \n  X_tau &lt;- matrix(0, nrow = T, ncol = 3)\n  X_tau[1, 1] &lt;- 1  # Column for tau_0\n  X_tau[1:(Tb-1), 2] &lt;- 1  # Column for mu1 for t &lt; Tb\n  X_tau[Tb:T, 3] &lt;- 1  # Column for mu2 for t &gt;= Tb\n  \n  # Define H\n  H &lt;- diag(T)\n  for (i in 2:T) {\n    H[i, i-1] &lt;- -1\n  }\n  \n  V_prior_inv &lt;- solve(V_prior)\n  \n  V_bar_inv &lt;- (1/sigma_eta_sq) * t(X_tau) %*% X_tau + V_prior_inv\n  V_bar &lt;- solve(V_bar_inv)\n  beta_bar &lt;- V_bar %*% ((1/sigma_eta_sq) * t(X_tau) %*% (H %*% tau) \n                         + V_prior_inv %*% beta_prior)\n  \n  # Draw a sample from the multivariate normal distribution\n  beta_posterior &lt;- mvrnorm(1, beta_bar, V_bar)\n  \n  return(beta_posterior)\n}"
  },
  {
    "objectID": "index.html#student-t-error-terms",
    "href": "index.html#student-t-error-terms",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t error terms",
    "text": "Student-t error terms\nT-distributed error terms enhance robustness against outliers in data. This section outlines a method to incorporate t-distributed error terms into unobserved component models where the error term structure is as follows:\n\\[\n\\boldsymbol\\epsilon\\sim N_{T}(0_{T},{\\sigma^{2}}diag({\\boldsymbol\\lambda)})\n\\]\nwhere vector \\(\\boldsymbol\\lambda = (\\lambda_1, \\dots, \\lambda_T)'\\) collects th auxiliary latent variables. Each \\(\\lambda_{t}\\) follows independently the following prior: \\[\\lambda_t\\sim\\mathcal{IG}2(\\nu,\\nu)\\] which makes \\(\\epsilon_{t}\\) marginally Student-t-distributed.\nThe likelihood is given by:\n\\[\nL(\\mathbf{y}|\\boldsymbol\\tau,\\sigma^{2},\\boldsymbol\\lambda) = (\\sigma^2)^{-\\frac{T}{2}}\\left(\\prod_{t=1}^{T}(\\lambda_{t})^{-\\frac{1}{2}}\\right)\n\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^{2}}\\sum_{t=1}^{T}\\frac{1}{\\lambda_{t}}(y_t-\\tau_t)^2\\right\\}.\n\\]\nThe prior density of is proportional to\n\\[\n\\lambda_{t}^{-\\frac{\\nu+2}{2}} \\exp\\left\\{-\\frac{1}{2}\\frac{\\nu}{\\lambda_{t}}\\right\\}.\n\\]\nCombining the likelihood and the prior\n\\[\\begin{align}\np(\\lambda_{t}|y,\\tau,\\sigma^{2})&\\propto L(\\lambda_{t},\\tau,\\sigma^{2}|y)p(\\lambda_{t}|\\nu_{\\lambda})\\\\\n&= (\\lambda_{t})^{-\\frac{{1}}{2}}\\exp\\{-\\frac{1}{2}\\frac{1}{\\sigma^{2}}\\frac{1}{\\lambda_{t}}(y-\\tau)'(y-(\\tau)\\} \\lambda_{t}^{-\\frac{\\nu+2}{2}} \\exp\\left\\{-\\frac{1}{2}\\frac{\\nu}{\\lambda_{t}}\\right\\}\n\\end{align}\\]\nYields the posterior kernel\n\\[\n(\\lambda_{t})^{-\\frac{_{1+\\nu+2}}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\lambda_{t}}\\left[ \\nu + \\frac{\\epsilon_t^2}{\\sigma^{2}}\\right]\\right\\}\n\\]\nwhere \\(\\epsilon_{t}=y_{t}-\\tau_{t}\\). Therefore, the full conditional posterior for each \\(\\lambda_t\\) is:\n\\[IG2\\sim(1+\\nu,\\nu+\\frac{\\epsilon_{t}^2}{\\sigma^{2}})\\]\n\nlambda_t &lt;- function(y_t,tau_t,sigma2, nu){\n  \n  e_t     &lt;- y_t - tau_t\n  lambda  &lt;- MCMCpack::rinvgamma(1, (1 + nu)/2, (nu + (e_t^2) / sigma2)/2)\n    \n  return(lambda)\n}"
  },
  {
    "objectID": "index.html#conditional-heteroskedasticity",
    "href": "index.html#conditional-heteroskedasticity",
    "title": "Bayesian Unobserved Component Models",
    "section": "Conditional heteroskedasticity",
    "text": "Conditional heteroskedasticity\nModelling Conditional heteroskedasticity includes the use of Stochastic Volatility models, which provides more flexibility in forecasting, enhancing its performance, improves in-sample fit of model and estimation precision.\n\nBasic model\nGiven that the shocks \\(\\epsilon_t\\) is heteroskedastic with conditional variances \\(\\sigma_t^2\\) the distribution of the shock becomes\n\\[\n\\boldsymbol \\epsilon \\mid \\tau \\sim \\mathcal{N}_T \\left( 0_T, \\operatorname{diag}(\\boldsymbol\\sigma^2) \\right)\n\\] where \\(\\boldsymbol\\sigma^2 = \\left(\\sigma^2_1 , \\ldots, \\sigma^2_T \\right)\\) which follows a Stochastic Volatility process.\nThe likelihood function is derived as:\n\\[\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y}) =\n(2\\pi)^{-\\frac{T}{2}}\\left(\\operatorname{diag}(\\boldsymbol\\sigma^2)\\right)^{-\\frac{1}{2}}\\exp\\left\\{-\\frac{1}{2}(\\mathbf{y} - \\boldsymbol\\tau)'\\operatorname{diag}(\\boldsymbol\\sigma^2)^{-1}(\\mathbf{y} - \\boldsymbol\\tau)\\right\\}.\n\\]\nThe prior distribution for \\(\\boldsymbol\\tau\\) is :\n\\[\\begin{align}\n\\boldsymbol\\tau \\mid \\tau_{0}, \\sigma_\\eta^2 &\\sim \\mathcal{N}_T \\left( \\mathbf{H}^{-1} \\mathbf{i}\\tau_{0}, \\sigma_\\eta^2 (\\mathbf{H'H})^{-1} \\right)\\\\\n&\\propto \\exp \\left\\{ -\\frac{1}{2} \\frac{1}{\\sigma_\\eta^2} \\left( \\tau - H^{-1} \\mathbf{i}\\tau_{0} \\right)' H' H \\left( \\tau - H^{-1} \\mathbf{i}\\tau_{0}\\right) \\right\\}\n\\end{align}\\]\nThen derive the Full Conditional Posterior for \\(\\boldsymbol\\tau\\) is :\n\\[\\begin{align}\np(\\boldsymbol\\tau \\mid \\mathbf{y},  \\tau_{0}, \\sigma_\\eta^2, \\sigma^2) &\\propto L(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y})p(\\tau \\mid \\tau_{0}, \\sigma_\\eta^2)\\\\\n&\\propto exp\\left\\{-\\frac{1}{2}(\\mathbf{y} - \\boldsymbol\\tau)'\\operatorname{diag}(\\boldsymbol\\sigma^2)^{-1}(\\mathbf{y} - \\boldsymbol\\tau)\\right\\} \\\\\n&\\qquad\\exp \\left\\{ -\\frac{1}{2} \\frac{1}{\\sigma_\\eta^2} \\left( \\boldsymbol\\tau - \\mathbf{H}^{-1} \\mathbf{i}\\tau_{0} \\right)' \\mathbf{H' H} \\left( \\boldsymbol\\tau - \\mathbf{H}^{-1} \\mathbf{i}\\tau_{0}\\right) \\right\\}\n\\end{align}\\]\nThe posterior parameters for \\(\\tau\\) follows a normal distribution\n\\[\\begin{align}\n\\boldsymbol\\tau \\mid \\mathbf{y},  \\tau_{0}, \\sigma_\\eta^2, \\sigma^2&\\mathcal{N}_T \\left( \\bar{\\boldsymbol\\tau}, \\bar{\\mathbf{V}}_\\tau \\right)\\\\\n\\bar{\\mathbf{V}}_\\tau &= \\left[ \\operatorname{diag}(\\boldsymbol\\sigma^2)^{-1} + \\sigma_\\eta^{-2} \\mathbf{H' H} \\right]^{-1}\\\\\n\\bar{\\boldsymbol\\tau} &= \\bar{\\mathbf{V}}_\\tau \\left[ \\operatorname{diag}(\\boldsymbol\\sigma^2)^{-1} \\mathbf{y} + \\sigma_\\eta^{-2} \\mathbf{H}'\\mathbf{i}\\tau_{0} \\right]\n\\end{align}\\]\n\n\nAlgorithm\nThe sampler for posterior parameters for \\(\\boldsymbol\\tau\\) is:\n\ntau_hetero.sampler = function(y, aux){\n  #aux includes\n  # sigma2 = a diagonal matrix of sigma2 from Stochastic Volatility models\n  # tau0 = a scalar of initial value of tau\n  # sigma_eta2 = a scalar sigma_eta^2\n  \n  #y = data\n  T1 = nrow(y)\n  \n  # i_m = Tx1 matrix with 1 in the first row and 0 in all other rows\n  i_m = matrix(0, nrow = T1, ncol = 1)\n  i_m[1,] = 1\n  \n  H                    &lt;-  diag(T1)\n  mgcv::sdiag(H,-1)    &lt;-  -1\n # H = TxT matrix with 1 in main diagonal and -1 under the diagonal elements\n  \n  sigma2 = aux$sigma2 \n  sigma2.inv = diag(1/diag(sigma2)) #get inverse \n  tau0 = aux$tau0\n  sigma_eta2 = aux$sigma_eta2\n  sigma_eta2.inv = 1/sigma_eta2 #get inverse\n  \n  #compute the posterior parameters\n  V_post = solve(sigma2.inv + sigma_eta2.inv*t(H)%*%H)\n  tau_post = V_post%*%(sigma2.inv%*%y + sigma_eta2.inv * t(H)%*%(i_m*tau0))\n  \n  #sample tau from multivariate normal and return the sample tau\n  tau_draw &lt;- mvtnorm::rmvnorm(T1, tau_post, V_post)\n\n  return (tau_draw)\n}"
  },
  {
    "objectID": "index.html#predictive-density",
    "href": "index.html#predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Predictive density",
    "text": "Predictive density\nThe predictive density is derived by writing down the joint distribution of future data and parameters condition on observed data, then integrating out the parameters. By integrating out the posterior distribution of parameters, the predictive density accurately accounts for estimation uncertainty.\nThe h-period ahead predictive density is given by:\n\\[\n\\begin{aligned}\np(y_{T+h}, \\dots, y_{T+1} | \\mathbf y)\n= \\int\\int\n&p(y_{T+h} | \\tau_{T+h}, \\sigma^{2})\np(\\tau_{T+h} | \\tau_{T+h-1}, \\sigma^{2}_\\eta)\\\\\n&\\dots\\\\\n&\\times p(y_{T+1} | \\tau_{T+1}, \\sigma^{2})\np(\\tau_{T+1} | \\tau_{T}, \\sigma^{2}_\\eta)\\\\\n& \\times p(\\tau_{T}, \\sigma^{2}, \\sigma^{2}_\\eta | \\mathbf y)\n\\ d\\left(\\tau_{T+h},\\dots,\\tau_{T+1}\\right) d\\left(\\tau_{T}, \\sigma^2,\\sigma^2_\\eta\\right) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "index.html#sampling-from-the-predictive-density",
    "href": "index.html#sampling-from-the-predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Sampling from the predictive density",
    "text": "Sampling from the predictive density\nThe form of the predictive density above suggests the following general algorithm for a h-periods ahead forecast.\nFor each posterior draw \\(\\left\\{ \\tau_T^{(s)}, \\sigma^{2(s)}, \\sigma^{2(s)}_\\eta \\right\\}\\):\nâ€ƒ For \\(i=1,\\cdots,h:\\)\nâ€ƒâ€ƒ Sample \\(\\tau_{T+i}^{(s)} \\sim \\mathcal N(\\tau_{T+i-1}^{(s)}, \\sigma^{2(s)}_\\eta)\\)\nâ€ƒâ€ƒ Sample \\(y_{T+i}^{(s)} \\sim \\mathcal N(\\tau_{T+i}^{(s)}, \\sigma^{2(s)})\\)\nReturn \\(\\left\\{ y_{T+1}^{(s)},\\cdots,y_{T+h}^{(s)} \\right\\}_{s=1}^S\\)\nHere is an implementation in R:\n\n#' @title h-step ahead Bayesian forecasting of the local-level model\n#' \n#' @param h                   Number of periods ahead\n#' @param posterior.tauT      Sx1 vector of trend component at time T\n#' @param posterior.sigma2    Sx1 vector of observation equation error variance\n#' @param posterior.sigma2eta Sx1 vector of state equation error variance\n#'\n#' @return hxS matrix of forecast values\nforecast = function(h, \n                    posterior.tauT, \n                    posterior.sigma2, \n                    posterior.sigma2eta) {\n  \n  S = length(posterior.tauT)\n  y = matrix(NA, nrow = h, ncol = S)\n  \n  for (s in 1:S) {\n    tauT      = posterior.tauT[s]\n    sigma2    = posterior.sigma2[s]\n    sigma2eta = posterior.sigma2eta[s]\n    \n    for (t in 1:h) {\n      tauT1   = rnorm(1, tauT,  sqrt(sigma2eta))\n      y[t, s] = rnorm(1, tauT1, sqrt(sigma2))\n    }\n  }\n  \n  return(y)\n}"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Bayesian Unobserved Component Models",
    "section": "References",
    "text": "References"
  }
]