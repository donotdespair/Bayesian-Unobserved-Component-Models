[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Unobserved Component Models",
    "section": "",
    "text": "Abstract. We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. We scrutinise Bayesian forecasting and sampling from the predictive density.\nKeywords. Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling"
  },
  {
    "objectID": "index.html#matrix-notation-for-the-model",
    "href": "index.html#matrix-notation-for-the-model",
    "title": "Bayesian Unobserved Component Models",
    "section": "Matrix notation for the model",
    "text": "Matrix notation for the model\nTo simplify the notation and the derivations introduce matrix notation for the model. Let \\(T\\) be the available sample size for the variable \\(y\\). Define a \\(T\\)-vector of zeros, \\(\\mathbf{0}_T\\), and of ones, \\(\\boldsymbol\\imath_T\\), the identity matrix of order \\(T\\), \\(\\mathbf{I}_T\\), as well as \\(T\\times1\\) vectors: \\[\\begin{align}\n\\mathbf{y} = \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_T \\end{bmatrix},\\quad\n\\boldsymbol\\tau = \\begin{bmatrix} \\tau_1\\\\ \\vdots\\\\ \\tau_T \\end{bmatrix},\\quad\n\\boldsymbol\\epsilon = \\begin{bmatrix} \\epsilon_1\\\\ \\vdots\\\\ \\epsilon_T \\end{bmatrix},\\quad\n\\boldsymbol\\eta = \\begin{bmatrix} \\eta_1\\\\ \\vdots\\\\ \\eta_T \\end{bmatrix},\\qquad\n\\mathbf{i} = \\begin{bmatrix} 1\\\\0\\\\ \\vdots\\\\ 0 \\end{bmatrix},\n\\end{align}\\] and a \\(T\\times T\\) matrix \\(\\mathbf{H}\\) with the elements: \\[\\begin{align}\n\\mathbf{H} = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 & 0\\\\\n-1 & 1 & \\cdots & 0 & 0\\\\\n0 & -1 & \\cdots & 0 & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n0 & 0 & \\cdots & 1 & 0\\\\\n0 & 0 & \\cdots & -1 & 1\n\\end{bmatrix}.\n\\end{align}\\]\nThen the model can be written in a concise notation as: \\[\\begin{align}\n\\mathbf{y} &= \\mathbf{\\tau} + \\boldsymbol\\epsilon,\\\\\n\\mathbf{H}\\boldsymbol\\tau &= \\mathbf{i} \\tau_0 + \\boldsymbol\\eta,\\\\\n\\boldsymbol\\epsilon &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma^2\\mathbf{I}_T\\right),\\\\\n\\boldsymbol\\eta &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma_\\eta^2\\mathbf{I}_T\\right).\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#likelihood-function",
    "href": "index.html#likelihood-function",
    "title": "Bayesian Unobserved Component Models",
    "section": "Likelihood function",
    "text": "Likelihood function\nThe model equations imply the predictive density of the data vector \\(\\mathbf{y}\\). To see this, consider the model equation as a linear transformation of a normal vector \\(\\boldsymbol\\epsilon\\). Therefore, the data vector follows a multivariate normal distribution given by: \\[\\begin{align}\n\\mathbf{y}\\mid \\boldsymbol\\tau, \\sigma^2 &\\sim\\mathcal{N}_T\\left(\\boldsymbol\\tau, \\sigma^2\\mathbf{I}_T\\right).\n\\end{align}\\]\nThis distribution determines the shape of the likelihood function that is defined as the sampling data density: \\[\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y})\\equiv p\\left(\\mathbf{y}\\mid\\boldsymbol\\tau, \\sigma^2 \\right).\n\\end{align}\\]\nThe likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of \\(\\mathbf{y}\\), is considered a function of parameters \\(\\boldsymbol\\tau\\) and \\(\\sigma^2\\) is given by: \\[\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y}) =\n(2\\pi)^{-\\frac{T}{2}}\\left(\\sigma^2\\right)^{-\\frac{T}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y} - \\boldsymbol\\tau)'(\\mathbf{y} - \\boldsymbol\\tau)\\right\\}.\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#prior-distributions",
    "href": "index.html#prior-distributions",
    "title": "Bayesian Unobserved Component Models",
    "section": "Prior distributions",
    "text": "Prior distributions"
  },
  {
    "objectID": "index.html#gibbs-sampler",
    "href": "index.html#gibbs-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler"
  },
  {
    "objectID": "index.html#simulation-smoother-and-precision-sampler",
    "href": "index.html#simulation-smoother-and-precision-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Simulation smoother and precision sampler",
    "text": "Simulation smoother and precision sampler"
  },
  {
    "objectID": "index.html#analytical-solution-for-a-joint-posterior",
    "href": "index.html#analytical-solution-for-a-joint-posterior",
    "title": "Bayesian Unobserved Component Models",
    "section": "Analytical solution for a joint posterior",
    "text": "Analytical solution for a joint posterior"
  },
  {
    "objectID": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "href": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating gamma error term variance prior scale",
    "text": "Estimating gamma error term variance prior scale\nTo estimate the gamma error term variance, we need to firstly put a prior on the prior scale of and present its full conditional posterior distribution.\nThe prior structure is below:\n\\[\\begin{align*}\n\\sigma^2 \\mid s &\\sim \\text{IG2}(s, \\nu) \\\\\ns &\\sim \\mathcal{G}(s, a)\n\\end{align*}\\]\nThen we use Gibbs sampling to draw samples from the full conditional distributions.\nThe code define the prior parameters for \\(s\\) and \\(\\sigma^2\\) as ‘s_prior’, ‘nu_prior’, ‘a_prior’ and ‘b_prior’. Then we initialize samples for \\(s\\) and \\(\\sigma^2\\). In each iteration, first update \\(\\sigma^2\\) then update \\(s\\). Also, we run 1000 iterations of sampling and plot the posterior distributions.\n\nlibrary(MCMCpack)\n\n# Define prior parameters\ns_prior <- 2\nnu_prior <- 2\na_prior <- 2\nb_prior <- 1  \n\n# Define the function for the conditional posterior distribution\nposterior_sampler <- function(y, n_iter = 1000, s_prior, nu_prior, a_prior, b_prior) {\n  # Initialize and store samples\n  s <- rgamma(1, shape = a_prior, rate = b_prior)\n  sigma2 <- rinvgamma(1, shape = s_prior, scale = nu_prior)  \n\n  s_samples <- numeric(n_iter)\n  sigma2_samples <- numeric(n_iter)\n\n  for (i in 1:n_iter) {\n    # Update sigma2\n    shape_sigma2 <- (length(y) / 2) + s\n    rate_sigma2 <- (sum((y - mean(y))^2) / 2) + nu_prior\n    sigma2 <- rinvgamma(1, shape = shape_sigma2, scale = rate_sigma2)  \n\n    # Update s\n    shape_s <- a_prior + 1\n    rate_s <- b_prior + sigma2\n    s <- rgamma(1, shape = shape_s, rate = rate_s)\n\n    # Store the samples\n    sigma2_samples[i] <- sigma2\n    s_samples[i] <- s\n  }\n\n  return(list(sigma2_samples = sigma2_samples, s_samples = s_samples))\n}\n\nset.seed(123)\ny <- rnorm(100, mean = 0, sd = 1)\n\n# Run the sampler\nposterior_samples <- posterior_sampler(y, n_iter = 1000, s_prior, nu_prior, a_prior, b_prior)\n\n# Plot posterior distributions\npar(mfrow = c(2, 1))\nhist(posterior_samples$sigma2_samples, main = expression(paste(\"Posterior of \", sigma^2)), xlab = expression(sigma^2))\nhist(posterior_samples$s_samples, main = \"Posterior of s\", xlab = \"s\")\n\n\n\n\nThe results show that the posterior distribution of \\(\\sigma^2\\) lies between 0.6 and 1.2, indicating a high probability density within this interval.From the peak we can see the \\(\\sigma^2\\) are concentrated around 0.8.\nThe posterior distribution of \\(s\\) lies between 0 and 4, indicating a high probability density within this interval.And the histogram shows that the values of \\(s\\) are concentrated between 1 and 2.\nFor the full conditional posterior distribution of \\(\\sigma^2\\), we can drive it based on the prior information.\nThe observed data is normal distribution:\n\\[\\begin{align}\n\\mathbf{y} \\mid \\boldsymbol{\\tau}, \\sigma^2 \\sim \\mathcal{N}_T(\\boldsymbol{\\tau}, \\sigma^2 I_T)\n\\end{align}\\]\nThen combine the observed data and \\(s\\), we can get the likelihood function:\n\\[\\begin{align}\np(\\mathbf{y} \\mid \\boldsymbol{\\tau}, \\sigma^2) = (2\\pi\\sigma^2)^{-T/2} \\exp \\left( -\\frac{1}{2\\sigma^2} (\\mathbf{y} - \\boldsymbol{\\tau})' (\\mathbf{y} - \\boldsymbol{\\tau}) \\right)\n\\end{align}\\]\nThe prior distribution is:\n\\[\\begin{align}\np(\\sigma^2 \\mid s) \\propto (\\sigma^2)^{-s-1} \\exp \\left( -\\frac{\\nu}{\\sigma^2} \\right)\n\\end{align}\\]\nCombine the likelihood function and prior distribution, we can know the posterior distribution:\n\\[\\begin{align}\np(\\sigma^2 \\mid \\mathbf{y}, \\boldsymbol{\\tau}, s) \\propto (\\sigma^2)^{-s-1-T/2} \\exp \\left( -\\frac{(\\mathbf{y} - \\boldsymbol{\\tau})' (\\mathbf{y} - \\boldsymbol{\\tau})/2 + \\nu}{\\sigma^2} \\right)\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "href": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating inverted-gamma 2 error term variance prior scale",
    "text": "Estimating inverted-gamma 2 error term variance prior scale"
  },
  {
    "objectID": "index.html#estimating-the-initial-condition-prior-scale",
    "href": "index.html#estimating-the-initial-condition-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating the initial condition prior scale",
    "text": "Estimating the initial condition prior scale"
  },
  {
    "objectID": "index.html#student-t-prior-for-the-trend-component",
    "href": "index.html#student-t-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t prior for the trend component",
    "text": "Student-t prior for the trend component"
  },
  {
    "objectID": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "href": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating Student-t degrees of freedom parameter",
    "text": "Estimating Student-t degrees of freedom parameter"
  },
  {
    "objectID": "index.html#laplace-prior-for-the-trend-component",
    "href": "index.html#laplace-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Laplace prior for the trend component",
    "text": "Laplace prior for the trend component"
  },
  {
    "objectID": "index.html#estimation-of-autoregressive-parameters-for-cycle-component",
    "href": "index.html#estimation-of-autoregressive-parameters-for-cycle-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimation of autoregressive parameters for cycle component",
    "text": "Estimation of autoregressive parameters for cycle component"
  },
  {
    "objectID": "index.html#autoregressive-cycle-component",
    "href": "index.html#autoregressive-cycle-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Autoregressive cycle component",
    "text": "Autoregressive cycle component"
  },
  {
    "objectID": "index.html#random-walk-with-time-varying-drift-parameter",
    "href": "index.html#random-walk-with-time-varying-drift-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Random walk with time-varying drift parameter",
    "text": "Random walk with time-varying drift parameter"
  },
  {
    "objectID": "index.html#student-t-error-terms",
    "href": "index.html#student-t-error-terms",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t error terms",
    "text": "Student-t error terms"
  },
  {
    "objectID": "index.html#conditional-heteroskedasticity",
    "href": "index.html#conditional-heteroskedasticity",
    "title": "Bayesian Unobserved Component Models",
    "section": "Conditional heteroskedasticity",
    "text": "Conditional heteroskedasticity"
  },
  {
    "objectID": "index.html#predictive-density",
    "href": "index.html#predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Predictive density",
    "text": "Predictive density"
  },
  {
    "objectID": "index.html#sampling-from-the-predictive-density",
    "href": "index.html#sampling-from-the-predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Sampling from the predictive density",
    "text": "Sampling from the predictive density"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Bayesian Unobserved Component Models",
    "section": "References",
    "text": "References"
  }
]