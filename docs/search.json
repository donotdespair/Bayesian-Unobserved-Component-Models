[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Unobserved Component Models",
    "section": "",
    "text": "Abstract. We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. We scrutinise Bayesian forecasting and sampling from the predictive density.\nKeywords. Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling"
  },
  {
    "objectID": "index.html#matrix-notation-for-the-model",
    "href": "index.html#matrix-notation-for-the-model",
    "title": "Bayesian Unobserved Component Models",
    "section": "Matrix notation for the model",
    "text": "Matrix notation for the model\nTo simplify the notation and the derivations introduce matrix notation for the model. Let \\(T\\) be the available sample size for the variable \\(y\\). Define a \\(T\\)-vector of zeros, \\(\\mathbf{0}_T\\), and of ones, \\(\\boldsymbol\\imath_T\\), the identity matrix of order \\(T\\), \\(\\mathbf{I}_T\\), as well as \\(T\\times1\\) vectors: \\[\\begin{align}\n\\mathbf{y} = \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_T \\end{bmatrix},\\quad\n\\boldsymbol\\tau = \\begin{bmatrix} \\tau_1\\\\ \\vdots\\\\ \\tau_T \\end{bmatrix},\\quad\n\\boldsymbol\\epsilon = \\begin{bmatrix} \\epsilon_1\\\\ \\vdots\\\\ \\epsilon_T \\end{bmatrix},\\quad\n\\boldsymbol\\eta = \\begin{bmatrix} \\eta_1\\\\ \\vdots\\\\ \\eta_T \\end{bmatrix},\\qquad\n\\mathbf{i} = \\begin{bmatrix} 1\\\\0\\\\ \\vdots\\\\ 0 \\end{bmatrix},\n\\end{align}\\] and a \\(T\\times T\\) matrix \\(\\mathbf{H}\\) with the elements: \\[\\begin{align}\n\\mathbf{H} = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 & 0\\\\\n-1 & 1 & \\cdots & 0 & 0\\\\\n0 & -1 & \\cdots & 0 & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n0 & 0 & \\cdots & 1 & 0\\\\\n0 & 0 & \\cdots & -1 & 1\n\\end{bmatrix}.\n\\end{align}\\]\nThen the model can be written in a concise notation as: \\[\\begin{align}\n\\mathbf{y} &= \\mathbf{\\tau} + \\boldsymbol\\epsilon,\\\\\n\\mathbf{H}\\boldsymbol\\tau &= \\mathbf{i} \\tau_0 + \\boldsymbol\\eta,\\\\\n\\boldsymbol\\epsilon &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma^2\\mathbf{I}_T\\right),\\\\\n\\boldsymbol\\eta &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma_\\eta^2\\mathbf{I}_T\\right).\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#likelihood-function",
    "href": "index.html#likelihood-function",
    "title": "Bayesian Unobserved Component Models",
    "section": "Likelihood function",
    "text": "Likelihood function\nThe model equations imply the predictive density of the data vector \\(\\mathbf{y}\\). To see this, consider the model equation as a linear transformation of a normal vector \\(\\boldsymbol\\epsilon\\). Therefore, the data vector follows a multivariate normal distribution given by: \\[\\begin{align}\n\\mathbf{y}\\mid \\boldsymbol\\tau, \\sigma^2 &\\sim\\mathcal{N}_T\\left(\\boldsymbol\\tau, \\sigma^2\\mathbf{I}_T\\right).\n\\end{align}\\]\nThis distribution determines the shape of the likelihood function that is defined as the sampling data density: \\[\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y})\\equiv p\\left(\\mathbf{y}\\mid\\boldsymbol\\tau, \\sigma^2 \\right).\n\\end{align}\\]\nThe likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of \\(\\mathbf{y}\\), is considered a function of parameters \\(\\boldsymbol\\tau\\) and \\(\\sigma^2\\) is given by: \\[\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y}) =\n(2\\pi)^{-\\frac{T}{2}}\\left(\\sigma^2\\right)^{-\\frac{T}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y} - \\boldsymbol\\tau)'(\\mathbf{y} - \\boldsymbol\\tau)\\right\\}.\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#prior-distributions",
    "href": "index.html#prior-distributions",
    "title": "Bayesian Unobserved Component Models",
    "section": "Prior distributions",
    "text": "Prior distributions"
  },
  {
    "objectID": "index.html#gibbs-sampler",
    "href": "index.html#gibbs-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler"
  },
  {
    "objectID": "index.html#simulation-smoother-and-precision-sampler",
    "href": "index.html#simulation-smoother-and-precision-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Simulation smoother and precision sampler",
    "text": "Simulation smoother and precision sampler"
  },
  {
    "objectID": "index.html#analytical-solution-for-a-joint-posterior",
    "href": "index.html#analytical-solution-for-a-joint-posterior",
    "title": "Bayesian Unobserved Component Models",
    "section": "Analytical solution for a joint posterior",
    "text": "Analytical solution for a joint posterior"
  },
  {
    "objectID": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "href": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating gamma error term variance prior scale",
    "text": "Estimating gamma error term variance prior scale"
  },
  {
    "objectID": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "href": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating inverted-gamma 2 error term variance prior scale",
    "text": "Estimating inverted-gamma 2 error term variance prior scale"
  },
  {
    "objectID": "index.html#estimating-the-initial-condition-prior-scale",
    "href": "index.html#estimating-the-initial-condition-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating the initial condition prior scale",
    "text": "Estimating the initial condition prior scale"
  },
  {
    "objectID": "index.html#student-t-prior-for-the-trend-component",
    "href": "index.html#student-t-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t prior for the trend component",
    "text": "Student-t prior for the trend component"
  },
  {
    "objectID": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "href": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating Student-t degrees of freedom parameter",
    "text": "Estimating Student-t degrees of freedom parameter\nThe Student-t distribution is commonly used in statistical modeling to handle data with heavier tails than the normal distribution. An essential parameter of the Student-t distribution is the degrees of freedom (), which controls the tail heaviness. In this note, we present the Bayesian estimation of the degrees of freedom parameter for an \\(N\\)-variate Student-t distribution using the Inverted-Gamma 2 (IG2) scale mixture of normals.\nThe \\(N\\)-variate Student-t distribution can be represented as a scale mixture of normals:\n\\[\n\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda \\sim \\mathcal{N}(\\mathbf{\\mu}, \\lambda \\mathbf{I}_N)\n\\]\n\\[\n\\lambda \\mid s, \\nu \\sim \\mathcal{IG2}(s, \\nu)\n\\]\nwhere:\n\n\\(\\mathbf{y}\\) is the \\(N\\)-dimensional observation vector.\n\\(\\mathbf{\\mu}\\) is the mean vector.\n\\(\\lambda\\) is the latent scale variable.\n\\(\\nu\\) is the degrees of freedom parameter.\n\\(s = \\nu - 2\\)\n\n\nDerivation of Full Conditional Posteriors\n\nFull Conditional Posterior of \\(\\lambda\\)\nGiven the prior distribution:\n\\[\n\\lambda \\mid s, \\nu \\sim \\mathcal{IG2}(s, \\nu)\n\\]\nand \\(s = \\nu - 2\\), the likelihood of the data given \\(\\lambda\\) is:\n\\[\n\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda \\sim \\mathcal{N}(\\mathbf{\\mu}, \\lambda \\mathbf{I}_T)\n\\]\nThe full conditional posterior of \\(\\lambda\\) can be derived as follows:\n\nLikelihood of \\(\\mathbf{y}\\) given \\(\\mathbf{\\mu}\\) and \\(\\lambda\\):\n\\[\np(\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda) \\propto \\lambda^{-\\frac{T}{2}} \\exp\\left(-\\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{\\lambda}\\right)\n\\]\nPrior for \\(\\lambda\\) given \\(s\\) and \\(\\nu\\):\n\n\\[\n   p(\\lambda \\mid s, \\nu) \\propto \\lambda^{-s} \\exp\\left(-\\frac{\\nu}{\\lambda}\\right)\n   \\]\n\nJoint distribution:\n\\[\np(\\mathbf{y}, \\lambda \\mid \\mathbf{\\mu}, s, \\nu) = p(\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda) p(\\lambda \\mid s, \\nu)\n\\]\nFull conditional posterior:\n\\[\np(\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, s, \\nu) \\propto \\lambda^{-\\frac{T}{2}} \\exp\\left(-\\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{\\lambda}\\right) \\lambda^{-s} \\exp\\left(-\\frac{\\nu}{\\lambda}\\right)\n\\]\nCombining terms:\n\\[\np(\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, s, \\nu) \\propto \\lambda^{-\\left(s + \\frac{T}{2}\\right)} \\exp\\left(-\\frac{\\nu + (\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{\\lambda}\\right)\n\\]\nThis is recognized as the kernel of an Inverted-Gamma 2 distribution:\n\\[\n\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, s, \\nu \\sim \\mathcal{IG2}\\left(s + (\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu}), \\nu + T\\right)\n\\]\n\n\n\nFull Conditional Posterior of \\(\\nu\\)\nTo estimate \\(\\nu\\), we use the Metropolis-Hastings algorithm due to its non-standard form. The steps for deriving the full conditional posterior of \\(\\nu\\) are as follows:\n\nEstimate the Prior for \\(\\nu\\):\nAssuming a Gamma prior for \\(\\nu\\), we estimate its parameters from the data:\n\\[\n\\nu \\sim \\text{Gamma}(\\alpha_{\\nu}, \\beta_{\\nu})\n\\]\nWe can use the method of moments or maximum likelihood to estimate \\(\\alpha_{\\nu}\\) and \\(\\beta_{\\nu}\\) from the data \\(y\\).\nLikelihood of \\(\\lambda\\) given \\(\\nu\\):\n\\[\np(\\lambda \\mid s, \\nu) = \\frac{\\nu^s}{\\Gamma(s)} \\lambda^{-\\left(s + 1\\right)} \\exp\\left(-\\frac{\\nu}{\\lambda}\\right)\n\\]\nLog-likelihood for\\(\\nu\\) given \\(\\lambda\\):\n\\[\n\\log p(\\lambda \\mid s, \\nu) = s \\log(\\nu) - \\log\\Gamma(s) - \\left(s + 1\\right) \\log \\lambda - \\frac{\\nu}{\\lambda}\n\\]\nLog-prior for \\(\\nu\\) (estimated from data):\n\\[\n\\log p(\\nu) = (\\alpha_{\\nu} - 1) \\log(\\nu) - \\beta_{\\nu} \\nu - \\log\\Gamma(\\alpha_{\\nu}) + \\alpha_{\\nu} \\log(\\beta_{\\nu})\n\\]\nFull conditional posterior:\nThe full conditional posterior for \\(\\nu\\) is proportional to the product of the likelihood and the prior:\n\\[\np(\\nu \\mid \\lambda) \\propto p(\\lambda \\mid s, \\nu) p(\\nu)\n\\]\nCombining terms:\n\\[\n\\log p(\\nu \\mid \\lambda) = s \\log(\\nu) - \\log\\Gamma(s) - \\left(s + 1\\right) \\log \\lambda - \\frac{\\nu}{\\lambda} + (\\alpha_{\\nu} - 1) \\log(\\nu) - \\beta_{\\nu} \\nu - \\log\\Gamma(\\alpha_{\\nu}) + \\alpha_{\\nu} \\log(\\beta_{\\nu})\n\\]\nThis expression does not have a closed form, so we use the Metropolis-Hastings algorithm to sample from this posterior.\n\n\n\n\nR Function for Gibbs Sampler\nBelow is the R function implementing the Gibbs sampler for estimating \\(\\nu\\) using the IG2-scale mixture of normals representation.\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.118   6.519   7.252   7.694   8.789  11.099 \n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.24991  0.01076  0.08409  0.08034  0.15527  0.38873 \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.8344  1.1177  1.2089  1.2185  1.3046  1.7318 \n\n\n\n\nConclusion\nThis note provided a comprehensive step-by-step algebraic derivation and a sampler for estimating the degrees of freedom parameter \\(\\nu\\) for an \\(N\\)-variate Student-t distribution using an IG2-scale mixture of normals approach within a Bayesian framework. By using the Metropolis-Hastings algorithm and estimating the prior for \\(\\nu\\) from the data, we avoid the need to assume a fixed prior distribution, allowing for more flexible modeling of heavy-tailed data, which is often encountered in practice.\n\n\nReferences\n\nGeweke, J. (1993). Bayesian treatment of the independent Student-t linear model. Journal of Applied Econometrics, 8(S1), S19-S40.\nChib, S., & Greenberg, E. (1995). Understanding the Metropolis-Hastings Algorithm. The American Statistician, 49(4), 327-335."
  },
  {
    "objectID": "index.html#laplace-prior-for-the-trend-component",
    "href": "index.html#laplace-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Laplace prior for the trend component",
    "text": "Laplace prior for the trend component"
  },
  {
    "objectID": "index.html#autoregressive-cycle-component",
    "href": "index.html#autoregressive-cycle-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Autoregressive cycle component",
    "text": "Autoregressive cycle component"
  },
  {
    "objectID": "index.html#random-walk-with-time-varying-drift-parameter",
    "href": "index.html#random-walk-with-time-varying-drift-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Random walk with time-varying drift parameter",
    "text": "Random walk with time-varying drift parameter"
  },
  {
    "objectID": "index.html#student-t-error-terms",
    "href": "index.html#student-t-error-terms",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t error terms",
    "text": "Student-t error terms"
  },
  {
    "objectID": "index.html#conditional-heteroskedasticity",
    "href": "index.html#conditional-heteroskedasticity",
    "title": "Bayesian Unobserved Component Models",
    "section": "Conditional heteroskedasticity",
    "text": "Conditional heteroskedasticity"
  },
  {
    "objectID": "index.html#predictive-density",
    "href": "index.html#predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Predictive density",
    "text": "Predictive density"
  },
  {
    "objectID": "index.html#sampling-from-the-predictive-density",
    "href": "index.html#sampling-from-the-predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Sampling from the predictive density",
    "text": "Sampling from the predictive density"
  },
  {
    "objectID": "index.html#missing-observations",
    "href": "index.html#missing-observations",
    "title": "Bayesian Unobserved Component Models",
    "section": "Missing observations",
    "text": "Missing observations"
  },
  {
    "objectID": "index.html#references-1",
    "href": "index.html#references-1",
    "title": "Bayesian Unobserved Component Models",
    "section": "References",
    "text": "References\n```"
  }
]