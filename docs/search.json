[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Unobserved Component Models",
    "section": "",
    "text": "Abstract. We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. We scrutinise Bayesian forecasting and sampling from the predictive density.\nKeywords. Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling"
  },
  {
    "objectID": "index.html#matrix-notation-for-the-model",
    "href": "index.html#matrix-notation-for-the-model",
    "title": "Bayesian Unobserved Component Models",
    "section": "Matrix notation for the model",
    "text": "Matrix notation for the model\nTo simplify the notation and the derivations introduce matrix notation for the model. Let \\(T\\) be the available sample size for the variable \\(y\\). Define a \\(T\\)-vector of zeros, \\(\\mathbf{0}_T\\), and of ones, \\(\\boldsymbol\\imath_T\\), the identity matrix of order \\(T\\), \\(\\mathbf{I}_T\\), as well as \\(T\\times1\\) vectors: \\[\\begin{align}\n\\mathbf{y} = \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_T \\end{bmatrix},\\quad\n\\boldsymbol\\tau = \\begin{bmatrix} \\tau_1\\\\ \\vdots\\\\ \\tau_T \\end{bmatrix},\\quad\n\\boldsymbol\\epsilon = \\begin{bmatrix} \\epsilon_1\\\\ \\vdots\\\\ \\epsilon_T \\end{bmatrix},\\quad\n\\boldsymbol\\eta = \\begin{bmatrix} \\eta_1\\\\ \\vdots\\\\ \\eta_T \\end{bmatrix},\\qquad\n\\mathbf{i} = \\begin{bmatrix} 1\\\\0\\\\ \\vdots\\\\ 0 \\end{bmatrix},\n\\end{align}\\] and a \\(T\\times T\\) matrix \\(\\mathbf{H}\\) with the elements: \\[\\begin{align}\n\\mathbf{H} = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 & 0\\\\\n-1 & 1 & \\cdots & 0 & 0\\\\\n0 & -1 & \\cdots & 0 & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n0 & 0 & \\cdots & 1 & 0\\\\\n0 & 0 & \\cdots & -1 & 1\n\\end{bmatrix}.\n\\end{align}\\]\nThen the model can be written in a concise notation as: \\[\\begin{align}\n\\mathbf{y} &= \\mathbf{\\tau} + \\boldsymbol\\epsilon,\\\\\n\\mathbf{H}\\boldsymbol\\tau &= \\mathbf{i} \\tau_0 + \\boldsymbol\\eta,\\\\\n\\boldsymbol\\epsilon &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma^2\\mathbf{I}_T\\right),\\\\\n\\boldsymbol\\eta &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma_\\eta^2\\mathbf{I}_T\\right).\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#likelihood-function",
    "href": "index.html#likelihood-function",
    "title": "Bayesian Unobserved Component Models",
    "section": "Likelihood function",
    "text": "Likelihood function\nThe model equations imply the predictive density of the data vector \\(\\mathbf{y}\\). To see this, consider the model equation as a linear transformation of a normal vector \\(\\boldsymbol\\epsilon\\). Therefore, the data vector follows a multivariate normal distribution given by: \\[\\begin{align}\n\\mathbf{y}\\mid \\boldsymbol\\tau, \\sigma^2 &\\sim\\mathcal{N}_T\\left(\\boldsymbol\\tau, \\sigma^2\\mathbf{I}_T\\right).\n\\end{align}\\]\nThis distribution determines the shape of the likelihood function that is defined as the sampling data density: \\[\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y})\\equiv p\\left(\\mathbf{y}\\mid\\boldsymbol\\tau, \\sigma^2 \\right).\n\\end{align}\\]\nThe likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of \\(\\mathbf{y}\\), is considered a function of parameters \\(\\boldsymbol\\tau\\) and \\(\\sigma^2\\) is given by: \\[\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y}) =\n(2\\pi)^{-\\frac{T}{2}}\\left(\\sigma^2\\right)^{-\\frac{T}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y} - \\boldsymbol\\tau)'(\\mathbf{y} - \\boldsymbol\\tau)\\right\\}.\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#prior-distributions",
    "href": "index.html#prior-distributions",
    "title": "Bayesian Unobserved Component Models",
    "section": "Prior distributions",
    "text": "Prior distributions"
  },
  {
    "objectID": "index.html#gibbs-sampler",
    "href": "index.html#gibbs-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler"
  },
  {
    "objectID": "index.html#simulation-smoother-and-precision-sampler",
    "href": "index.html#simulation-smoother-and-precision-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Simulation smoother and precision sampler",
    "text": "Simulation smoother and precision sampler"
  },
  {
    "objectID": "index.html#analytical-solution-for-a-joint-posterior",
    "href": "index.html#analytical-solution-for-a-joint-posterior",
    "title": "Bayesian Unobserved Component Models",
    "section": "Analytical solution for a joint posterior",
    "text": "Analytical solution for a joint posterior\n\\[\\underset{T \\times 1}{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix} \\qquad\n\\underset{T \\times 1}{\\tau} = \\begin{pmatrix} \\tau_1 \\\\ \\tau_2 \\\\ \\vdots \\\\\\tau_T \\end{pmatrix} \\qquad\n\\underset{T \\times 1}{\\epsilon} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_T \\end{pmatrix} \\qquad\n\\underset{T \\times 1}{\\eta} = \\begin{pmatrix} \\eta_1 \\\\ \\eta_2 \\\\ \\vdots \\\\ \\eta_T \\end{pmatrix}\\]\n\\[\\underset{T \\times T}{H} =\n\\begin{pmatrix} 1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n-1 & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & -1 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n0&0&0 & \\cdots & 1 & 0  \\\\\n0 & 0 & 0 &0& -1 & 1 \\end{pmatrix}\\]\nIn this setting, we have out state equations as:\n\\[y = \\tau + \\epsilon \\quad\\quad\\qquad \\qquad \\qquad \\qquad \\text{measurement equation}\\]\n\\[\\color{purple}{\\mathbf{H\\tau = \\eta \\implies \\tau = H^{-1} \\eta}} \\quad \\qquad \\color{black}{  \\qquad \\qquad\\text{state equation 1}}\\]\nThe state equations in matrix notation are derived as follows:\n\nIn an unobserved component model, the measurement equation is defined as \\(y_t = \\tau_t + \\epsilon_t\\), in matrix notation, we have:\n\n\\[y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix} = \\begin{pmatrix} \\tau_1 \\\\ \\tau_2 \\\\ \\vdots \\\\ \\tau_T \\end{pmatrix} + \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_T \\end{pmatrix} = \\tau + \\epsilon\\] (2) We have for each t = 1,â€¦,T, \\(\\tau_t = \\tau_{t-1} + \\eta_t\\), and \\(\\tau_0 = 0\\), so in matrix notation:\n\\[\\tau = \\begin{pmatrix} \\tau_1 \\\\ \\tau_2 \\\\ \\vdots \\\\ \\tau_T \\end{pmatrix} = \\begin{pmatrix} \\eta_1 \\\\\n\\tau_1+ \\eta_2 \\\\\n\\tau_2+ \\eta_3 \\\\\n\\vdots \\\\\n\\tau_{T-1} + \\eta_T  \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\tau_1 \\\\ \\vdots \\\\ \\tau_{T-1} \\end{pmatrix} + \\eta\\]\nMoving the lagged \\(\\tau\\) terms on the right hand side to the left:\n\\[\\begin{pmatrix}\n1 & 0 & \\cdots & 0 \\\\\n-1 & 1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & -1 & 1\n\\end{pmatrix} \\begin{pmatrix} \\tau_1 \\\\ \\tau_2 \\\\ \\vdots \\\\ \\tau_T \\end{pmatrix}  = \\begin{pmatrix} \\tau_1 \\\\ \\tau_2-\\tau_1 \\\\ \\vdots \\\\ \\tau_T - \\tau_{T-1} \\end{pmatrix} = \\eta\\]\nHence, we have: \\[H \\tau = \\eta \\implies \\tau = H^{-1}\\eta\\]\nWe assume \\(\\mathbf{\\color{purple}{\\eta \\sim \\mathcal{N}(0_T, c\\sigma^2I_T)}}\\) and \\(\\sigma^2 \\sim \\mathcal{IG2}(s_{prior}, \\nu_{prior})\\)\nThe parameters to be estimated are: \\(\\mathbf{\\color{purple}{\\tau, \\sigma^2}}\\)\n\n2.Prior distributions\n\nPrior distribution for \\(\\tau\\)\n\nFrom the state equation, we have: \\[\\tau =  H^{-1}\\eta\\] we know that \\[\\mathbf{\\color{purple}{\\eta \\sim \\mathcal{N}(0_T, c \\sigma^2 I_T) \\implies H^{-1}\\eta \\sim \\mathcal{N}(0_T, c\\sigma^2(H^T H)^{-1})}}\\] since \\(Var(H^{-1}\\eta) = H^{-1}Var(\\eta)(H^{-1})^T =c\\sigma^2(H^TH)^{-1}\\) Then, the prior distribution of \\(\\tau|c\\sigma^2\\) is \\[\\tau|c\\sigma^2 \\sim \\mathcal{N}(0_T, c\\sigma^2(H^TH)^{-1}) \\\\ \\propto (c\\sigma^2)^{-\\frac{T}{2}}exp\\left(-\\frac{\\tau^TH^T H\\tau}{2c\\sigma^2}\\right)\n\\] (2) Prior assumptions for \\(\\sigma^2\\)\n\\[\\sigma^2 \\sim \\mathcal{IG2}(s_{prior}, \\nu_{prior}) \\propto (\\sigma^2)^\\frac{-\\nu_{prior}+2}{2}exp\\left(-\\frac{s_{prior}}{2\\sigma^2}\\right)\\]\n\n\n3. Posterior distributions\n\n\\(\\mathbf{p(\\tau|y, \\alpha, \\beta, \\sigma^2_e)}\\) We have, from the measurement equation: \\[y = \\tau + \\epsilon\\] since \\(\\epsilon \\sim \\mathcal{N}(0_T, \\sigma^2I_T)\\) \\[y|\\tau, \\sigma^2 \\sim \\mathcal{N}(\\tau, \\sigma^2I_T)\\propto (\\sigma^2)^{-\\frac{T}{2}}exp\\left(-\\frac{1}{2\\sigma^2}(y-\\tau)^T(y-\\tau)\\right)\\] and the prior of \\(\\tau\\) is:\n\n\\[\\tau|c\\sigma^2 \\sim \\mathcal{N}(0_T, c\\sigma^2I_T) \\\\ \\propto (\\sigma^2)^{-\\frac{T}{2}}exp\\left(-\\frac{\\tau^TH^T H\\tau}{2c\\sigma^2}\\right)\\]\nthe prior of \\(\\sigma^2\\) is: \\[p(\\sigma^2) \\propto (\\sigma^2)^-{\\frac{\\nu_{prior}+2}{2}}exp(-\\frac{s_{prior}}{2\\sigma^2})\\]\nThe joint posterior of \\(\\tau\\) and \\(\\sigma^2\\) can be derived as follows:\n\\[\np(\\tau, \\sigma^2 |y) = \\frac{p(\\tau, \\sigma^2, y)}{p(y)} \\propto p(\\tau, \\sigma^2, y) = p(y | \\tau, \\sigma^2)p(\\tau, \\sigma^2) =p(y | \\tau, \\sigma^2)p(\\tau|\\sigma^2)p(\\sigma^2)\n\\]\n\\[\n\\propto (\\sigma^2)^{-\\frac{T}{2}}exp\\left(-\\frac{(y-\\tau)^T(y-\\tau)}{2\\sigma^2}\\right) \\times\n(\\sigma^2)^{-\\frac{T}{2}}exp\\left(-\\frac{\\tau^TH^T H\\tau}{2c\\sigma^2}\\right) \\times\n(\\sigma^2)^{-\\frac{\\nu_{prior}+2}{2}} exp(-\\frac{s_{prior}}{2\\sigma^2})\n\\] \\[\n\\propto exp(-\\frac{y^Ty - 2\\tau^T y + \\tau^T\\tau + c^{-1}\\tau^TH^T H\\tau}{2\\sigma^2}) \\times \\\\\nexp(-\\frac{s_{prior}}{2\\sigma^2})\\times(\\sigma^2)^{-\\frac{2T+\\nu_{prior}+2}{2}}\n\\] \\[ = exp(-\\frac{\\tau^T(c^{-1}H^T H + I_T)\\tau - 2\\tau^Ty}{2\\sigma^2}) \\times \\\\\nexp(-\\frac{y^Ty + s_{prior}}{2\\sigma^2})\\times(\\sigma^2)^{-\\frac{2T+\\nu_{prior}+2}{2}} \\]\nHence, \\(\\tau, \\Sigma |y \\sim \\mathcal{NIG2}(\\bar{\\tau}, \\bar{\\Sigma}, \\bar{\\nu}, \\bar{s})\\) where \\[\\bar{\\Sigma} = (c^{-1}H^T H + I_T)^{-1}\\] \\[\\bar{\\tau} = \\bar{\\Sigma}y\\] \\[\\bar{\\nu} = 2T+\\nu_{prior}\\] \\[\\bar{s} = s_{prior}+y^Ty\\]\n\nUC.local.simple.Gibbs.sampler    = function(S, starting.values, priors){\n  # Estimation of a local level UC model with \\tau_0 = 0 and no AR trend. \n  aux     = starting.values\n  T       = nrow(aux$Y)\n  posteriors    = list(\n    tau     = matrix(NA,T,S),\n    sigma   = matrix(NA,2,S),\n    non.stationary.iterations = rep(NA,S)\n  )\n\n  for (s in 1:S){\n    # Sampling sigma\n    ###########################\n    sigma.s   = priors$sigma.s +t(aux$Y)%*%aux$Y\n    sigma.nu  = priors$sigma.nu + 2*T\n    sigma.draw= sigma.s/rchisq(1,sigma.nu)\n\n    # Sampling tau\n    ###########################\n    V.tau.inv     = priors$c^(-1)*t(aux$H)%*%(aux$H) + diag(T)\n    V.tau.inv     = 0.5*(V.tau.inv + t(V.tau.inv))\n    b.tau         = aux$Y\n    precision.L   = t(bandchol(V.tau.inv))\n    epsilon       = rnorm(T)\n    b.tau.tmp     = forwardsolve(precision.L, b.tau)\n    tau.draw      = backsolve(t(precision.L), b.tau.tmp + epsilon)\n    aux$tau       = tau.draw\n\n    posteriors$tau[,s]     = aux$tau\n    posteriors$sigma[,s]   = aux$sigma\n    # posteriors$non.stationary.iterations[s] = ns.i\n    if (s%%1000==0){cat(\" \",s)}\n  }\n\n  output      = list(\n    posterior = posteriors,\n    last.draw = aux\n  )\n  return(output)\n}"
  },
  {
    "objectID": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "href": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating gamma error term variance prior scale",
    "text": "Estimating gamma error term variance prior scale\nTo estimate the gamma error term variance, we need to firstly put a prior on the prior scale of and present its full conditional posterior distribution.\nThe prior is below:\n\\[\\begin{align*}\n\\sigma^2 \\mid s &\\sim \\text{IG2}(s, \\nu) \\\\\ns &\\sim \\mathcal{G}(s, a)\n\\end{align*}\\]\nThen we use Gibbs sampling to draw samples from the full conditional distributions:\n\nlibrary(MCMCpack)\n# Define prior parameters\ns_prior &lt;- 2\nnu_prior &lt;- 2\na_prior &lt;- 2\n\n# Define the function for the conditional posterior distribution\nposterior_sampler &lt;- function(y, n_iter = 1000) {\n  # Store samples\n  s &lt;- rgamma(1, shape = a_prior, rate = s_prior)\n  sigma2 &lt;- rinvgamma(1, shape = s, scale = nu_prior)  \n\n  s_samples &lt;- numeric(n_iter)\n  sigma2_samples &lt;- numeric(n_iter)\n\n  for (i in 1:n_iter) {\n    shape_sigma2 &lt;- (length(y) / 2) + s\n    rate_sigma2 &lt;- (sum((y - mean(y))^2) / 2) + nu_prior\n    sigma2 &lt;- rinvgamma(1, shape = shape_sigma2, scale = rate_sigma2)  \n\n    shape_s &lt;- a_prior + 1\n    rate_s &lt;- s_prior + sigma2\n    s &lt;- rgamma(1, shape = shape_s, rate = rate_s)\n\n    sigma2_samples[i] &lt;- sigma2\n    s_samples[i] &lt;- s\n  }\n\n  return(list(sigma2_samples = sigma2_samples, s_samples = s_samples))\n}\n\nset.seed(123)\ny &lt;- rnorm(100, mean = 0, sd = 1)\n\n# Run the sampler\nposterior_samples &lt;- posterior_sampler(y, n_iter = 1000)\n\n# Plot posterior distributions\npar(mfrow = c(2, 1))\nhist(posterior_samples$sigma2_samples, main = expression(paste(\"Posterior of \", sigma^2)), xlab = expression(sigma^2))\nhist(posterior_samples$s_samples, main = \"Posterior of s\", xlab = \"s\")\n\n\n\n\n\n\n\n\nBased on the sampling results, we summarize the posterior distribution of these parameters.\nAs can be seen from the histogram, the values of \\(\\sigma^2\\) are mostly concentrated between 0.6 and 1.0. The distribution has a certain skewness, with a small number of higher values on the right, but it is generally concentrated.\nMost of the values of \\(s\\) are concentrated between 0 and 2, indicating that the probability of s is higher in this range and the distribution has a longer right tail.\nTo derive the full conditional posteriors for \\(\\sigma^2\\) and \\(s\\), we start with the joint posterior distribution of these parameters given the data \\(y\\).\nThe joint posterior distribution is below:\n\\[\\begin{equation}\np(\\sigma^2, s \\mid y) \\propto p(y \\mid \\sigma^2) p(\\sigma^2 \\mid s) p(s)\n\\end{equation}\\]\nFull Conditional Posterior for \\(\\sigma^2\\) is:\n\\[\\begin{equation}\np(\\sigma^2 \\mid y, s) \\propto (\\sigma^2)^{-\\left( \\frac{T}{2} + s + 1 \\right)} \\exp \\left( -\\frac{1}{\\sigma^2} \\left( \\frac{(y - \\tau)'(y - \\tau)}{2} + \\nu \\right) \\right)\n\\end{equation}\\]\nFull Conditional Posterior for \\(s\\) is:\n\\[\\begin{equation}\np(s \\mid \\sigma^2) \\propto s^{a-1} \\nu^s (\\sigma^2)^{-(s+1)} \\exp \\left( -s - \\frac{\\nu}{\\sigma^2} \\right)\n\\end{equation}\\]"
  },
  {
    "objectID": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "href": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating inverted-gamma 2 error term variance prior scale",
    "text": "Estimating inverted-gamma 2 error term variance prior scale"
  },
  {
    "objectID": "index.html#estimating-the-initial-condition-prior-scale",
    "href": "index.html#estimating-the-initial-condition-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating the initial condition prior scale",
    "text": "Estimating the initial condition prior scale"
  },
  {
    "objectID": "index.html#student-t-prior-for-the-trend-component",
    "href": "index.html#student-t-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t prior for the trend component",
    "text": "Student-t prior for the trend component"
  },
  {
    "objectID": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "href": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating Student-t degrees of freedom parameter",
    "text": "Estimating Student-t degrees of freedom parameter\nThe Student-t distribution is commonly used in statistical modeling to handle data with heavier tails than the normal distribution. An essential parameter of the Student-t distribution is the degrees of freedom \\(\\nu\\), which controls the tail heaviness. In this note, we present the Bayesian estimation of the degrees of freedom parameter for an N-variate Student-t distribution using the Inverted-Gamma 2 (IG2) scale mixture of normals.\nThe N-variate Student-t distribution can be represented as a scale mixture of normals:\n\\[\n\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda \\sim \\mathcal{N}(\\mathbf{\\mu}, \\lambda \\mathbf{I}_N)\n\\]\n\\[\n\\lambda \\mid \\nu \\sim \\mathcal{IG2}(\\nu, \\nu)\n\\]\nwhere:\n\n\\(\\mathbf{y}\\) is the \\(N\\)-dimensional observation vector.\n\\(\\mathbf{\\mu}\\) is the mean vector.\n\\(\\lambda\\) is the latent scale variable.\n\\(\\nu\\) is the degrees of freedom parameter.\n\n\nDerivation of Full Conditional Posteriors\n\nFull Conditional Posterior of \\(\\lambda\\)\nGiven the prior distribution:\n\\[\n\\lambda \\mid \\nu \\sim \\mathcal{IG2}(\\nu, \\nu)\n\\]\nThe likelihood of the data given \\(\\lambda\\) is:\n\\[\n\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda \\sim \\mathcal{N}(\\mathbf{\\mu}, \\lambda \\mathbf{I}_N)\n\\]\nThe full conditional posterior of \\(\\lambda\\) can be derived as follows:\n\nLikelihood of \\(\\mathbf{y}\\) given \\(\\mathbf{\\mu}\\) and \\(\\lambda\\):\n\\[\np(\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda) \\propto \\lambda^{-\\frac{N}{2}} \\exp\\left(-\\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{2\\lambda}\\right)\n\\]\nPrior for \\(\\lambda\\) given \\(\\nu\\):\n\\[\np(\\lambda \\mid \\nu) \\propto \\lambda^{-\\nu - 1} \\exp\\left(-\\frac{\\nu}{\\lambda}\\right)\n\\]\nJoint distribution:\n\\[\np(\\mathbf{y}, \\lambda \\mid \\mathbf{\\mu}, \\nu) = p(\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda) p(\\lambda \\mid \\nu)\n\\]\nFull conditional posterior:\n\\[\np(\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, \\nu) \\propto \\lambda^{-\\frac{N}{2}} \\exp\\left(-\\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{2\\lambda}\\right) \\lambda^{-\\nu - 1} \\exp\\left(-\\frac{\\nu}{\\lambda}\\right)\n\\]\nCombining terms:\n\\[\np(\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, \\nu) \\propto \\lambda^{-\\left(\\nu + \\frac{N}{2} + 1\\right)} \\exp\\left(-\\frac{\\nu + \\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{2}}{\\lambda}\\right)\n\\]\nThis is recognized as the kernel of an Inverted-Gamma 2 distribution:\n\\[\n\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, \\nu \\sim \\mathcal{IG2}\\left(\\nu + N, \\nu + (\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})\\right)\n\\]\n\n\n\nFull Conditional Posterior of \\(\\nu\\)\nTo estimate \\(\\nu\\), we use the Metropolis-Hastings algorithm due to its non-standard form. The steps for deriving the full conditional posterior of \\(\\nu\\) are as follows:\n\nLikelihood of \\(\\lambda\\) given \\(\\nu\\):\n\\[\np(\\lambda \\mid \\nu) = \\frac{\\left(\\frac{\\nu}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)} \\lambda^{-\\left(\\nu/2 + 1\\right)} \\exp\\left(-\\frac{\\nu}{2\\lambda}\\right)\n\\]\nLog-likelihood for \\(\\nu\\) given \\(\\lambda\\):\n\\[\n\\log p(\\lambda \\mid \\nu) = \\frac{\\nu}{2} \\log\\left(\\frac{\\nu}{2}\\right) - \\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\left(\\frac{\\nu}{2} + 1\\right) \\log \\lambda - \\frac{\\nu}{2\\lambda}\n\\]\nLog-prior for \\(\\nu\\) (assuming a non-informative prior):\n\\[\n\\log p(\\nu) = \\text{constant}\n\\]\nFull conditional posterior:\nThe full conditional posterior for \\(\\nu\\) is proportional to the product of the likelihood and the prior:\n\\[\np(\\nu \\mid \\lambda) \\propto p(\\lambda \\mid \\nu) p(\\nu)\n\\]\nSince \\(p(\\nu)\\) is constant, we focus on \\(p(\\lambda \\mid \\nu)\\):\n\\[\n\\log p(\\nu \\mid \\lambda) = \\frac{\\nu}{2} \\log\\left(\\frac{\\nu}{2}\\right) - \\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\left(\\frac{\\nu}{2} + 1\\right) \\log \\lambda - \\frac{\\nu}{2\\lambda}\n\\]\nThis expression does not have a closed form, so we use the Metropolis-Hastings algorithm to sample from this posterior.\n\n\n\n\nR Function for Gibbs Sampler\nBelow is the R function implementing the Gibbs sampler for estimating \\(\\nu\\) using the IG2-scale mixture of normals representation.\n\nmetropolis_hastings_nu &lt;- function(y, mu, n_iter, init_nu, proposal_sd) {\n  # Initialize parameter\n  nu &lt;- init_nu\n  N &lt;- length(y)\n  \n  # Storage for samples\n  nu_samples &lt;- numeric(n_iter)\n  \n  # Log-likelihood function\n  log_likelihood &lt;- function(nu, y, mu) {\n    sum(dt((y - mu) / sqrt(nu), df = nu, log = TRUE))\n  }\n  \n  for (i in 1:n_iter) {\n    # Propose new value for nu\n    nu_proposal &lt;- nu + rnorm(1, 0, proposal_sd)\n    \n    if (nu_proposal &gt; 0) {\n      # Calculate log acceptance ratio\n      log_acceptance_ratio &lt;- log_likelihood(nu_proposal, y, mu) - log_likelihood(nu, y, mu)\n      \n      # Accept or reject the proposal\n      if (log(runif(1)) &lt; log_acceptance_ratio) {\n        nu &lt;- nu_proposal\n      }\n    }\n    \n    # Store the sample\n    nu_samples[i] &lt;- nu\n  }\n  \n  return(nu_samples)\n}\n\ngibbs_sampler_t &lt;- function(y, n_iter, init_values) {\n  # Initialize parameters\n  nu &lt;- init_values$nu\n  mu &lt;- init_values$mu\n  N &lt;- length(y)\n  \n  # Storage for samples\n  nu_samples &lt;- numeric(n_iter)\n  mu_samples &lt;- numeric(n_iter)\n  lambda_samples &lt;- numeric(n_iter)\n  \n  for (i in 1:n_iter) {\n    # Sample lambda\n    shape_lambda &lt;- nu + N\n    rate_lambda &lt;- nu + sum((y - mu)^2)\n    lambda &lt;- 1 / rgamma(1, shape = shape_lambda, rate = rate_lambda)\n    \n    # Sample mu\n    mu &lt;- rnorm(1, mean = mean(y), sd = sqrt(lambda / N))\n    \n    # Sample nu using Metropolis-Hastings\n    log_likelihood &lt;- function(nu, y, mu) {\n      sum(dt((y - mu) / sqrt(nu), df = nu, log = TRUE))\n    }\n    \n    proposal_nu &lt;- nu + rnorm(1, 0, 0.1) # proposal distribution: normal random walk\n    if (proposal_nu &gt; 0) {\n      log_acceptance_ratio &lt;- log_likelihood(proposal_nu, y, mu) - log_likelihood(nu, y, mu)\n      if (log(runif(1)) &lt; log_acceptance_ratio\n\n) {\n        nu &lt;- proposal_nu\n      }\n    }\n    \n    # Store samples\n    nu_samples[i] &lt;- nu\n    mu_samples[i] &lt;- mu\n    lambda_samples[i] &lt;- lambda\n  }\n  \n  return(list(nu = nu_samples, mu = mu_samples, lambda = lambda_samples))\n}\n\n# Example usage\nset.seed(123)\ny &lt;- rnorm(100)\ninit_values &lt;- list(nu = 5, mu = mean(y))\nn_iter &lt;- 1000\nresult &lt;- gibbs_sampler_t(y, n_iter, init_values)\n\n# Display the results\nprint(summary(result$nu))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.118   6.414   6.947   7.398   7.888  10.821 \n\nprint(summary(result$mu))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.19910  0.02664  0.08867  0.08544  0.14827  0.34116 \n\nprint(summary(result$lambda))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6030  0.7907  0.8465  0.8537  0.9095  1.1743 \n\n\n\n\nConclusion\nThis note provided a comprehensive step-by-step algebraic derivation and a sampler for estimating the degrees of freedom parameter \\(\\nu\\) for an N-variate Student-t distribution using an IG2-scale mixture of normals approach within a Bayesian framework. By using the Metropolis-Hastings algorithm, we avoid the need to assume a prior distribution for \\(\\nu\\), simplifying the estimation process. This approach allows for flexible modeling of heavy-tailed data, which is often encountered in practice.\n\n\nReferences\n\nGeweke, J. (1993). Bayesian treatment of the independent Student-t linear model. Journal of Applied Econometrics, 8(S1), S19-S40.\nChib, S., & Greenberg, E. (1995). Understanding the Metropolis-Hastings Algorithm. The American Statistician, 49(4), 327-335."
  },
  {
    "objectID": "index.html#laplace-prior-for-the-trend-component",
    "href": "index.html#laplace-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Laplace prior for the trend component",
    "text": "Laplace prior for the trend component"
  },
  {
    "objectID": "index.html#autoregressive-cycle-component",
    "href": "index.html#autoregressive-cycle-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Autoregressive cycle component",
    "text": "Autoregressive cycle component"
  },
  {
    "objectID": "index.html#random-walk-with-time-varying-drift-parameter",
    "href": "index.html#random-walk-with-time-varying-drift-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Random walk with time-varying drift parameter",
    "text": "Random walk with time-varying drift parameter"
  },
  {
    "objectID": "index.html#student-t-error-terms",
    "href": "index.html#student-t-error-terms",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t error terms",
    "text": "Student-t error terms"
  },
  {
    "objectID": "index.html#conditional-heteroskedasticity",
    "href": "index.html#conditional-heteroskedasticity",
    "title": "Bayesian Unobserved Component Models",
    "section": "Conditional heteroskedasticity",
    "text": "Conditional heteroskedasticity"
  },
  {
    "objectID": "index.html#predictive-density",
    "href": "index.html#predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Predictive density",
    "text": "Predictive density"
  },
  {
    "objectID": "index.html#sampling-from-the-predictive-density",
    "href": "index.html#sampling-from-the-predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Sampling from the predictive density",
    "text": "Sampling from the predictive density"
  },
  {
    "objectID": "index.html#missing-observations",
    "href": "index.html#missing-observations",
    "title": "Bayesian Unobserved Component Models",
    "section": "Missing observations",
    "text": "Missing observations"
  },
  {
    "objectID": "index.html#references-1",
    "href": "index.html#references-1",
    "title": "Bayesian Unobserved Component Models",
    "section": "References",
    "text": "References"
  }
]