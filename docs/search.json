[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Unobserved Component Models",
    "section": "",
    "text": "Abstract. We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. We scrutinise Bayesian forecasting and sampling from the predictive density.\nKeywords. Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling"
  },
  {
    "objectID": "index.html#matrix-notation-for-the-model",
    "href": "index.html#matrix-notation-for-the-model",
    "title": "Bayesian Unobserved Component Models",
    "section": "Matrix notation for the model",
    "text": "Matrix notation for the model\nTo simplify the notation and the derivations introduce matrix notation for the model. Let \\(T\\) be the available sample size for the variable \\(y\\). Define a \\(T\\)-vector of zeros, \\(\\mathbf{0}_T\\), and of ones, \\(\\boldsymbol\\imath_T\\), the identity matrix of order \\(T\\), \\(\\mathbf{I}_T\\), as well as \\(T\\times1\\) vectors: \\[\\begin{align}\n\\mathbf{y} = \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_T \\end{bmatrix},\\quad\n\\boldsymbol\\tau = \\begin{bmatrix} \\tau_1\\\\ \\vdots\\\\ \\tau_T \\end{bmatrix},\\quad\n\\boldsymbol\\epsilon = \\begin{bmatrix} \\epsilon_1\\\\ \\vdots\\\\ \\epsilon_T \\end{bmatrix},\\quad\n\\boldsymbol\\eta = \\begin{bmatrix} \\eta_1\\\\ \\vdots\\\\ \\eta_T \\end{bmatrix},\\qquad\n\\mathbf{i} = \\begin{bmatrix} 1\\\\0\\\\ \\vdots\\\\ 0 \\end{bmatrix},\n\\end{align}\\] and a \\(T\\times T\\) matrix \\(\\mathbf{H}\\) with the elements: \\[\\begin{align}\n\\mathbf{H} = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 & 0\\\\\n-1 & 1 & \\cdots & 0 & 0\\\\\n0 & -1 & \\cdots & 0 & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n0 & 0 & \\cdots & 1 & 0\\\\\n0 & 0 & \\cdots & -1 & 1\n\\end{bmatrix}.\n\\end{align}\\]\nThen the model can be written in a concise notation as: \\[\\begin{align}\n\\mathbf{y} &= \\mathbf{\\tau} + \\boldsymbol\\epsilon,\\\\\n\\mathbf{H}\\boldsymbol\\tau &= \\mathbf{i} \\tau_0 + \\boldsymbol\\eta,\\\\\n\\boldsymbol\\epsilon &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma^2\\mathbf{I}_T\\right),\\\\\n\\boldsymbol\\eta &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma_\\eta^2\\mathbf{I}_T\\right).\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#likelihood-function",
    "href": "index.html#likelihood-function",
    "title": "Bayesian Unobserved Component Models",
    "section": "Likelihood function",
    "text": "Likelihood function\nThe model equations imply the predictive density of the data vector \\(\\mathbf{y}\\). To see this, consider the model equation as a linear transformation of a normal vector \\(\\boldsymbol\\epsilon\\). Therefore, the data vector follows a multivariate normal distribution given by: \\[\\begin{align}\n\\mathbf{y}\\mid \\boldsymbol\\tau, \\sigma^2 &\\sim\\mathcal{N}_T\\left(\\boldsymbol\\tau, \\sigma^2\\mathbf{I}_T\\right).\n\\end{align}\\]\nThis distribution determines the shape of the likelihood function that is defined as the sampling data density: \\[\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y})\\equiv p\\left(\\mathbf{y}\\mid\\boldsymbol\\tau, \\sigma^2 \\right).\n\\end{align}\\]\nThe likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of \\(\\mathbf{y}\\), is considered a function of parameters \\(\\boldsymbol\\tau\\) and \\(\\sigma^2\\) is given by: \\[\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y}) =\n(2\\pi)^{-\\frac{T}{2}}\\left(\\sigma^2\\right)^{-\\frac{T}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y} - \\boldsymbol\\tau)'(\\mathbf{y} - \\boldsymbol\\tau)\\right\\}.\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#prior-distributions",
    "href": "index.html#prior-distributions",
    "title": "Bayesian Unobserved Component Models",
    "section": "Prior distributions",
    "text": "Prior distributions"
  },
  {
    "objectID": "index.html#gibbs-sampler",
    "href": "index.html#gibbs-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler"
  },
  {
    "objectID": "index.html#simulation-smoother-and-precision-sampler",
    "href": "index.html#simulation-smoother-and-precision-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Simulation smoother and precision sampler",
    "text": "Simulation smoother and precision sampler"
  },
  {
    "objectID": "index.html#analytical-solution-for-a-joint-posterior",
    "href": "index.html#analytical-solution-for-a-joint-posterior",
    "title": "Bayesian Unobserved Component Models",
    "section": "Analytical solution for a joint posterior",
    "text": "Analytical solution for a joint posterior\n\n1. Model Setup\nIn this section, we present an analytical solution for a joint posterior for and for a variation to the standard Bayesian unobserbed component model. Specifically, we assume in this case \\(\\tau_0 = 0\\) and \\(\\sigma_{\\eta}^2 = c\\sigma^2\\) is constant and c is constant signal-to-noise ratio. We have the following setup:\n\\[\\underset{T \\times 1}{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix} \\qquad\n\\underset{T \\times 1}{\\tau} = \\begin{pmatrix} \\tau_1 \\\\ \\tau_2 \\\\ \\vdots \\\\\\tau_T \\end{pmatrix} \\qquad\n\\underset{T \\times 1}{\\epsilon_{1:T}} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_T \\end{pmatrix} \\qquad\n\\underset{T \\times 1}{\\eta} = \\begin{pmatrix} \\eta_1 \\\\ \\eta_2 \\\\ \\vdots \\\\ \\eta_T \\end{pmatrix}\\qquad\n\\underset{T \\times 1}{e} = \\begin{pmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_T \\end{pmatrix} \\] \\[\\underset{T \\times 1}{I_T} = \\begin{pmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{pmatrix} \\qquad \\underset{T \\times 1}{e_{1.T}} = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\qquad\n\\underset{2 \\times 1}{\\beta} = \\begin{pmatrix}\\mu \\\\ 0 \\end{pmatrix} \\qquad\n\\underset{p \\times 1}{\\alpha} = \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_p \\end{pmatrix} \\qquad \\color{purple}{\\mathbf{\\underset{T \\times 1}{x_{\\tau}} = I_T}}\\]\n\\[\\underset{T \\times p}{X_{\\epsilon}} = \\begin{bmatrix}\\epsilon_{0:T-1} & \\epsilon_{-1:T-2} & \\cdots & \\epsilon_{-p+1:T-p} \\end{bmatrix} = \\begin{pmatrix} \\epsilon_0 & \\epsilon_1 & \\cdots & \\epsilon_{-p+1} \\\\ \\epsilon_1 & \\epsilon_2 & \\cdots & \\epsilon_{-p+2} \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ \\epsilon_{T-1} & \\epsilon_{T-2} & \\cdots & \\epsilon_{T-p} \\end{pmatrix}\\]\n\\[\\underset{T \\times T}{H} =\n\\begin{pmatrix} 1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n-1 & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & -1 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n0&0&\\vdots & \\vdots & 1 & 0  \\\\\n0 & 0 & 0 &0& -1 & 1 \\end{pmatrix} \\qquad\n\\underset{T \\times T}{H_{\\alpha}} =\n\\begin{pmatrix} 1 & 0 & 0 & 0 & \\cdots&\\cdots& \\cdots&0 & 0 \\\\\n-\\alpha_1 & 1 &0 & 0 & \\cdots & \\cdots&\\cdots&0 & 0 \\\\\n-\\alpha_2 & -\\alpha_1 & 1 & 0 & \\cdots & \\cdots&\\cdots& 0 & 0 \\\\\n- \\alpha_3 & -\\alpha_2 & -\\alpha_1 & 1 & \\ddots &  \\cdots& \\cdots&0 & 0 \\\\\n\\ddots & \\ddots & \\ddots & \\ddots&\\ddots& \\ddots & \\ddots & \\ddots & \\ddots \\\\\n-\\alpha_p & -\\alpha_{p-1} & \\ddots & \\cdots &\\cdots& \\cdots& \\cdots & \\cdots & 0 \\\\\n0 & -\\alpha_p &  \\ddots & \\ddots &\\cdots&\\cdots& \\ddots & \\ddots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\ddots&\\ddots&\\ddots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots  & -\\alpha_p & \\cdots & -\\alpha_3& - \\alpha_2& -\\alpha_1 & 1\\end{pmatrix}\\]\nIn this setting, we have out state equations as:\n\\[y = \\tau + \\epsilon \\qquad \\qquad\\qquad\\qquad \\quad\\qquad \\qquad \\qquad \\qquad \\text{measurement equation}\\]\n\\[\\tau = H^{-1}X_{\\tau}\\beta + H^{-1} \\eta  = \\color{purple}{\\mathbf{H^{-1}\\mu I_T + H^{-1}\\eta}}  \\quad \\color{black}{  \\qquad \\qquad\\text{state equation 1}}\\]\n\\[\\epsilon = X_{\\epsilon} \\alpha + e = H^{-1}_{\\alpha}e  \\qquad\\quad \\qquad \\quad \\qquad \\qquad \\qquad \\qquad\\text{state equation 2} \\]\nThe state equations in matrix notation are derived as follows:\n\nIn an unobserved component model, the measurement equation is defined as \\(y_t = \\tau_t + \\epsilon_t\\), in matrix notation, we have:\n\n\\[y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix} = \\begin{pmatrix} \\tau_1 \\\\ \\tau_2 \\\\ \\vdots \\\\ \\tau_T \\end{pmatrix} + \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_T \\end{pmatrix} = \\tau + \\epsilon\\] (2) We have for each t = 1,…,T, \\(\\tau_t = \\mu + \\tau_{t-1} + \\eta_t\\), and \\(\\tau_0 = 0\\), so in matrix notation:\n\\[\\tau = \\begin{pmatrix} \\tau_1 \\\\ \\tau_2 \\\\ \\vdots \\\\ \\tau_T \\end{pmatrix} = \\begin{pmatrix} \\mu + \\eta_1 \\\\\n\\mu + \\tau_1+ \\eta_2 \\\\\n\\mu + \\tau_2+ \\eta_3 \\\\\n\\vdots \\\\\n\\mu + \\tau_{T-1} + \\eta_T  \\end{pmatrix} = \\mu I_T + \\begin{pmatrix} 0 \\\\ \\tau_1 \\\\ \\vdots \\\\ \\tau_{T-1} \\end{pmatrix} + \\eta\\]\nMoving the lagged \\(\\tau\\) terms on the right hand side to the left:\n\\[\\begin{pmatrix}\n1 & 0 & \\cdots & 0 \\\\\n-1 & 1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & -1 & 1\n\\end{pmatrix} \\begin{pmatrix} \\tau_1 \\\\ \\tau_2 \\\\ \\vdots \\\\ \\tau_T \\end{pmatrix}  = \\begin{pmatrix} \\tau_1 \\\\ \\tau_2-\\tau_1 \\\\ \\vdots \\\\ \\tau_T - \\tau_{T-1} \\end{pmatrix} = \\mu I_T + \\eta\\]\nHence, we have: \\[H \\tau = \\mu I_T + \\eta = x_{\\tau}\\beta + \\eta \\implies \\tau = H^{-1}X_\\tau\\beta + H^{-1}\\eta\\]\n\nthe unit room stationary component is defined as: \\(\\epsilon_t = \\alpha_1 \\epsilon_{t-1} + \\alpha_2 \\epsilon_{t-2} + \\cdots + \\alpha_p \\epsilon_{t-p} + e+t\\) In matrix, notation:\n\n\\[\\begin{pmatrix}\n\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_T \\end{pmatrix} = \\begin{pmatrix} \\alpha_1 \\epsilon_0 + \\alpha_2 \\epsilon_{-1} + \\cdots+\\alpha_p \\epsilon_{-p+1} +e_1 \\\\\n\\alpha_1 \\epsilon_1+ \\alpha_2 \\epsilon_{0} + \\cdots+\\alpha_p \\epsilon_{-p+2} +e_2 \\\\\n\\vdots  \\\\\n\\alpha_1 \\epsilon_T + \\alpha_2 \\epsilon_{T-1} + \\cdots+\\alpha_p \\epsilon_{T-p} +e_T\n\\end{pmatrix} \\implies H_{\\alpha} \\epsilon = e \\implies \\epsilon = H_{\\alpha}^{-1}e\\] since we assume \\(\\begin{pmatrix}\\epsilon_0, \\epsilon_1, \\cdots, \\epsilon_T\\end{pmatrix}^T = 0\\) Similarly, we have\n\\[\\begin{pmatrix}\n\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_T \\end{pmatrix} =\n\\begin{pmatrix} \\epsilon_0 & \\epsilon_{-1} & \\cdots & \\epsilon_{-p+1} \\\\\n\\epsilon_1 & \\epsilon_{0} & \\cdots & \\epsilon_{-p+2} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\epsilon_{T-1} & \\epsilon_{T-2} & \\cdots & \\epsilon_{-p+T} \\\\\n\\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_p \\end{pmatrix} + \\begin{pmatrix}  e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_T \\end{pmatrix} = X_{\\epsilon} \\alpha + e\\]\nWe have \\(\\mathbf{\\color{purple}{\\eta \\sim \\mathcal{N}(0_T, c\\sigma^2I_T)}}\\) and \\(e \\sim \\mathcal{N}(0_T, \\sigma^2_eI_T)\\).\nThe parameters to be estimated are: \\(\\mathbf{\\color{purple}{\\sigma^2_e, \\tau, \\epsilon, \\beta = \\mu}}\\)\n\n\n2.Prior distributions\n\nPrior distribution for \\(\\tau\\)\n\nFrom the state equation, we have: \\[\\tau = H^{-1}\\mu I_T + H^{-1}\\eta\\] we know that \\[\\mathbf{\\color{purple}{\\eta \\sim \\mathcal{N}(0_T, c \\sigma^2 I_T) \\implies H^{-1}\\eta \\sim \\mathcal{N}(0_T, c\\sigma^2(H^TH)^{-1})}}\\] since \\(Var(H^{-1}\\eta) = H^{-1}Var(\\eta)(H^{-1})^T =c\\sigma^2(H^TH)^{-1}\\) Then, the prior distribution of \\(\\tau|\\beta, c\\sigma^2\\) is \\[\\tau|\\beta, c\\sigma^2 \\sim \\mathcal{N}(H^{-1}\\mu I_T, c\\sigma^2(H^TH)^{-1}) \\\\ \\propto exp\\left(-\\frac{1}{2}\\frac{(\\tau - H^{-1}\\mu I_T)^TH^TH(\\tau - H^{-1}\\mu I_T)}{c\\sigma^2}\\right)\\] (2) Prior distribution for \\(\\epsilon\\) From the state equation, we have:\n\\[\\epsilon = H^{-1}_{\\alpha}e\\] since \\(e \\sim \\mathcal{N}(0_T, \\sigma^2_eI_T)\\), we have:\n\\[\\epsilon | \\sigma_e, \\alpha \\sim \\mathcal{N}(0_T,\\sigma_e^2(H_\\alpha^TH_\\alpha)^{-1}) \\\\ \\propto exp\\left(\\frac{1}{2\\sigma^2_e}\\epsilon^T(H_\\alpha^TH_\\alpha)\\epsilon\\right)\\] (3) Prior assumptions for other parameters\n\\[\\alpha \\sim \\mathcal{N}({\\alpha_{prior}, V_{prior}})\\mathbb{1}_{\\alpha \\in A} \\propto exp\\left({-\\frac{1}{2}(\\alpha - \\alpha_{prior})^T}V_{\\alpha,prior}^{-1}(\\alpha - \\alpha_{prior})\\right)\\mathbb{1}_{\\alpha \\in A}\\] \\[\\beta = \\mu\\sim \\mathcal{N}(\\beta_{prior}, V_{\\beta, prior} \\propto exp\\left({-\\frac{1}{2}(\\beta - \\beta_{prior})^T}V_{\\beta,prior}^{-1}(\\beta - \\beta_{prior})\\right)\\] \\[\\sigma^2_e \\sim \\mathcal{IG2}(s_{prior}, \\nu_{prior}) \\propto (\\sigma^2_e)^\\frac{-\\nu+2}{2}exp\\left(-\\frac{s_{prior}}{2\\sigma^2_e}\\right)\\] and\n\\[p(\\tau, \\epsilon, \\alpha, \\beta, \\sigma^2_e) = p(\\tau|\\beta)p(\\beta)p(\\epsilon|\\alpha,\\sigma^2_e)p(\\alpha)p(\\sigma^2_e)\\]\n\n\n3. Posterior distributions\n\n\\(\\mathbf{p(\\tau|y, \\alpha, \\beta, \\sigma^2_e)}\\) We have, from the measurement equation: \\[y = \\tau + \\epsilon\\] since \\(\\epsilon \\sim \\mathcal{N}(0_T, \\sigma^2_e(H^T_\\alpha H_\\alpha)^{-1})\\) \\[y|\\tau, \\alpha, \\sigma^2_e \\sim \\mathcal{N}(\\tau, \\sigma^2_e(H^T_\\alpha H_\\alpha)^{-1})\\propto exp\\left(-\\frac{1}{2\\sigma^2_e}(y-\\tau)^TH_\\alpha^TH_\\alpha(y-\\tau)\\right)\\] and the prior of \\(\\tau\\) is:\n\n\\[\\tau|\\beta, c\\sigma^2 \\sim \\mathcal{N}(H^{-1}\\mu I_T, c\\sigma^2(H^TH)^{-1}) \\\\ \\propto exp\\left(-\\frac{1}{2}\\frac{(\\tau - H^{-1}\\mu I_T)^TH^TH(\\tau - H^{-1}\\mu I_T)}{c\\sigma^2}\\right)\\]\nThe posterior of \\(\\tau\\) is:\n\\[p(\\tau |\\alpha,  \\beta, \\sigma^2_e, y) = \\frac{p(\\tau,\\alpha, \\beta, \\sigma^2_e, y)}{p(\\alpha, \\beta, \\sigma^2_e, y)} \\propto p(\\tau,\\alpha, \\beta, \\sigma^2_e, y)=p(y|\\tau, \\alpha, , \\beta, \\sigma^2_e)p(\\tau, \\alpha, \\beta, \\sigma^2_e)\\]\n\\[= p(y|\\tau, \\alpha, \\beta, \\sigma^2_e)p(\\tau|\\alpha, \\beta, \\sigma^2_e)p(\\alpha, \\beta, \\sigma^2_e) \\propto  p(y|\\tau, \\alpha, \\beta, \\sigma^2_e)p(\\tau|\\alpha, \\beta, \\sigma^2_e)\\]\n\\[\\propto exp\\left(-\\frac{1}{2\\sigma^2_e}(y-\\tau)^TH_\\alpha^TH_\\alpha(y-\\tau)\\right)exp\\left(-\\frac{1}{2}\\frac{(\\tau - H^{-1}\\mu I_T)^TH^TH(\\tau - H^{-1}\\mu I_T)}{c\\sigma^2}\\right)\\]\n\\[= exp\\left(-\\frac{1}{2}\\left[\\sigma^{-2}_ey^T H_\\alpha^TH_\\alpha y - 2\\sigma_e^{-2}\\tau^TH_\\alpha^TH_\\alpha y +  \\sigma_e^{-2}\\tau^T H_\\alpha^TH_\\alpha \\tau\n+ c^{-1}\\sigma^{-2}\\tau^T H^TH \\tau -2 c^{-1}\\sigma^{-2} \\tau^T H^T H H^{-1} \\mu I_T + \\mu I_T^T (H^{-1})^TH^THH^{-1}\\mu I_T\n\\right]\\right)\\]\n\\[\\propto exp\\left(-\\frac{1}{2}\\left[\n\\tau^T(\\sigma_e^{-2}H_\\alpha^TH_\\alpha + c^{-1}\\sigma^{-2}H^TH)\\tau - 2\\tau^T(\\sigma_e^{-2}H_\\alpha^TH_\\alpha y + c^{-1}\\sigma^{-2} H^T \\mu I_T)\n\\right]\\right)\\]\nHence, \\(\\tau|\\alpha, \\beta, \\sigma^2_e, y \\sim \\mathcal{N}(\\bar{\\tau}, \\bar{V_\\tau})\\) where,\n\\[\\bar{V_\\tau} = [(\\sigma_e^{-2}H_\\alpha^TH_\\alpha+\\color{purple}{\\mathbf{c^{-1}\\sigma^{-2}H^T H)}}]^{-1}\\] \\[\\bar{\\tau} =  \\bar{V_\\tau}(\\sigma_e^{-2}H_\\alpha^TH_\\alpha y + \\color{purple}{\\mathbf{c^{-1}\\sigma^{-2}H^T \\mu I_T}})\\] 2. \\(\\mathbf{p(\\epsilon|y, \\alpha, \\beta, \\sigma^2_e)}\\) We have, from the measurement equation: \\[y = \\tau + \\epsilon\\] and the prior for \\(\\tau\\) is \\[\\tau|\\beta, c\\sigma^2 \\sim \\mathcal{N}(H^{-1}\\mu I_T, c\\sigma^2(H^TH)^{-1})\\] Hence, we have\n\\[y|\\epsilon \\sim \\mathcal{N}(\\epsilon + H^{-1}\\mu I_T, c\\sigma^2(H^TH)^{-1}) \\propto exp\\left(-\\frac{1}{2}\\frac{(y - \\epsilon- H^{-1}\\mu I_T)^TH^TH(y - \\epsilon - H^{-1}\\mu I_T)}{c\\sigma^2}\\right)\\] We have the prior for \\(\\epsilon\\): \\[\\epsilon | \\sigma_e, \\alpha \\sim \\mathcal{N}(0_T,\\sigma_e^2(H_\\alpha^TH_\\alpha)^{-1}) \\\\ \\propto exp\\left(-\\frac{1}{2\\sigma^2_e}\\epsilon^T(H_\\alpha^TH_\\alpha)\\epsilon\\right)\\] To derive the posterior distribution for \\(epsilon\\), we use Baye’s rule: \\[\\mathbf{p(\\epsilon|y, \\alpha, \\beta, \\sigma^2_e)} = \\frac{\\mathbf{p(\\epsilon, y, \\alpha, \\beta, \\sigma^2_e)}}{\\mathbf{p(y, \\alpha, \\beta, \\sigma^2_e)}} \\propto \\mathbf{p(\\epsilon, y, \\alpha, \\beta, \\sigma^2_e)} = \\mathbf{p(y|\\epsilon,  \\alpha, \\beta, \\sigma^2_e)}p(\\epsilon,  \\alpha, \\beta, \\sigma^2_e)\\] \\[= \\mathbf{p(y|\\epsilon, \\beta)}p(\\epsilon|\\alpha, \\beta, \\sigma^2_e)p(\\alpha, \\beta, \\sigma^2_e) \\propto \\mathbf{p(y|\\epsilon, \\beta)}p(\\epsilon|\\alpha, \\sigma^2_e)\\] \\[\\propto exp\\left(-\\frac{1}{2}\\frac{(y - \\epsilon- H^{-1}\\mu I_T)^TH^TH(y - \\epsilon - H^{-1}\\mu I_T)}{c\\sigma^2}\\right)exp\\left(-\\frac{1}{2\\sigma^2_e}\\epsilon^T(H_\\alpha^TH_\\alpha)\\epsilon\\right)\\]\n\\[\\propto exp\\left( \\frac{y^TH^THy - 2\\epsilon^T H^T Hy - 2y^TH^THH^{-1}\\mu I_T+\\epsilon^T H^TH \\epsilon + 2\\epsilon^T H^THH^{-1}\\mu I_T + I_T^T\\mu (H^{-1})^TH^THH^{-1}\\mu I_T\n}{2c\\sigma^2}\\right)exp\\left(\\frac{1}{2\\sigma^2_e}\\epsilon^T(H_\\alpha^TH_\\alpha)\\epsilon\\right)\\]\n\\[\\propto exp\\left(- \\frac{c^{-1}\\sigma^{-2}\\epsilon^TH^THy + c^{-1}\\sigma^{-2}\\epsilon^TH^TH\\epsilon+2c^{-1}\\sigma^{-2}\\mu\\epsilon^TH^T + \\sigma_e^{-2}\\epsilon^T H_\\alpha^TH_\\alpha \\epsilon }{2}\\right)\\] \\[= exp\\left( \\frac{\\epsilon^T[c^{-1}\\sigma^{-2}H^TH + \\sigma_e^{-2}H_\\alpha^T H_\\alpha]\\epsilon - 2\\epsilon^T[c^{-1}\\sigma^{-2}H^THy-c^{-1}\\sigma^{-2}\\mu H^T]}{2}\\right)\\] Hence, \\(\\epsilon|y, \\alpha, \\beta, \\sigma^2_e \\sim \\mathcal{N}(\\bar{\\epsilon}, \\bar{V}_\\epsilon)\\), where\n\\[\\bar{V}_\\epsilon = [\\color{purple}{\\mathbf{c^{-1}\\sigma^{-2}}}\\color{black}{H^T H + \\sigma_e^{-2}H_\\alpha^T H_\\alpha}]^{-1} \\qquad \\bar{\\epsilon} = \\bar{V}_\\epsilon H^T H\\color{purple}{\\mathbf{c^{-1}\\sigma^{-2}}}\\color{black}{[y-\\mu H^{-1}]}\\] 3. \\(\\mathbf{p(\\alpha|y, \\alpha, \\epsilon, \\sigma^2_e)}\\) From state equation 2, we have: \\[\\epsilon = X_{\\epsilon} \\alpha + e \\qquad \\text{where} \\qquad e \\sim \\mathcal{N}(0, \\sigma^2_eI_T)\\] We note that \\(\\alpha\\) is not in the measurement equation, so the updates for \\(\\alpha\\) does not directly come from the data. Hence we have: \\[p(\\epsilon|\\alpha , \\sigma^2_e) \\sim \\mathcal{N}(X_\\epsilon \\alpha, \\sigma^2_eI_T) \\propto exp\\left(\\frac{(\\epsilon - X_\\epsilon\\alpha)^T(\\epsilon-X_\\epsilon \\alpha)}{2 \\sigma^2_e}\\right)\\] \\[p(\\alpha) \\sim \\mathcal{N}({\\alpha_{prior}, V_{prior}})\\mathbb{1}_{\\alpha \\in A} \\propto exp\\left({-\\frac{1}{2}(\\alpha - \\alpha_{prior})^T}V_{\\alpha,prior}^{-1}(\\alpha - \\alpha_{prior})\\right)\\mathbb{1}_{\\alpha \\in A} \\]\nTherefore,\n\\[p(\\alpha | y, \\epsilon, \\sigma_e^2) \\propto p(\\alpha, y, \\epsilon, \\sigma^2_e) \\propto p(\\epsilon|y, \\sigma^2_e, \\alpha)p(\\alpha|y, \\sigma^2_e)p(y, \\sigma^2_e)\\propto p(\\epsilon|y, \\sigma^2_e, \\alpha)p(\\alpha|y, \\sigma^2_e)\\]\n\\[\\propto exp\\left(\\frac{(\\epsilon - X_\\epsilon\\alpha)^T(\\epsilon-X_\\epsilon \\alpha)}{2 \\sigma^2_e}\\right)exp\\left({-\\frac{1}{2}(\\alpha - \\alpha_{prior})^T}V_{\\alpha,prior}^{-1}(\\alpha - \\alpha_{prior})\\right)\\mathbb{1}_{\\alpha \\in A}\\]\n\\[\\propto exp\\left(\\frac{-2\\sigma^{-2}_e \\alpha^TX_\\epsilon^T\\epsilon + \\sigma^{-2}_e\\alpha^TX_\\epsilon X_\\epsilon \\alpha + \\alpha^TV_{\\alpha,prior}^{-1}\\alpha - 2 \\alpha^TV_{\\alpha,prior}^{-1}\\alpha_{prior}}{2} \\right)\\mathbb{1}_{\\alpha \\in A} \\] \\[= exp\\left(\\frac{\n\\alpha^T(\\sigma^{-2}_eX_\\epsilon X_\\epsilon + V_{\\alpha,prior}^{-1}) \\alpha - 2 \\alpha^T(\\sigma^{-2}_e X_\\epsilon^T\\epsilon + V_{\\alpha,prior}^{-1}\\alpha_{prior})}{2} \\right)\\mathbb{1}_{\\alpha \\in A}\\] Hence, we have the posterior distribution of \\(\\alpha\\) as:\n\\[\\alpha|y, \\epsilon, \\sigma^2_e \\sim \\mathcal{N}(\\bar{\\alpha}, \\bar{V}_\\alpha)\\mathbb{1}_{\\alpha \\in A} \\qquad where \\qquad \\bar{V}_\\alpha = [\\sigma^{-2}_eX_\\epsilon^T X_\\epsilon + V_{\\alpha,prior}^{-1}]^{-1} \\quad \\bar{\\alpha} = \\bar{V}_\\alpha[\\sigma^{-2}_e X_\\epsilon^T\\epsilon + V_{\\alpha,prior}^{-1}\\alpha_{prior}]\\]\n\n\\(\\mathbf{p(\\beta|y, \\tau)}\\)\n\nFrom state equation 2, we have: \\[H \\tau = \\ X_{\\tau}\\beta + \\eta = \\color{purple}{\\mathbf{\\mu I_T + \\eta}}\\],\nwhere \\(\\eta \\sim \\color{purple}{\\mathbf{\\mathcal{N}(0_T, c\\sigma^2I_T)}}\\) and \\[H \\tau \\sim \\color{purple}{\\mathbf{\\mathcal{N}(\\mu I_T, c\\sigma^2I_T)}} \\propto exp\\left(-\\frac{(H\\tau - \\mu I_T)^T(H\\tau - \\mu I_T)}{2c\\sigma^2}\\right)\\]\nWe have the prior of \\(\\beta\\) as: \\[\\beta = \\mu\\sim \\mathcal{N}(\\beta_{prior}, V_{\\beta, prior}) \\propto exp\\left({-\\frac{1}{2}(\\beta - \\beta_{prior})^T}V_{\\beta,prior}^{-1}(\\beta - \\beta_{prior})\\right) = exp\\left({-\\frac{1}{2}(\\mu I_T - \\beta_{prior})^T}V_{\\beta,prior}^{-1}(\\mu I_t - \\beta_{prior})\\right)\\]\nThen, the posterior distributuion of \\(\\beta\\) is:\n\\[p(\\beta | y, \\tau) \\propto p(\\beta, y, \\tau) \\propto p(H\\tau | \\beta,y)p(\\beta) \\] \\[\\propto exp\\left({-\\frac{1}{2}(\\mu I_T - \\beta_{prior})^T}V_{\\beta,prior}^{-1}(\\mu I_t - \\beta_{prior})\\right)exp\\left(-\\frac{(H\\tau - \\mu I_T)^T(H\\tau - \\mu I_T)}{2c\\sigma^2}\\right)\\]\n\\[\\propto exp\\left( \\frac{\\mu^2 V_{\\beta,prior}^{-1} - 2\\mu V_{\\beta,prior}^{-1}\\beta_{prior}-2\\mu c^{-1}\\sigma^{-2}H\\tau+c^{-1}\\sigma^{-2}\\mu^2}{2}\\right) \\propto exp\\left( \\frac{\\mu^2 V_{\\beta,prior}^{-1} - 2\\mu V_{\\beta,prior}^{-1}\\beta_{prior}-2\\mu c^{-1}\\sigma^{-2} H\\tau+c^{-1}\\sigma^{-2}\\mu^2}{2}\\right)\\] \\[\\propto  exp\\left( \\frac{\\mu^2 (V_{\\beta,prior}^{-1}+c^{-1}\\sigma^{-2}) - 2\\mu( V_{\\beta,prior}^{-1}\\beta_{prior} +c^{-1}\\sigma^{-2} H\\tau)}{2}\\right) \\sim \\mathcal{N}(\\bar{\\beta}, \\bar{V}_\\beta)\\] and\n\\[\\bar{V}_\\beta = (V_{\\beta,prior}^{-1}+\\color{purple}{\\mathbf{c^{-1}\\sigma^{-2}}}\\color{black}{)^{-1}  \\qquad \\bar{\\beta} = \\bar{V}_\\beta[V_{\\beta,prior}^{-1}\\beta_{prior}} +\\color{purple}{\\mathbf{c^{-1}\\sigma^{-2}}\\color{black}{ H\\tau]}}\\]\n\n\\(\\mathbf{p(\\sigma^2_e|y, \\epsilon, \\alpha)}\\) Again from state equation 2, we have:\n\n\\[\\epsilon = X_{\\epsilon} \\alpha + e \\qquad \\text{where} \\qquad e \\sim \\mathcal{N}(0, \\sigma^2_eI_T)\\] Hence, we have \\[\\epsilon | \\alpha, \\sigma^2_e \\sim \\mathcal{N}(X_\\epsilon \\alpha, \\sigma_e^2) \\propto (\\sigma_e^2)^{\\frac{T}{2}}exp(-\\frac{(\\epsilon - X_\\epsilon \\alpha)^T(\\epsilon - X_\\epsilon \\alpha)}{2\\sigma^2_e})\\]\nThe prior distribution of \\(\\sigma^2_e\\) is assumed to follow an inverse-gamma 2 distribution, with\n\\[\\sigma^2_e \\sim \\mathcal{N}(s_{prior}, \\nu_{prior}) \\propto (\\sigma^2_e)^{-\\frac{\\nu_{prior}+2}{2}}exp(-\\frac{s_{prior}}{2\\sigma^2_e})\\] Then, the posterior distribution of \\(\\sigma^2_e\\) can be derived as follows:\n\\[p(\\sigma^2_e|\\epsilon, \\alpha, \\sigma^2_e, y) \\propto \\frac{p(\\sigma^2_e, \\epsilon, \\alpha, \\sigma^2_e, y)}{p(\\epsilon, \\alpha, \\sigma^2_e, y)} \\propto p(\\sigma^2_e, \\epsilon, \\alpha, \\sigma^2_e, y)\\] \\[ \\propto p(\\epsilon|\\sigma^2_e,\\alpha, y)p(\\sigma^2_e,\\alpha, y)\\propto p(\\epsilon|\\sigma^2_e,\\alpha, y)p(\\sigma^2_e)\\]\n\\[\\propto (\\sigma^2_e)^{-\\frac{\\nu_{prior}+2}{2}}exp(-\\frac{s_{prior}}{\\sigma^2_e})(\\sigma_e^2)^{\\frac{T}{2}}exp(-\\frac{(\\epsilon - X_\\epsilon \\alpha)^T(\\epsilon - X_\\epsilon \\alpha)}{2\\sigma^2_e})\\] \\[\\propto (\\sigma^2_e)^{-\\frac{\\nu_{prior}+T+2}{2}}exp(-\\frac{(\\epsilon - X_\\epsilon \\alpha)^T(\\epsilon - X_\\epsilon \\alpha)+s_{prior}}{2\\sigma^2_e}) \\sim \\mathcal{IG2}(\\bar{s_{e}}, \\bar{\\nu_e})\\] where \\[\\bar{s}_e = s_{prior} + (\\epsilon - X_\\epsilon \\alpha)^T(\\epsilon - X_\\epsilon \\alpha) \\qquad \\bar{\\nu}_e = T+ \\nu_{prior}\\]\nNote that in this case, \\(\\sigma^2_\\eta\\) is assumed to be a constant, with no posterior distribution.\n\nUC.AR.Gibbs.sampler.Sigma.eta    = function(S, starting.values, priors, sigma_eta_fixed){\n  # Estimation of a simple UC-AR(p) model with sigma_{\\eta e}=0, sigma_eta fixed and tau_0 = 0\n  \n  aux     = starting.values\n  p       = length(aux$alpha)\n  T       = nrow(aux$Y)\n  #####  #####  #####  #####  #####  #####  #####  #####  #####\n  HH = t(aux$H)%*%aux$H\n  #####  #####  #####  #####  #####  #####  #####  #####  #####\n  posteriors    = list(\n    tau     = matrix(NA,T,S),\n    epsilon = matrix(NA,T,S),\n    beta    = matrix(NA,2,S),\n    alpha   = matrix(NA,p,S),\n    sigma   = matrix(NA,2,S),\n    non.stationary.iterations = rep(NA,S)\n  )\n\n  for (s in 1:S){\n\n    # Sampling beta\n    ###########################\n    ######Modified from simple model####################################\n    #beta.v.inv    = diag(1/diag(priors$beta.v))\n    beta.v.inv = 1/priors$beta.v\n    #V.beta.bar    = solve( (1/aux$sigma[1])*crossprod(priors$X.tau) + beta.v.inv )\n    V.beta.bar    = (1/sigma_eta_fixed + beta.v.inv)^(-1)\n    #beta.bar      = V.beta.bar %*% ( (1/aux$sigma[1])*crossprod(priors$X.tau, c(aux$tau[1],diff(aux$tau))) + beta.v.inv%*%priors$beta.m )\n    beta.bar = V.beta.bar*(beta.v.inv*priors$beta.m + sigma_eta_fixed*aux$H*aux$tau)\n\n    #beta.draw     = rmvnorm(1,as.vector(beta.bar),V.beta.bar)\n    beta.draw     = rnorm(1,beta.bar,V.beta.bar)\n    #aux$beta      = as.vector(beta.draw)\n    aux$beta = beta.draw\n    ####################################################\n    X.epsilon     = matrix(NA,T,0)\n    for (i in 1:p){\n      X.epsilon   = cbind(X.epsilon, c(rep(0,i),aux$epsilon[1:(T-i),]))\n    }\n\n    # Sampling alpha\n    ###########################\n    alpha.v.inv   = diag(1/diag(priors$alpha.v))\n    V.alpha.bar   = solve((1/aux$sigma[2])*crossprod(X.epsilon) + alpha.v.inv )\n    V.alpha.bar   = 0.5*(V.alpha.bar + t(V.alpha.bar))\n    alpha.bar     = V.alpha.bar %*% ( (1/aux$sigma[2])*crossprod(X.epsilon, aux$epsilon) + alpha.v.inv%*%priors$alpha.m )\n    # non.stationary= TRUE\n    # ns.i          = 1\n    # while(non.stationary){\n      alpha.draw    = rmvnorm(1,as.vector(alpha.bar),V.alpha.bar)\n    #   non.stationary= max(Mod(eigen(rbind(alpha.draw,cbind(diag(p-1),rep(0,p-1))))$values))&gt;=1\n    #   ns.i          = ns.i + 1\n    # }\n    aux$alpha     = as.vector(alpha.draw)\n\n    H.alpha       = diag(T)\n    for (i in 1:p){\n      sdiag(H.alpha,-i) =  -aux$alpha[i]\n    }\n    aux$H.alpha   = H.alpha\n\n    # Sampling sigma\n    ###########################\n    #sigma.eta.s   = as.numeric(priors$sigma.s + crossprod( (c(aux$tau[1],diff(aux$tau)) - X.tau%*%aux$beta), (priors$H%*%aux$tau - X.tau%*%aux$beta)))\n    #sigma.eta.nu  = priors$sigma.nu + T\n    #sigma.eta.draw= sigma.eta.s/rchisq(1,sigma.eta.nu)\n\n    sigma.e.s     = as.numeric(priors$sigma.s + crossprod(aux$H.alpha%*%aux$epsilon))\n    sigma.e.nu    = priors$sigma.nu + T\n    sigma.e.draw  = sigma.e.s/rchisq(1,sigma.e.nu)\n    #aux$sigma     = c(sigma.eta.draw,sigma.e.draw)\n    aux$sigma     = c(sigma_eta_fixed,sigma.e.draw)\n    # Sampling tau\n    ###########################\n    #V.tau.inv     = (1/aux$sigma[2])*crossprod(aux$H.alpha) + (1/aux$sigma[1])*HH\n    V.tau.inv     = (1/aux$sigma[2])*crossprod(aux$H.alpha) + (1/sigma_eta_fixed)*HH\n    V.tau.inv     = 0.5*(V.tau.inv + t(V.tau.inv))\n    #b.tau         = (1/aux$sigma[2])*crossprod(aux$H.alpha, aux$H.alpha%*%aux$Y) + (1/aux$sigma[1])*crossprod(H, X.tau%*%aux$beta)\n    b.tau         = (1/aux$sigma[2])*crossprod(aux$H.alpha, aux$H.alpha%*%aux$Y) + (1/sigma_eta_fixed)*t(aux$H) * aux$beta\n    precision.L   = t(bandchol(V.tau.inv))\n    epsilon       = rnorm(T)\n    b.tau.tmp     = forwardsolve(precision.L, b.tau)\n    tau.draw      = backsolve(t(precision.L), b.tau.tmp + epsilon)\n    aux$tau       = tau.draw\n\n    # Sampling epsilon\n    ###########################\n    #b.epsilon     = (1/aux$sigma[1])*HH%*%(aux$Y- cumsum(X.tau%*%aux$beta))\n    b.epsilon     = (1/sigma_eta_fixed)*HH%*%(aux$Y- aux$beta*solve(aux$H))\n    epsilon.n     = rnorm(T)\n    b.epsilon.tmp = forwardsolve(precision.L, b.epsilon)\n    epsilon.draw  = backsolve(t(precision.L), b.epsilon.tmp + epsilon.n)\n    aux$epsilon   = epsilon.draw\n\n    posteriors$tau[,s]     = aux$tau\n    posteriors$epsilon[,s] = aux$epsilon\n    posteriors$beta[,s]    = aux$beta\n    posteriors$alpha[,s]   = aux$alpha\n    posteriors$sigma[,s]   = aux$sigma\n    # posteriors$non.stationary.iterations[s] = ns.i\n    if (s%%1000==0){cat(\" \",s)}\n  }\n\n  output      = list(\n    posterior = posteriors,\n    last.draw = aux\n  )\n  return(output)\n}"
  },
  {
    "objectID": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "href": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating gamma error term variance prior scale",
    "text": "Estimating gamma error term variance prior scale"
  },
  {
    "objectID": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "href": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating inverted-gamma 2 error term variance prior scale",
    "text": "Estimating inverted-gamma 2 error term variance prior scale"
  },
  {
    "objectID": "index.html#estimating-the-initial-condition-prior-scale",
    "href": "index.html#estimating-the-initial-condition-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating the initial condition prior scale",
    "text": "Estimating the initial condition prior scale"
  },
  {
    "objectID": "index.html#student-t-prior-for-the-trend-component",
    "href": "index.html#student-t-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t prior for the trend component",
    "text": "Student-t prior for the trend component"
  },
  {
    "objectID": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "href": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating Student-t degrees of freedom parameter",
    "text": "Estimating Student-t degrees of freedom parameter\nThe Student-t distribution is commonly used in statistical modeling to handle data with heavier tails than the normal distribution. An essential parameter of the Student-t distribution is the degrees of freedom \\(\\nu\\), which controls the tail heaviness. In this note, we present the Bayesian estimation of the degrees of freedom parameter for an N-variate Student-t distribution using the Inverted-Gamma 2 (IG2) scale mixture of normals.\nThe N-variate Student-t distribution can be represented as a scale mixture of normals:\n\\[\n\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda \\sim \\mathcal{N}(\\mathbf{\\mu}, \\lambda \\mathbf{I}_N)\n\\]\n\\[\n\\lambda \\mid \\nu \\sim \\mathcal{IG2}(\\nu, \\nu)\n\\]\nwhere:\n\n\\(\\mathbf{y}\\) is the \\(N\\)-dimensional observation vector.\n\\(\\mathbf{\\mu}\\) is the mean vector.\n\\(\\lambda\\) is the latent scale variable.\n\\(\\nu\\) is the degrees of freedom parameter.\n\n\nDerivation of Full Conditional Posteriors\n\nFull Conditional Posterior of \\(\\lambda\\)\nGiven the prior distribution:\n\\[\n\\lambda \\mid \\nu \\sim \\mathcal{IG2}(\\nu, \\nu)\n\\]\nThe likelihood of the data given \\(\\lambda\\) is:\n\\[\n\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda \\sim \\mathcal{N}(\\mathbf{\\mu}, \\lambda \\mathbf{I}_N)\n\\]\nThe full conditional posterior of \\(\\lambda\\) can be derived as follows:\n\nLikelihood of \\(\\mathbf{y}\\) given \\(\\mathbf{\\mu}\\) and \\(\\lambda\\):\n\\[\np(\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda) \\propto \\lambda^{-\\frac{N}{2}} \\exp\\left(-\\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{2\\lambda}\\right)\n\\]\nPrior for \\(\\lambda\\) given \\(\\nu\\):\n\\[\np(\\lambda \\mid \\nu) \\propto \\lambda^{-\\nu - 1} \\exp\\left(-\\frac{\\nu}{\\lambda}\\right)\n\\]\nJoint distribution:\n\\[\np(\\mathbf{y}, \\lambda \\mid \\mathbf{\\mu}, \\nu) = p(\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda) p(\\lambda \\mid \\nu)\n\\]\nFull conditional posterior:\n\\[\np(\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, \\nu) \\propto \\lambda^{-\\frac{N}{2}} \\exp\\left(-\\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{2\\lambda}\\right) \\lambda^{-\\nu - 1} \\exp\\left(-\\frac{\\nu}{\\lambda}\\right)\n\\]\nCombining terms:\n\\[\np(\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, \\nu) \\propto \\lambda^{-\\left(\\nu + \\frac{N}{2} + 1\\right)} \\exp\\left(-\\frac{\\nu + \\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{2}}{\\lambda}\\right)\n\\]\nThis is recognized as the kernel of an Inverted-Gamma 2 distribution:\n\\[\n\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, \\nu \\sim \\mathcal{IG2}\\left(\\nu + N, \\nu + (\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})\\right)\n\\]\n\n\n\nFull Conditional Posterior of \\(\\nu\\)\nTo estimate \\(\\nu\\), we use the Metropolis-Hastings algorithm due to its non-standard form. The steps for deriving the full conditional posterior of \\(\\nu\\) are as follows:\n\nLikelihood of \\(\\lambda\\) given \\(\\nu\\):\n\\[\np(\\lambda \\mid \\nu) = \\frac{\\left(\\frac{\\nu}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)} \\lambda^{-\\left(\\nu/2 + 1\\right)} \\exp\\left(-\\frac{\\nu}{2\\lambda}\\right)\n\\]\nLog-likelihood for \\(\\nu\\) given \\(\\lambda\\):\n\\[\n\\log p(\\lambda \\mid \\nu) = \\frac{\\nu}{2} \\log\\left(\\frac{\\nu}{2}\\right) - \\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\left(\\frac{\\nu}{2} + 1\\right) \\log \\lambda - \\frac{\\nu}{2\\lambda}\n\\]\nLog-prior for \\(\\nu\\) (assuming a non-informative prior):\n\\[\n\\log p(\\nu) = \\text{constant}\n\\]\nFull conditional posterior:\nThe full conditional posterior for \\(\\nu\\) is proportional to the product of the likelihood and the prior:\n\\[\np(\\nu \\mid \\lambda) \\propto p(\\lambda \\mid \\nu) p(\\nu)\n\\]\nSince \\(p(\\nu)\\) is constant, we focus on \\(p(\\lambda \\mid \\nu)\\):\n\\[\n\\log p(\\nu \\mid \\lambda) = \\frac{\\nu}{2} \\log\\left(\\frac{\\nu}{2}\\right) - \\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\left(\\frac{\\nu}{2} + 1\\right) \\log \\lambda - \\frac{\\nu}{2\\lambda}\n\\]\nThis expression does not have a closed form, so we use the Metropolis-Hastings algorithm to sample from this posterior.\n\n\n\n\nR Function for Gibbs Sampler\nBelow is the R function implementing the Gibbs sampler for estimating \\(\\nu\\) using the IG2-scale mixture of normals representation.\n\nmetropolis_hastings_nu &lt;- function(y, mu, n_iter, init_nu, proposal_sd) {\n  # Initialize parameter\n  nu &lt;- init_nu\n  N &lt;- length(y)\n  \n  # Storage for samples\n  nu_samples &lt;- numeric(n_iter)\n  \n  # Log-likelihood function\n  log_likelihood &lt;- function(nu, y, mu) {\n    sum(dt((y - mu) / sqrt(nu), df = nu, log = TRUE))\n  }\n  \n  for (i in 1:n_iter) {\n    # Propose new value for nu\n    nu_proposal &lt;- nu + rnorm(1, 0, proposal_sd)\n    \n    if (nu_proposal &gt; 0) {\n      # Calculate log acceptance ratio\n      log_acceptance_ratio &lt;- log_likelihood(nu_proposal, y, mu) - log_likelihood(nu, y, mu)\n      \n      # Accept or reject the proposal\n      if (log(runif(1)) &lt; log_acceptance_ratio) {\n        nu &lt;- nu_proposal\n      }\n    }\n    \n    # Store the sample\n    nu_samples[i] &lt;- nu\n  }\n  \n  return(nu_samples)\n}\n\ngibbs_sampler_t &lt;- function(y, n_iter, init_values) {\n  # Initialize parameters\n  nu &lt;- init_values$nu\n  mu &lt;- init_values$mu\n  N &lt;- length(y)\n  \n  # Storage for samples\n  nu_samples &lt;- numeric(n_iter)\n  mu_samples &lt;- numeric(n_iter)\n  lambda_samples &lt;- numeric(n_iter)\n  \n  for (i in 1:n_iter) {\n    # Sample lambda\n    shape_lambda &lt;- nu + N\n    rate_lambda &lt;- nu + sum((y - mu)^2)\n    lambda &lt;- 1 / rgamma(1, shape = shape_lambda, rate = rate_lambda)\n    \n    # Sample mu\n    mu &lt;- rnorm(1, mean = mean(y), sd = sqrt(lambda / N))\n    \n    # Sample nu using Metropolis-Hastings\n    log_likelihood &lt;- function(nu, y, mu) {\n      sum(dt((y - mu) / sqrt(nu), df = nu, log = TRUE))\n    }\n    \n    proposal_nu &lt;- nu + rnorm(1, 0, 0.1) # proposal distribution: normal random walk\n    if (proposal_nu &gt; 0) {\n      log_acceptance_ratio &lt;- log_likelihood(proposal_nu, y, mu) - log_likelihood(nu, y, mu)\n      if (log(runif(1)) &lt; log_acceptance_ratio\n\n) {\n        nu &lt;- proposal_nu\n      }\n    }\n    \n    # Store samples\n    nu_samples[i] &lt;- nu\n    mu_samples[i] &lt;- mu\n    lambda_samples[i] &lt;- lambda\n  }\n  \n  return(list(nu = nu_samples, mu = mu_samples, lambda = lambda_samples))\n}\n\n# Example usage\nset.seed(123)\ny &lt;- rnorm(100)\ninit_values &lt;- list(nu = 5, mu = mean(y))\nn_iter &lt;- 1000\nresult &lt;- gibbs_sampler_t(y, n_iter, init_values)\n\n# Display the results\nprint(summary(result$nu))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.118   6.414   6.947   7.398   7.888  10.821 \n\nprint(summary(result$mu))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.19910  0.02664  0.08867  0.08544  0.14827  0.34116 \n\nprint(summary(result$lambda))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6030  0.7907  0.8465  0.8537  0.9095  1.1743 \n\n\n\n\nConclusion\nThis note provided a comprehensive step-by-step algebraic derivation and a sampler for estimating the degrees of freedom parameter \\(\\nu\\) for an N-variate Student-t distribution using an IG2-scale mixture of normals approach within a Bayesian framework. By using the Metropolis-Hastings algorithm, we avoid the need to assume a prior distribution for \\(\\nu\\), simplifying the estimation process. This approach allows for flexible modeling of heavy-tailed data, which is often encountered in practice.\n\n\nReferences\n\nGeweke, J. (1993). Bayesian treatment of the independent Student-t linear model. Journal of Applied Econometrics, 8(S1), S19-S40.\nChib, S., & Greenberg, E. (1995). Understanding the Metropolis-Hastings Algorithm. The American Statistician, 49(4), 327-335."
  },
  {
    "objectID": "index.html#laplace-prior-for-the-trend-component",
    "href": "index.html#laplace-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Laplace prior for the trend component",
    "text": "Laplace prior for the trend component"
  },
  {
    "objectID": "index.html#autoregressive-cycle-component",
    "href": "index.html#autoregressive-cycle-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Autoregressive cycle component",
    "text": "Autoregressive cycle component"
  },
  {
    "objectID": "index.html#random-walk-with-time-varying-drift-parameter",
    "href": "index.html#random-walk-with-time-varying-drift-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Random walk with time-varying drift parameter",
    "text": "Random walk with time-varying drift parameter"
  },
  {
    "objectID": "index.html#student-t-error-terms",
    "href": "index.html#student-t-error-terms",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t error terms",
    "text": "Student-t error terms"
  },
  {
    "objectID": "index.html#conditional-heteroskedasticity",
    "href": "index.html#conditional-heteroskedasticity",
    "title": "Bayesian Unobserved Component Models",
    "section": "Conditional heteroskedasticity",
    "text": "Conditional heteroskedasticity"
  },
  {
    "objectID": "index.html#predictive-density",
    "href": "index.html#predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Predictive density",
    "text": "Predictive density"
  },
  {
    "objectID": "index.html#sampling-from-the-predictive-density",
    "href": "index.html#sampling-from-the-predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Sampling from the predictive density",
    "text": "Sampling from the predictive density"
  },
  {
    "objectID": "index.html#missing-observations",
    "href": "index.html#missing-observations",
    "title": "Bayesian Unobserved Component Models",
    "section": "Missing observations",
    "text": "Missing observations"
  },
  {
    "objectID": "index.html#references-1",
    "href": "index.html#references-1",
    "title": "Bayesian Unobserved Component Models",
    "section": "References",
    "text": "References"
  }
]